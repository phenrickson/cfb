---
title: "Calculating CFB Elo Ratings"
author: "Phil Henrickson"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE #adds a Table of Contents
    theme: cerulean
    number_sections: TRUE #number your headings/sections
    toc_float: TRUE #let your ToC follow you as you scroll
    keep_md: no
    fig.caption: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = F,
                      error = F,
                      warning=F,
                      dev="png",
                      fig.width = 10,
                      fig.height = 6)

options(knitr.duplicate.label = "allow")

options(scipen=999)

```

```{r connect to snowflake}

library(DBI)
library(odbc)
library(RODBC)
library(keyring)

# connect to snowflake
myconn <- DBI::dbConnect(odbc::odbc(),
                         "SnowflakeDSII",
                         Database = "CFB_DEMO",
                         warehouse = "DEMO_WH",
                         uid="phil.henrickson",
                         pwd=keyring::key_get("AE_Snowflake"))

```

```{r packages, include=F} 

source(here::here("scripts/load_packages.R"))
library(rstan)
library(rstanarm)
library(rstantools)
library(jsonlite)
library(forcats)
library(teamcolors)
library(forcats)

conflict_prefer("lag", "dplyr")

```

```{r get colors for ncaa teams}

# look at colors for ncca
ncaa_colors = teamcolors %>%
        filter(league == 'ncaa') 

# define palette for ncaa
#league_pal("ncaa", 1)

```

```{r flextable settings}

set_flextable_defaults(theme_fun = theme_alafoli,
                       font.color = "grey10",
                       font.size=8,
                       padding.bottom = 6, 
                       padding.top = 6,
                       padding.left = 6,
                       padding.right = 6,
                       background.color = "white")

```

```{r load functions}

# integer plotting function
int_breaks <- function(x, n = 5) {
  l <- pretty(x, n)
  l[abs(l %% 1) < .Machine$double.eps ^ 0.5] 
}

source(here::here("functions/theme_phil.R"))
rm(a)

```

```{r connect to raw data sources}

# data on teams with logos and colors
teams_raw = bind_rows(
        DBI::dbGetQuery(myconn,
                             paste('SELECT * FROM CFB_DEMO.CFD_RAW.TEAMS')) %>%
        as_tibble())  %>%
        rename(TEAM = SCHOOL,
               TEAM_ID = ID,
               TEAM_MASCOT = MASCOT,
               TEAM_ABBREVIATION = ABBREVIATION)

# load teamcolors
teamcolors = teamcolors %>%
        mutate(location = case_when(location == 'Ole' ~ name,
                                    location == 'Southern California Trojans' ~ 'USC',
                                    location == 'Miami' & name == 'Miami (OH)' ~ name,
                                    TRUE ~ location))

# get games
games_raw = DBI::dbGetQuery(myconn,
                             paste('SELECT * FROM CFB_DEMO.CFD_RAW.ANALYSIS_GAMES')) %>%
        as_tibble() %>%
        mutate(CONFERENCE_GAME = case_when(CONFERENCE_GAME == 'true' ~ T,
                                           CONFERENCE_GAME == 'false' ~ F)) %>%
        mutate(GAME_DATE = as.Date(START_DATE)) %>%
        mutate(START_DATE = as_datetime(START_DATE)) %>%
        arrange(START_DATE) %>%
        group_by(SEASON, SEASON_TYPE, HOME_CONFERENCE) %>%
        mutate(LAST_GAME = START_DATE == max(START_DATE)) %>% 
        ungroup() %>%
        # logic for flagging conference championship games
        mutate(CONFERENCE_CHAMPIONSHIP = case_when(CONFERENCE_GAME == T & 
                                                           (HOME_CONFERENCE != 'FBS Independents') & 
                                                           (HOME_CONFERENCE != 'FCS Independents') &
                                                           !(HOME_CONFERENCE %in% 'Big Ten' & SEASON < 2014) &
                                                           !(HOME_CONFERENCE %in% 'Western Athletic') &
                                                           (NEUTRAL_SITE == T | is.na(VENUE) |
                                                                    VENUE == 'Glass Bowl' |
                                                                    VENUE == 'Bright House Networks Stadium' |
                                                                    VENUE == 'Alamodome' |
                                                                    VENUE == 'Georgia Dome') &
                                                           SEASON > 2000 &
                                                           SEASON_TYPE == 'regular' &
                                                           WEEK > 13 &
                                                           LAST_GAME == T ~ T,
                                                   TRUE ~ F)) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                    HOME_POINTS < AWAY_POINTS ~ 'no',
                                    HOME_POINTS == AWAY_POINTS ~ 'no'),
               levels = c("no", "yes"))) %>%
        mutate(HOME_DIVISION  = replace_na(HOME_DIVISION, "missing"),
               AWAY_DIVISION = replace_na(AWAY_DIVISION, "missing")) %>%
        left_join(., teams_raw %>%
                          select(TEAM, TEAM_ABBREVIATION) %>%
                          rename(HOME_TEAM = TEAM,
                                 HOME_TEAM_ABBR = TEAM_ABBREVIATION),
                  by = c("HOME_TEAM")) %>%
        left_join(., teams_raw %>%
                          select(TEAM, TEAM_ABBREVIATION) %>%
                          rename(AWAY_TEAM = TEAM,
                                 AWAY_TEAM_ABBR = TEAM_ABBREVIATION),
                  by = c("AWAY_TEAM"))

# make a table of team conference by season
team_conference_season = games_raw %>% 
  select(SEASON, HOME_TEAM, HOME_CONFERENCE) %>% 
  rename(TEAM = HOME_TEAM, CONFERENCE = HOME_CONFERENCE) %>% 
  bind_rows(.,
            games_raw %>%
                    select(SEASON, AWAY_TEAM, AWAY_CONFERENCE) %>% 
                    rename(TEAM = AWAY_TEAM, CONFERENCE = AWAY_CONFERENCE)) %>%
  select(SEASON, TEAM, CONFERENCE) %>%
  unique()

```

# Elo Ratings for CFB

We can now use historical CFB games to learn Elo Ratings for college football teams, which we will then use to predict upcoming games and seasons. The data we're using is at the game level, in which we have features indicating the season, the week, the location, the teams, and the score. 

```{r game level data for all teams}

set.seed(8)
games_raw %>%
        filter(SEASON > 2000 & SEASON < 2017) %>%
        sample_n(5) %>%
        select(GAME_ID, SEASON, WEEK, VENUE, HOME_TEAM, AWAY_TEAM, HOME_POINTS, AWAY_POINTS, HOME_WIN) %>%
        mutate(GAME_ID = as.character(GAME_ID),
               SEASON = as.character(SEASON)) %>%
        flextable() %>%
        autofit()

```

We also have additional features on team conferences, attendance, start time, betting line, as well as information from an existing Elo Model. We'll set that aside for now, as our goal here is to go through the process of developing our own Elo model. 

We lose some information the further we go back, but we have outcomes of games back to the very first game between Rutgers and Princeton in 1869. Note: We have more games in recent years due to having more data on FCS teams in these seasons.

```{r get outcomes since 1869}

games_raw %>%
        filter(SEASON < 2022) %>%
        group_by(SEASON) %>%
        summarize(n_games = n_distinct(GAME_ID)) %>%
        ggplot(., aes(x=SEASON,
                      y=n_games))+
        geom_col()+
        theme_phil()

```

We could start this analysis going back all the way to 1869 as the starting point, but I'll start at 1970 for now since I'm at least somewhat familiar with teams in this era and will be able to eyeball some of the results

I'll set every team FBS's initial rating to 1500, non FBS teams to 1200, and the scaling factor to 400. The appropriate value for K is something that I'll want to explore empirically, but I'll start it at 25 and go from there. 

I'm using two more functions here to determine elo ratings. The first determines how Elo ratings are updated after each game, looking at each team's pre game Elo rating and then determing their new rating baed on the outcome and the settings for $K$ and $V$. I also have added in the option to add a slight bump to the Elo rating for the home team in order to capture home field advantage, I'll leave it at zero for now.

```{r load function for updating elo ratings, echo=T}

# functions for updating elo ratings\
# load function
source(here::here("functions/get_expected_score.R"))
source(here::here("functions/get_new_elos.R"))

# updating
get_new_elos

```

The next function is what I'll use to calculate Elo ratings based on historical data, using a set of games as an input and settings for the various options to calculate Elo ratings.

```{r load function for getting elo ratings based on historical games}

source(here::here("functions/calc_elo_ratings.R"))

# calc_elo_ratings

```

## Initial Settings

Let's run from 1970 to 2000 to see how the results change each team's Elo rating over the course of a couple decades. This means we loop over every game, look at the result, and then update each team's Elo rating based on the outcome and the margin of victory (as well as our other settings).

```{r run v1 from 1970 to 2000, include=F}

# now loop through a set of games
games = games_raw %>%
        filter(SEASON >=1970) %>%
        filter(SEASON <= 2000) %>%
        arrange(START_DATE) %>%
        select(GAME_ID, 
               SEASON, 
               WEEK,
               SEASON_TYPE,
               START_DATE,
               GAME_DATE,
               NEUTRAL_SITE, 
               HOME_TEAM,
               AWAY_TEAM,
               HOME_CONFERENCE,
               AWAY_CONFERENCE,
               HOME_DIVISION,
               AWAY_DIVISION,
               HOME_POINTS,
               AWAY_POINTS)

# run over games
elo_returned = calc_elo_ratings(games,
                          home_field_advantage = 0,
                          reversion = 0,
                          k = 25,
                          v = 400,
                          verbose = T)

```

Running this gives us a pre and post game Elo rating for every team for every game over the selected time period.

```{r show example}

set.seed(1999)

elo_returned$game_outcomes %>%
        sample_n(5) %>%
        arrange(GAME_DATE) %>%
        rename(DATE = GAME_DATE,
               HOME = HOME_TEAM,
               AWAY = AWAY_TEAM,
               HOME_PRE_ELO = HOME_PREGAME_ELO,
               AWAY_PRE_ELO = AWAY_PREGAME_ELO,
               HOME_POST_ELO = HOME_POSTGAME_ELO,
               AWAY_POST_ELO = AWAY_POSTGAME_ELO) %>%
        mutate(HOME_WIN = case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                    TRUE ~ 'no')) %>%
        select(SEASON, HOME, AWAY, HOME_WIN, HOME_POINTS, AWAY_POINTS, HOME_PRE_ELO, AWAY_PRE_ELO, HOME_POST_ELO, AWAY_POST_ELO) %>%
        mutate(SEASON = factor(SEASON)) %>%
        mutate_if(is.numeric, round, 0) %>%
        flextable() %>%
        autofit()

```

### Examples

We can see how these ratings adjust for selected teams over time. At the current settings, here is each SEC team's Elo ratings fared over the selected time period, starting from 1985.

```{r get SEC v1, fig.height=8}

elo_returned$team_outcomes %>%
        filter(CONFERENCE == 'SEC') %>%
        filter(SEASON > 1985) %>%
        group_by(SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ TEAM)) %>%
        left_join(.,
                  teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                          by = c("TEAM")) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      label = TEAM_LABEL,
                      color = name,
                      y=POSTGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 2.5,
               direction = "y",
               hjust = -1.2,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 5)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(0, 18))+
        scale_color_teams(name = "TEAM")+
        ggtitle("Elo Ratings at Initial Settings")

```

And here's the Big 10.

```{r get big ten v1, fig.height=8}

elo_returned$team_outcomes %>%
        filter(CONFERENCE == 'Big Ten') %>%
        filter(SEASON > 1985) %>%
        group_by(SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ TEAM)) %>%
        left_join(.,
                  teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                          by = c("TEAM")) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      label = TEAM_LABEL,
                      color = name,
                      y=POSTGAME_ELO))+
t       geom_text_repel(
               fontface = "bold",
               size = 2.5,
               direction = "y",
               hjust = -1.2,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 5)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(0, 18))+
        scale_color_teams(name = "TEAM")+
        ggtitle("Elo Ratings at Initial Settings")


```

And the Southwest Conference (RIP).

```{r southwest over this time period}

elo_returned$team_outcomes %>%
        filter(CONFERENCE == 'Southwest') %>%
        filter(SEASON > 1985) %>%
        group_by(SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ TEAM)) %>%
        left_join(.,
                  teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                          by = c("TEAM")) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      label = TEAM_LABEL,
                      color = name,
                      y=POSTGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 2.5,
               direction = "y",
               hjust = -1.2,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 5)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(0, 18))+
        scale_color_teams(name = "TEAM")+
        ggtitle("Elo Ratings at Initial Settings")

```

### Results

How do we evaluate how *good* these Elo ratings are? First, we can look at the log loss for the predicted score for the home team (ie, the probability the home team wins) compared to the outcome for the home team.

How did we do? I'll compare the result of the Elo ratings to a baseline model that assigns the home team a 60% chance of winning. 

```{r log loss by week by season}

# log loss
elo_returned$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, games) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate(method = 'Elo') %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PROB = .6) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                          mn_log_loss(truth = HOME_WIN,
                                    estimate = HOME_PROB,
                                    event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=SEASON,
                      color = method,
                      y=.estimate,
                      size = games))+
        geom_point()+
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("log-loss")+
        scale_color_manual(values = c("deepskyblue1",
                                      "grey40"))+
        ggtitle("Average Log-Loss by Season")+
        coord_cartesian(ylim = c(0, 1))

```

As we would hope, the Elo ratings outperform simply predicting that the home team will win.

We can also just look at the number of games correctly predicted, and find similar results.

```{r accuracy by each for initial version}

# accuracy
elo_returned$game_outcomes %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes', TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, games) %>%
        yardstick::accuracy(truth = HOME_WIN,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        mutate(method = 'Elo') %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PRED = factor('yes',
                                                    levels = c('no', 'yes'))) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                             yardstick::accuracy(truth = HOME_WIN,
                                                 estimate = HOME_PRED,
                                                 event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=SEASON,
                      color = method,
                      y=.estimate,
                      size = games))+
        geom_point()+
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.25)+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("Accuracy")+
        scale_color_manual(values = c("deepskyblue1",
                                      "grey40"))+
        ggtitle("Average Accuracy by Season")+
        coord_cartesian(ylim = c(0, 1))

```

```{r save v1 results for comparison}

elo_returned_v1 = elo_returned

```

## Adding Mean Reversion

As it's constructed so far, the Elo ratings for a team in one season pick up right from where they left off in the previous season. This is particularly an issue in the world college football, given that players graduate and turnover in rosters year over year is high. We shouldn't expect, in other words, a team at the beginning of its 1980 season to be the exact same as it ended its 1979 season.

Currently, our Elo ratings do not reset in anyway, and this means we ended up being a little bit worse at the beginning of each season.

```{r look at results by season and week}

## accuracy
elo_returned$game_outcomes %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes', TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON, WEEK) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, WEEK, games) %>%
        yardstick::accuracy(truth = HOME_WIN,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        mutate(method = 'Elo') %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PRED = factor('yes',
                                                    levels = c('no', 'yes'))) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON, WEEK) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, WEEK, games) %>%
                          yardstick::accuracy(truth = HOME_WIN,
                                                 estimate = HOME_PRED,
                                                 event_level = 'second') %>%
                          mutate(method = 'Home Win')) %>%
        ggplot(., aes(x=WEEK,
                      group = method,
                      color = method,
                      by = SEASON,
                      y=.estimate))+
        geom_point(aes(size = games),
                   alpha = 0.5)+
        geom_smooth(
                  stat = 'smooth',
                  method = 'loess',
                  se=F,
                  formula = 'y ~ x')+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("Accuracy")+
        scale_color_manual(values = c("deepskyblue1",
                                      "grey40"))+
        ggtitle("Accuracy by Week of Season")+
        coord_cartesian(ylim = c(0, 1))

# elo_returned$game_outcomes %>%
#         mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes', TRUE ~ 'no'),
#                                   levels = c("no", "yes"))) %>%
#         mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
#                                        TRUE ~'no'))) %>%
#         group_by(SEASON, WEEK) %>%
#         mutate(games = n_distinct(GAME_ID)) %>%
#         group_by(SEASON, WEEK, games) %>%
#         yardstick::mn_log_loss(truth = HOME_WIN,
#                     estimate = HOME_PROB,
#                     event_level = 'second') %>%
#         mutate(method = 'Elo') %>%
#         bind_rows(.,
#                   elo_returned$game_outcomes %>%
#                           mutate(HOME_PROB = .6) %>%
#                           mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
#                                        TRUE ~'no'))) %>%
#                           group_by(SEASON, WEEK) %>%
#                           mutate(games = n_distinct(GAME_ID)) %>%
#                           group_by(SEASON, WEEK, games) %>%
#                           yardstick::mn_log_loss(truth = HOME_WIN,
#                                                  estimate = HOME_PROB,
#                                                  event_level = 'second') %>%
#                           mutate(method = 'Home Win')) %>%
#         ggplot(., aes(x=WEEK,
#                       group = method,
#                       color = method,
#                       by = SEASON,
#                       y=.estimate))+
#         geom_point(aes(size = games),
#                    alpha = 0.5)+
#         geom_smooth(
#                   stat = 'smooth',
#                   method = 'loess',
#                   se=F,
#                   formula = 'y ~ x')+
#         theme_phil()+
#         theme(legend.title = element_text())+
#         guides(size = guide_legend(title = 'Number of Games',
#                                    title.position = 'top'),
#                color = guide_legend(title.position = 'top'))+
#             scale_x_continuous(breaks = int_breaks)+
#         ylab("Log Loss")+
#         scale_color_manual(values = c("deepskyblue1",
#                                       "grey40"))+
#         ggtitle("Average Log-Loss by Week of Season")+
#         coord_cartesian(ylim = c(0, 1))


```

One way we can try to improve this is by regressing each team back towards the mean at the start of each season. If we think about Elo ratings in a Bayesian way, we started each team out with a mostly uninformative prior that we updated based on new information. By the end of a season we have a much better idea about how good a team is. But, between seasons, teams can change quite a bit, and we should become less certain about what we know about a team at the start of a season. 

I'll handle this by mean reverting FBS and non FBS teams slightly back to their original setting (1500 and 1200, respectively) at the start of each season. To start, I'll set this reversion to 1/4, meaning that a team's rating at the start of a season is 3/4 the result of its prior games and 1/4 back to the mean. Good teams will get pulled down slightly and bad teams will get pulled up.

```{r now re run with mean reversion, include=F}

rm(elo_returned)

# run over games
elo_returned = calc_elo_ratings(games,
                          home_field_advantage = 0,
                          reversion = 1/4,
                          k = 25,
                          v = 400)

```

### Examples

How do the ratings with mean reversion compare to the ratings with no reversion?

I'll take a team like Wisconsin and compare their pregame ratings over this time period under both version. At the start, the two ratings are the same, as Wisconsin hasn't played enough games to shift that far from the initial setting of 1500. But over the years, we can see how reverting Wisconsin a bit at the start of each season changes its rating. Over the course of 40 odd years, this difference starts to really add up in terms of the scale.

```{r look at the same team for both versions, fig.height=8}

bind_rows(elo_returned_v1$team_outcomes %>%
                  mutate(method = 'Elo'),
        elo_returned$team_outcomes %>%
                mutate(method = 'Elo + Mean Reversion')) %>%
        left_join(.,
                          teamcolors %>%
                                  filter(league == 'ncaa') %>%
                                  mutate(TEAM = location),
                          by = c("TEAM")) %>%
        filter(TEAM == 'Wisconsin') %>%
        group_by(method, SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ PREGAME_ELO)) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      linetype = method,
                      by = name,
                      label = round(TEAM_LABEL,0),
                      color = name,
                      y=PREGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 3,
               direction = "y",
               hjust = 0,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1.05,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.25)+
        facet_wrap(TEAM + SEASON~.,
                   ncol = 7)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                   #     xlim = c(NA, 15),
                        ylim = c(1200, 2200))+
        scale_color_teams(name = "TEAM")

```

If we look at Big Ten teams during this time period, we should see them tighten up at the beginning of each season.

```{r big ten with mean reversion}

elo_returned$team_outcomes %>%
        filter(CONFERENCE == 'Big Ten') %>%
        filter(SEASON > 1985) %>%
        group_by(SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ TEAM)) %>%
        left_join(.,
                  teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                          by = c("TEAM")) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      label = TEAM_LABEL,
                      color = name,
                      y=POSTGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 2.5,
               direction = "y",
               hjust = -1.2,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 5)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(0, 18))+
        scale_color_teams(name = "TEAM")+
        ggtitle("Elo Ratings at Initial Settings")

```


### Results

I'll now compare the initial ratings to the updated ratings that include mean reversion. Looking at the two methods season over season, there's not much difference between them. I could do like, a Wald test, to compare the two formally, but who has time for that.

```{r compare initial to reversion accuracy and log loss}

# accracy
elo_returned_v1$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes',
                                            TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        group_by(SEASON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, games) %>%
        yardstick::accuracy(truth = HOME_WIN,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        mutate(method = 'Elo ') %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                                  mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes',
                                            TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                          yardstick::accuracy(truth = HOME_WIN,
                                              estimate = HOME_PRED,
                                              event_level = 'second') %>%
                          mutate(method = 'Elo + Mean Reversion')) %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PRED = factor('yes',
                                                    levels = c("no", "yes"))) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                         yardstick::accuracy(truth = HOME_WIN,
                                    estimate = HOME_PRED,
                                    event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=SEASON,
                      color = method,
                      y=.estimate,
                      size = games))+
        geom_point()+
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("Accuracy")+
        scale_color_manual(values = c("deepskyblue1",
                                      "navy",
                                      "grey40"))+
        ggtitle("Average Accuracy by Season")+
        coord_cartesian(ylim = c(0, 1))

# log loss
elo_returned_v1$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, games) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate(method = 'Elo ') %>%
        bind_rows(.,
                     elo_returned$game_outcomes %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                          mn_log_loss(truth = HOME_WIN,
                                    estimate = HOME_PROB,
                                    event_level = 'second') %>%
                          mutate(method = 'Elo + Mean Reversion')) %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PROB = .6) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                          mn_log_loss(truth = HOME_WIN,
                                    estimate = HOME_PROB,
                                    event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=SEASON,
                      color = method,
                      y=.estimate,
                      size = games))+
        geom_point()+
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("log-loss")+
        scale_color_manual(values = c("deepskyblue1",
                                      "navy",
                                      "grey40"))+
        ggtitle("Average Log-Loss by Season")+
        coord_cartesian(ylim = c(0, 1))

```

If anything this reversion seems to be hurting the results over this time period. I wonder if we do better in the early weeks but worse in the latter?

```{r show results by week by season with updated version}

elo_returned_v1$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON, WEEK) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, WEEK, games) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate(method = 'Elo ') %>%
        bind_rows(.,
                     elo_returned$game_outcomes %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                         group_by(SEASON, WEEK) %>%
                        mutate(games = n_distinct(GAME_ID)) %>%
                        group_by(SEASON, WEEK, games) %>%
                        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
                          mutate(method = 'Elo + Mean Reversion')) %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PROB = .6) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                       group_by(SEASON, WEEK) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, WEEK, games) %>%
                          mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=WEEK,
                      group = method,
                      color = method,
                      by = SEASON,
                      y=.estimate))+
        geom_point(aes(size = games),
                   alpha = 0.5)+
        geom_smooth(
                  stat = 'smooth',
                  method = 'loess',
                  se=F,
                  formula = 'y ~ x')+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("Log Loss")+
        scale_color_manual(values = c("deepskyblue1",
                                      "navy",
                                      "grey40"))+
        ggtitle("Log Loss by Week of Season")+
        coord_cartesian(ylim = c(0, 1))

```

Hmmm. Not really. But, we'll keep reversion as a tuning parameter and see how it does with our other settings.

```{r save v2}

elo_returned_v2 = elo_returned

```

## Adding Home Field Advantage

I've seen other Elo rating systems explicitly add points to teams that are at home as a means of accounting for home field advantage. Right now, I'm not doing that, so we should expect to see the home team outperform their Elo rating. Is that the case? I'll look at the performance of the predictions from each of the models so far. Generally speaking, we don't do as well detecting when predicting the away team is going to win.

```{r results by home team}

class_metrics = metric_set(yardstick::precision,
                           yardstick::recall,
                           yardstick::f_meas,
                           yardstick::accuracy,
                           yardstick::bal_accuracy,
                           yardstick::kap,
                           yardstick::npv,
                           yardstick::ppv)

# v1 initial settings
elo_returned_v1$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        yardstick::conf_mat(HOME_WIN, HOME_PRED,
                            dnn = c("Home Pred", "Home Win"))

# v2 mean reversion
elo_returned_v2$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        yardstick::conf_mat(HOME_WIN, HOME_PRED, 
                            dnn = c("Home Pred", "Home Win"))

# bind together
bind_rows(
         elo_returned_v1$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo'),
         elo_returned_v2$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion')) %>%
        group_by(method) %>%
        class_metrics(truth = HOME_WIN,
                      estimate = HOME_PRED, 
                      event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()

```

The Elo ratings with mean reversion are basically the same overall, though both versions tend to do better when predicting that the home team will win (PPV). When the models predict that the home team will lose, they don't do as well (NPV). Right now, we tend to under predict how much the home team wins.

```{r look at probs for home vs away along with classification}

library(jcolors)

elo_returned_v2$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'home win',
                                               TRUE ~'away win'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'home win',
                                             TRUE ~ 'away win'),
                                          levels = c("away win", "home win"))) %>%
        mutate(method = 'Elo + Mean Reversion') %>%
        mutate(PRED_CORRECT = HOME_PRED == HOME_WIN) %>%
        ggplot(., aes(x=HOME_PROB,
                      fill = PRED_CORRECT))+
        geom_histogram(bins = 100, alpha = 0.7)+
        facet_wrap(HOME_WIN + method ~.,
                   ncol = 1)+
        theme_phil()+
        theme(legend.title = element_text())+
      #  scale_fill_jcolors()+
        geom_vline(xintercept = 0.5,
                   linetype = 'dashed')+
        scale_fill_manual(values = c("red", "blue"))
        

```

One other parameter I want to toggle here is a home team advantage. I'll start by adding a 25 point bump to the Elo score for the home team, as this roughly maps to about a 3% increase in the probability of winning. I won't add any bonus for games played at neutral sites. I'll now re run making this home team adjustment.

```{r amend functions to include home team adjustment, include=F}

rm(elo_returned)

# run over games
elo_returned = calc_elo_ratings(games,
                          home_field_advantage = 25,
                          reversion = 1/4,
                          k = 25,
                          v = 400)

elo_returned_v3 = elo_returned

```

### Results

I'll now compare the various versions we've computed so far, summarizing to their performance over the entire time period we've looked at.

```{r compare performance between all methods}

# bind together
bind_rows(
         elo_returned_v1$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo'),
         elo_returned_v2$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion'),
         elo_returned_v3$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion + Home Advantage')) %>%
        group_by(method) %>%
        class_metrics(truth = HOME_WIN,
                      estimate = HOME_PRED, 
                      event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()

bind_rows(
         elo_returned_v1$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo'),
         elo_returned_v2$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion'),
         elo_returned_v3$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion + Home Advantage')) %>%
        group_by(method) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()

```

The home field advantage makes us slightly more accurate by picking up a few more home wins, but generally speaking the differences are all still pretty small.

## Tuning Over All Parameters

At this point, we've covered a few different parameters that we can toggle when computing Elo ratings: mean reversion, home field advantage, as well as the original settings of the K factor and the scaling factor (what I have dubbed V). I'll now tune over these directly via a grid search and see well our Elo ratings perform at various different settings.

```{r tune elo rating, include=F}

k_pars = seq(5, 50, 15)
v_pars = 400
reversion_pars = seq(0, 0.3, 0.1)
home_pars = seq(0, 90, 15)

# make grid
grid_pars = expand.grid(k = k_pars,
            v = v_pars,
            reversion = reversion_pars,
            home_field_advantage = home_pars)

```

```{r set parallel core, eval=F}

doParallel::registerDoParallel(
        parallel::detectCores()-1
        )

```


```{r loop over parameters, eval=F}

# loop over settings in tuning grid in parallel
elo_tuning_results = foreach(i = 1:nrow(grid_pars), .combine = bind_rows) %dopar% {
        
        # get par row
        pars = grid_pars[i,]
        
        # compute elo
        elo_returned = calc_elo_ratings(games,
                          home_field_advantage = pars$home_field_advantage,
                          reversion = pars$reversion,
                          k = pars$k,
                          v = pars$v,
                          verbose = F)
        
        # grab game outcomes
        out = elo_returned$game_outcomes %>%
                mutate(home_field_advantage = pars$home_field_advantage,
                       reversion = pars$reversion,
                       k = pars$k,
                       v = pars$v)
            
        # progress           
        cat("\r", i, "of", nrow(grid_pars), "grid parameters completed");  flush.console()
        
        # return
        out
        
}

registerDoSEQ()

```

```{r register seq, eval=F}

save(elo_tuning_results,
     file = here::here("data/elo_tuning_results.Rdata"))

```

Once that's finished we can look at how the different ratings performed across the combination of tuning parameters.

```{r check on tuning results}

load(here::here("data/elo_tuning_results.Rdata"))

# log loss and accuracy
assess_metrics = metric_set(yardstick::mn_log_loss,
                           yardstick::accuracy)

# assess
tuning_assess = elo_tuning_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        mutate(yes = HOME_PROB) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        assess_metrics(truth = HOME_WIN,
                       yes,
                    estimate = HOME_PRED,
                    event_level = 'second')

# # accuracy
# tuning_assess %>%
#         filter(.metric == 'accuracy') %>%
#         slice_max(order_by = .estimate,
#                   n = 10) %>%
#         select(-.estimator) %>%
#         mutate_if(is.numeric, round, 3) %>%
#         flextable() %>%
#         autofit()
# 
# tuning_assess %>%
#         filter(.metric == 'mn_log_loss') %>%
#         slice_min(order_by = .estimate,
#                   n = 10) %>%
#         select(-.estimator) %>%
#         mutate_if(is.numeric, round, 3) %>%
#         flextable() %>%
#         autofit()
# 
# 
# # # roc auc
# # elo_tuning_results %>%
# #         mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
# #                                                TRUE ~'no'))) %>%
# #         mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
# #                                              TRUE ~ 'no'),
# #                                           levels = c("no", "yes"))) %>%
# #         group_by(k, v, reversion, home_field_advantage) %>%
# #         roc_auc(truth = HOME_WIN,
# #                     estimate = HOME_PROB,
# #                     event_level = 'second') %>%
# #         arrange(desc(.estimate)) %>%
# #         mutate_if(is.numeric, round, 3) %>%
# #         head(10) %>%
# #         flextable() %>%
# #         autofit()
# 
# # # prediction metrics
# # elo_tuning_results %>%
# #         mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
# #                                                TRUE ~'no'))) %>%
# #         mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
# #                                              TRUE ~ 'no'),
# #                                           levels = c("no", "yes"))) %>%
# #         group_by(k, v, reversion, home_field_advantage) %>%
# #         class_metrics(truth = HOME_WIN,
# #                       estimate = HOME_PRED, 
# #                       event_level = 'second') %>%
# #         filter(.metric == 'f_meas') %>%
# #         arrange(desc(.estimate)) %>%
# #         mutate_if(is.numeric, round, 3) %>%
# #         head(10) %>%
# #         flextable() %>%
# #         autofit()
# 
# # prediction metrics
# elo_tuning_results %>%
#         mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
#                                                TRUE ~'no'))) %>%
#         mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
#                                              TRUE ~ 'no'),
#                                           levels = c("no", "yes"))) %>%
#         group_by(k, v, reversion, home_field_advantage) %>%
#         class_metrics(truth = HOME_WIN,
#                       estimate = HOME_PRED, 
#                       event_level = 'second') %>%
#         filter(.metric == 'f_meas') %>%
#         arrange(desc(.estimate)) %>%
#         mutate_if(is.numeric, round, 3) %>%
#         head(10) %>%
#         flextable() %>%
#         autofit()

```

```{r plot across tuning parameters}

elo_tuning_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate(home_field_advantage = factor(home_field_advantage)) %>%
        ggplot(., aes(x=k,
                      y=.estimate,
                      color = home_field_advantage))+
        facet_wrap(v + reversion~.,
                   ncol = 2)+
        theme_bw()+
        geom_point()+
        geom_line()+
        scale_color_viridis_d()+
        ggtitle("Log-loss on training games")


elo_tuning_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        yardstick::accuracy(truth = HOME_WIN,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        mutate(home_field_advantage = factor(home_field_advantage)) %>%
        ggplot(., aes(x=k,
                      y=.estimate,
                      color = home_field_advantage))+
        facet_wrap(v + reversion~.,
                   ncol = 2)+
        theme_bw()+
        geom_point()+
        geom_line()+
        scale_color_viridis_d()+
        ggtitle("Accuracy on training games")

```

As before, we get pretty similar results across the board. I'll select some of the top candidates for each of these different metrics (I'm mostly looking at the log loss) and see how they do on the validation set. 

```{r select tuning parametets}

best_pars = bind_rows(tuning_assess %>%
                              filter(.metric == 'accuracy') %>%
                              slice_max(order_by =.estimate,
                                        n = 3),
                      tuning_assess %>%
                              filter(.metric == 'mn_log_loss') %>%
                              slice_min(order_by =.estimate,
                                        n = 3)) %>%
        select(-.estimate) %>%
        unique()

```

## Validating Selected Parameters

At the best candidates for the selected parameters, I'll compute the Elo ratings over the time period we've used so far. I'll then evaluate their performance over the next decade of games.

```{r get games for time periods}

# now loop through a set of games
train_games = games_raw %>%
        filter(SEASON >=1970) %>%
        filter(SEASON <= 2000) %>%
        arrange(START_DATE) %>%
        select(GAME_ID, 
               SEASON, 
               WEEK,
               SEASON_TYPE,
               START_DATE,
               GAME_DATE,
               NEUTRAL_SITE, 
               HOME_TEAM,
               AWAY_TEAM,
               HOME_CONFERENCE,
               AWAY_CONFERENCE,
               HOME_DIVISION,
               AWAY_DIVISION,
               HOME_POINTS,
               AWAY_POINTS)

# valid
valid_games = games_raw %>%
        filter(SEASON > 2000) %>%
        filter(SEASON <= 2010) %>%
        arrange(START_DATE) %>%
        select(GAME_ID, 
               SEASON, 
               WEEK,
               SEASON_TYPE,
               START_DATE,
               GAME_DATE,
               NEUTRAL_SITE, 
               HOME_TEAM,
               AWAY_TEAM,
               HOME_CONFERENCE,
               AWAY_CONFERENCE,
               HOME_DIVISION,
               AWAY_DIVISION,
               HOME_POINTS,
               AWAY_POINTS)

```


```{r get results on validation set, include=F}

elo_valid_results = foreach(i = 1:nrow(best_pars), .combine = bind_rows) %do% {
        
        # get par row
        pars = best_pars[i,]
        
        # compute elo
        elo_train = calc_elo_ratings(train_games,
                          home_field_advantage = pars$home_field_advantage,
                          reversion = pars$reversion,
                          k = pars$k,
                          v = pars$v,
                          verbose = F)
        
        # now use the ratings from this for the valid set
        elo_valid = calc_elo_ratings(valid_games,
                                     teams = elo_train$teams,
                                     team_seasons = elo_train$team_seasons,
                          home_field_advantage = pars$home_field_advantage,
                          reversion = pars$reversion,
                          k = pars$k,
                          v = pars$v,
                          verbose = F)
        
        # grab game outcomes
        out = elo_valid$game_outcomes %>%
                mutate(home_field_advantage = pars$home_field_advantage,
                       reversion = pars$reversion,
                       k = pars$k,
                       v = pars$v)
            
        # progress           
        cat("\r", i, "of", nrow(best_pars), "grid parameters completed");  flush.console()
        
        # return
        out 
        
}

```

Of these candidate parameters, which approach did the best for the next decade of games?

```{r next set of games}

elo_valid_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        mutate(yes = HOME_PROB) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        assess_metrics(truth = HOME_WIN,
                       yes,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        arrange(.estimate) %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()

# elo_valid_results %>%
#         mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
#                                                TRUE ~'no'))) %>%
#         mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
#                                              TRUE ~ 'no'),
#                                           levels = c("no", "yes"))) %>%
#         group_by(k, v, reversion, home_field_advantage) %>%
#         class_metrics(truth = HOME_WIN,
#                     estimate = HOME_PRED,
#                     event_level = 'second') %>%
#         mutate_if(is.numeric, round, 3) %>%
#         flextable() %>%
#         autofit()

```

Again, the results are pretty consistent, so I'll go with K at 35, V at 400, reversion at 10%, and home field advantage at 75.

```{r get elo ratings for trainng and valid, include=F}

selected_pars = tibble(k = 35,
                       v = 400,
                       reversion = 0.1,
                       home_field_advantage = 75)

rm(elo_train, 
   elo_valid)

# compute elo on train
elo_train = calc_elo_ratings(train_games,
                  home_field_advantage = selected_pars$home_field_advantage,
                  reversion = selected_pars$reversion,
                  k = selected_pars$k,
                  v = selected_pars$v,
                  verbose = T)
        
# now use the ratings from this as the initial ratings for the validation set
elo_valid = calc_elo_ratings(valid_games,
                             teams = elo_train$teams,
                             team_seasons = elo_train$team_seasons,
                  home_field_advantage = selected_pars$home_field_advantage,
                  reversion = selected_pars$reversion,
                  k = selected_pars$k,
                  v = selected_pars$v,
                  verbose = T)

```

# Elo Ratings and the Spread

All of the work so far has been focused on predicting the winner of games in terms of both the probability and the label. 

But, a nice feature of the Elo rating framework is that it also extends to predicting the difference in points between the two teams. Here is the relationship between the home team's point differential (Home Points - Away Points) and difference in pre game Elo ratings (Home Pre Elo - Away Pre Elo) from the seasons 1980 to 2000.

```{r we can also look at the predicted spread vs the actual, fig.height=8}

elo_train$game_outcomes %>%
        filter(SEASON >= 1980) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO + selected_pars$home_field_advantage - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        ggplot(., aes(x=HOME_ELO_DIFF, y=HOME_SCORE_DIFF))+
        geom_point(alpha=0.25)+
        facet_wrap(SEASON ~., ncol =5) +
        theme_phil()+
        geom_vline(xintercept =0,
                   linetype = 'dotted')+
        geom_hline(yintercept =0,
                   linetype = 'dotted')+
        geom_smooth(method = 'lm',
                    formula = 'y ~ x')+
        ggtitle("Predicted Point Spread via Pre Game Elo Scores")+
        xlab("Home Pregame Elo - Away Pre Game Elo")+
        ylab("Home Points - Away Points")

```

I'm omitting the first few years, as as the initial few seasons are being used to learn the Elo scores, but once we have a few seasons worth of data we start to see a pretty consistent relationship between the difference in Elo ratings and the game's point differential.

## Regression

What is this relationship? I'll regress the point spread on the difference in pregame Elo ratings for each of these individual years and over all years. Then, we'll look at the coefficent on the difference in Elo ratings.

```{r fit linear model to each year, include=F}

library(tidybayes)
library(broom.mixed)
library(mgcv)

# fit linear models at each year
suppressMessages({
nested_lm_points = elo_train$game_outcomes %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO + selected_pars$home_field_advantage - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        nest(-SEASON) %>%
        # now fit linear models, i'll do so using stan
        mutate(lm = map(data, ~ stan_glm(HOME_SCORE_DIFF ~ HOME_ELO_DIFF,
                             data = .x)))
})

# fit linear model after first few years in training
lm_points = elo_train$game_outcomes %>%
        filter(SEASON >=1975) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO + selected_pars$home_field_advantage - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        nest() %>%
        # now fit linear models, i'll do so using stan
        mutate(lm = map(data, ~ stan_glm(HOME_SCORE_DIFF ~ HOME_ELO_DIFF,
                             data = .x))) %>%
        mutate(tidied = map(lm, tidy, conf.int=T)) %>%
        mutate(glanced = map(lm, glance))

# fit linear model after first few years in training
points_model = elo_train$game_outcomes %>%
        filter(SEASON >=1975) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO + selected_pars$home_field_advantage - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        nest() %>%
        # now fit linear models, i'll do so using stan
        mutate(lm = map(data, ~ lm(HOME_SCORE_DIFF ~ HOME_ELO_DIFF,
                             data = .x))) %>%
        mutate(tidied = map(lm, tidy, conf.int=T)) %>%
        mutate(glanced = map(lm, glance)) %>%
        pluck("lm",1)

```

For the model trained on most of the training set of games, the estimated effect of HOME_ELO_DIFF was about .05. The intercept is about 0.5, which indicates the expected point differential in favor of the home team which the two teams have equivalent Elo ratings. This would be higher if we weren't accounting for home field advantage in the Elo ratings directly. The fact that we see a slight difference means that our Elo ratings suggests that we have almost, but not entirely, picked up the effect of home field advantage with our model.

The coefficient of .05 for the Elo diff means that a 20 point lead in Elo translates to about a point on the spread. This is linear, so a 200 point Elo lead translates to about a 10 point expected margin of victory.

```{r look at lm points}

lm_points %>%
        select(tidied) %>%
        unnest(tidied) %>%
        mutate_if(is.numeric, round, 2) %>%
        flextable() %>%
        autofit

```

This effect has changed a bit over time, which we can see by looking at model fit to each season. But it's been hovering around .05 for the time period shown here.

```{r tidy and plot the effect}

# plot the coef
nested_lm_points %>%
        mutate(tidied = map(lm, tidy, conf.int=T)) %>%
        select(SEASON, tidied) %>%
        unnest(tidied) %>%
        filter(term == 'HOME_ELO_DIFF') %>%
        ggplot(., aes(x=SEASON,
                      y=estimate,
                      ymin = conf.low,
                      ymax = conf.high))+
        geom_point()+
        geom_pointrange()+
        theme_phil()+
        geom_hline(yintercept = 0,
                   linetype = 'dashed',
                   lwd = 1.1)+
        geom_hline(yintercept = 0.05,
                   linetype = 'dotted',
                   color ='grey60',
                   lwd = 1.1)

```

This isn't to say that a 200 point lead on Elo means a 10 point spread is suddenly a sure thing. We can look at the standard deviation of the residuals (sigma) from each model to get a sense of how far off, on average, predictions for the point differential were. Generally, sigma was around 16, meaning the predictions were off by 16 points *on average*.

```{r look at the uncertainty}

# show sigma for model trained on multiple seasons
lm_points %>%
        select(glanced) %>%
        unnest(glanced) %>%
        mutate_if(is.numeric, round, 2) %>%
        flextable() %>%
        autofit

```

That number seems laughably high, but it's skewed somewhat by the model being *really off* for certain games. The in sample mean absolute error was closer to 13 points, with an Rsquared of around .38.

```{r look at the distribution of predicted vs actual}

lm_points %>%
        mutate(fitted = map(lm, ~ predict(.x))) %>%
        select(data, fitted) %>%
        unnest() %>%
        mutate(resid = round(fitted,0) - HOME_SCORE_DIFF) %>%
        ggplot(., aes(x=fitted, y=HOME_SCORE_DIFF))+
        geom_point(alpha = 0.2)+
        theme_phil()+
        geom_abline(slope =1, 
                    intercept=0)+
        geom_vline(xintercept = 0,
                   linetype = 'dashed')+
        geom_hline(yintercept=0,
                   linetype = 'dashed')+
        stat_cor(
                  aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")), 
                  label.x = 3,
                  p.accuracy = .01)
        
```

```{r mean absolute error}

lm_points %>%
        mutate(fitted = map(lm, ~ predict(.x))) %>%
        select(data, fitted) %>%
        unnest() %>%
        mutate(resid = round(fitted,0) - HOME_SCORE_DIFF) %>%
        yardstick::mae(truth = HOME_SCORE_DIFF,
                       estimate = fitted) %>%
        mutate_if(is.numeric, round, 2) %>%
        flextable() %>%
        autofit()

```

## Simulating the Margin of Victory

What's the point of this? In addition to using Elo ratings to predict the outcomes of games, we can also use them to simulate the expected point differential. I'll use one game as an example illustrate.

Let's go to a week 1 game in 2001: Wisconsin vs Virginia. After adjusting for home team advantage, Wisconsin was the clear favorite going into this game, with around a 200 point advantage in pregame Elo. As the home team, this means Wisconsin is about a 10.5 point favorite according to our model (home team advantage). That's the point estimate, but we can simulate from the model a few thousand times and plot the distribution of the simulations to see the uncertainty around the estimate.

```{r predict the point differential for one game in , fig.height=4}

# get model object out
lm_train_points = lm_points %>%
        pluck("lm", 1)

# get sample game
sample_game = elo_valid$game_outcomes %>% 
        filter(GAME_ID == 212370275) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO + selected_pars$home_field_advantage - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS)

# simulate this - add predicted draws
sample_sims = sample_game %>%
        add_predicted_draws(lm_train_points)

# plot the simulations
sample_sims %>%
        # mutate(HOME_WIN = case_when(sim > 0 ~ 'yes',
        #                             TRUE ~ 'no')) %>%
        ggplot(., aes(x=.prediction))+
        geom_histogram(bins = 100)+
        geom_vline(xintercept = sample_game$HOME_SCORE_DIFF,
                   lwd = 1.1,
                   color = 'red')+
        geom_vline(xintercept = 0,
                   linetype = 'dotted')+
        theme_phil()+
        xlab("Simulated Point Differential")

# # show distribution
# sample_sims %>%
#         median_qi(.width =c(.5, .8)) %>%
#         select(.prediction, .lower, .upper, .width) %>%
#         mutate_if(is.numeric, round, 1) %>%
#         flextable() %>%
#         autofit()
        

```

The median prediction was about 14 points and the 50% prediction interval for the game ranged from about 4 to 26 while the 80% interval ranged from about -6 to 36. 

What was the actual score? Wisconsin 26, Virginia 17, for a point differential of 9.

So in this case, the model was pretty close, but the point is the uncertainty is generally quite high with our estimates. If we look at all games from week 1, we can see that the model got some right and some wrong - the actual margin is in red, the 80% prediction interval is in black.

```{r use model to simulate 2001}

# get sample game
sample_games = elo_valid$game_outcomes %>% 
        filter(SEASON == 2001 & SEASON_TYPE == 'regular' & WEEK ==1) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO + selected_pars$home_field_advantage - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS)

# simulate this - add predicted draws
sample_sims = sample_games%>%
        add_predicted_draws(lm_train_points)

# 
sample_sims %>%
        median_qi(.width = c(.8)) %>%
        ggplot(aes(y = paste(HOME_TEAM,
                             AWAY_TEAM,
                             sep = "-"),
                   x = .prediction, xmin = .lower, xmax = .upper))+
        geom_pointinterval(position = position_dodge(width = .3))+
        geom_point(data = sample_sims %>%
                           median_qi(.width = c(0.5, .8)),
                   aes(x = HOME_SCORE_DIFF),
                   size = 4,
                   color = 'red')+
        theme_phil()+
        geom_vline(xintercept = 0,
                   linetype = 'dashed')+
        ylab("Game")+
        xlab("Home Points - Away Points")
#         
```

That's an 80% prediction interval. How many times did the actual score fall within that interval?
 
```{r count in interval}

sample_sims %>%
        median_qi(.width = c(0.8)) %>% 
        select(HOME_SCORE_DIFF, .prediction, .lower, .upper) %>% 
        mutate(.interval = '80%') %>%
        mutate(in_interval = HOME_SCORE_DIFF >=.lower & .prediction <=.upper) %>% 
        group_by(.interval, in_interval) %>% 
        count() %>%
        ungroup() %>%
        mutate(prop = n /sum(n)) %>%
        mutate_if(is.numeric, round, 2)

```
A little over 90% of the time.

We can do this for the entire season and look to see how one simulation looks for the whole season.

```{r look at sims for 2001 season, fig.height=8}

set.seed(1)
elo_valid$game_outcomes %>% 
        filter(SEASON == 2001) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO + selected_pars$home_field_advantage - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        add_predicted_draws(lm_train_points, ndraws = 1) %>%
        mutate(.pred = round(.prediction, 0)) %>%
        mutate(HOME_SCORE_DIFF_char = case_when(HOME_SCORE_DIFF > 0 ~ paste("+", HOME_SCORE_DIFF, sep=""),
                                    HOME_SCORE_DIFF == 0 ~ paste("Tie"),
                                    HOME_SCORE_DIFF < 0 ~ paste(HOME_SCORE_DIFF, sep=""))) %>%
        mutate(sim_char = case_when(.pred > 0 ~ paste("+", .pred, sep=""),
                                    .pred == 0 ~ paste("Tie"),
                                    .pred < 0 ~ paste(.pred, sep="")),
               GAME = paste(HOME_TEAM, " ", AWAY_TEAM, "\n",
                            "Actual: ", HOME_TEAM, " ", HOME_SCORE_DIFF_char, "\n",
                            "Sim: ", HOME_TEAM, " ", sim_char, sep="")) %>%
        mutate(CORRECT = case_when(.pred >0 & HOME_SCORE_DIFF >0 ~ 'yes',
                                   .pred <0 & HOME_SCORE_DIFF <0 ~ 'yes',
                                   TRUE ~ 'no')) %>%
        ggplot(., aes(x=HOME_SCORE_DIFF,
                      color = CORRECT,
                   #   label = GAME,
                      y=.pred))+geom_point()+
        facet_wrap(SEASON + WEEK ~., ncol =4)+
     #   geom_text(check_overlap=T, 
               #   vjust=-0.5, size=2)+
        theme_phil()+
       scale_color_manual(values = c("red", "blue"))+
        geom_vline(xintercept =0,
                   linetype = 'dotted')+
        geom_hline(yintercept = 0,
                   linetype = 'dotted')+
        xlab("Simulated Point Diff")+
        ylab("Actual Point Diff")


```

```{r rm elements computed so far}

rm(elo_returned,
   elo_returned_v1,
   elo_returned_v2,
   elo_returned_V3)

```


# Simulating an Entire Season

Okay, so at this point we have everything we need to try out simulating entire seasons. 

To simulate a season, we simulate each game based on the pre game Elo scores for each team. Say that Team A has a 67% chance of winning based on the two teams Elo scores. In that case, I can simulate the game by pulling from a Bernoulli distribution where p =.67. Based on that result, I then compute the outcome and update the ratings. I repeat this process N times for every game, meaning that the Elo ratings are themselves updated on the basis of each simulated result. If I simulate a season 1000 times, I get a distribution of simulated outcomes for every game and team, which I can then use to obtain probabilities of specific events. 

The snag with this approach is that I'm using a margin of victory multiplier to update ratings after each game. To use this approach, I need to not only simulate the winner, but also the point margin of each game.  This is where the model to predict the point differential comes in.

I can simulate the outcome and margin of each game by pulling one simulation from the points model, then use this to update the ratings. 

```{r load sim elo ratings func}

source(here::here("functions", "simulateX.R"))
source(here::here("functions", "sim_game_margin.R"))
source(here::here("functions", "sim_elo_ratings.R"))

# show functions
# sim_game_margin
# sim_elo_ratings

```

## The 2001 Season

To show this works, I'll simulate the entire 2001 season starting with the Elo ratings we computed on games from 1970-2000.

```{r set up parallel, include=F}

library(future.apply)
plan(multisession, workers = 7)

```

```{r simulate 2001 season}

# regress point differential on elo differential
# fit linear model after first few years in training
points_model = elo_train$game_outcomes %>%
        filter(SEASON >=1975) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO + selected_pars$home_field_advantage - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        nest() %>%
        # now fit linear models, i'll do so using stan
        mutate(lm = map(data, ~ lm(HOME_SCORE_DIFF ~ HOME_ELO_DIFF,
                                   data = .x))) %>%
        mutate(tidied = map(lm, tidy, conf.int=T)) %>%
        mutate(glanced = map(lm, glance)) %>%
        pluck("lm",1)

set.seed(1999)
sim_seasons = future_replicate(1000,
                               sim_elo_ratings(valid_games %>%
                                                       filter(SEASON == 2001),
                                               teams = elo_train$teams,
                                               team_seasons = elo_train$team_seasons,
                                               home_field_advantage = selected_pars$home_field_advantage,
                                               reversion = selected_pars$reversion,
                                               k = selected_pars$k,
                                               v = selected_pars$v,
                                               verbose=F,
                                               ties =F,
                                               points_model = points_model)
)

# get outcomes into data frames
sim_team_outcomes = rbindlist(sim_seasons['team_outcomes',],
                              idcol = T)

sim_game_outcomes = rbindlist(sim_seasons['game_outcomes',],
                              idcol = T)

```

```{r define functions for plotting season sims}

source(here::here("functions", "plot_elo_conference_season.R"))
source(here::here("functions", "plot_elo_team_season.R"))
source(here::here("functions", "plot_wins_conference_season.R"))
source(here::here("functions", "table_wins_conference_season.R"))
source(here::here("functions", "table_wins_team_season.R"))

```

### Simulated Elo Paths

I'll plot the results of these simulations for every team in the Big 12. This plot shows the simulated postgame Elo ratings for each team in this conference for the 2001 season, where each line is one possible path that the team's season might take based on the simulated results.

```{r plot big 12 elo for 2001}

plot_elo_conference_season(sim_team_outcomes,
                           games = games_raw,
                           conference = 'Big 12',
                           season = 2001)

```

This ends up painting a pretty interesting picture of how each team's season might unfold. In each simulation of a season, a team's Elo rating is updated after each game, which affects how likely they are to win their next game, which affects their Elo rating for the next game, and so on. When we see many lines overlap, this is because there is a typical path based on a team's schedule for how their season is likely to unfold.

Let's zoom into the simulations for Kansas State's 2001 season. I'll plot Kansas State's pregame Elo rating in every simulation across each game in this season. Going into the season, based on their ratings from the last season, they were slight favorites on the road at USC, but it could easily go the other way. The result of this game sends KState on two pretty different paths. Regardless of how they played at USC, they're likely to win against New Mexico State (and only marginally increase their Elo rating). But, in the simulations where they beat USC, they were more likely to beat Oklahoma; when they lost to USC, they were less likely to beat Oklahoma.

After each of these games, we would be able to update our Elo rating and simulate the rest of the season. But before any games happen, summing up the simulations gives us our preseason probabilities for each matchup.

```{r look at K state 2001}

plot_elo_team_season(sim_team_outcomes = sim_team_outcomes,
                     games = games_raw,
                     team = 'Kansas State',
                     season = 2001,
                     alpha = 0.25)

```

Interestingly, Kansas State had a poor year in 2001 relative to our expectations. Prior to the season, they were favorites to win 9 out of their 11 games. We'll dive into win totals next to take a look at this.

```{r looking now at our expected record}

table_wins_team_season(sim_team_outcomes,
                       games_raw,
                       'Kansas State',
                       season = 2001)

```


### Win Totals

The simulated Elo ratings can tell us the general expected story for a team's season. But what we probably care about is just the wins. I'll make a similar plot but show each team's simulated win count over the course of a season.

```{r show the win totals for 2001}

# wins
sim_team_outcomes %>%
        filter(SEASON == 2001) %>%
        filter(CONFERENCE == 'Big 12') %>%
        filter(SEASON_TYPE == 'regular') %>%
        mutate(WIN = case_when(SIM_MARGIN > 0 ~ 1,
                               TRUE ~ -1)) %>%
        arrange(.id, SEASON, TEAM, GAME_DATE) %>%
        group_by(.id, SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(wins = cumsum(WIN)) %>%
        left_join(., teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                  by = c("TEAM")) %>%
        ggplot(., aes(x=GAME,
                      color = name,
                      group = .id,
                      y = wins)) +
        geom_line(position=position_jitter(w=0.0, h=0.2),
                  alpha = 0.01,
                  lwd = 1.04)+
        # geom_line(stat = 'smooth',
        #           method = 'loess',
        #           formula = 'y ~ x',
        #           span = 0.25,
        #           alpha = 0.08)+
        theme_phil()+
        facet_wrap(paste(TEAM, SEASON) ~.,
                   ncol =4)+
        scale_color_teams(name = "TEAM")+
        guides(color = 'none')+
        ggtitle("Simulated Win Differential by Team over Season",
                subtitle = str_wrap("Each line is one result from simulating a team's season. Displaying 1000 simulations.", 120))+
        theme(plot.title = element_text(hjust = 0.5),
              plot.subtitle =  element_text(hjust = 0.5),
              strip.text.x = element_text(size = 10))+
        xlab("Game")+
        ylab("Win Total")+
        geom_hline(yintercept =0,
                   linetype = 'dashed')+
        coord_cartesian(ylim = c(-12, 12))

```

For win totals, we probably mostly care about showing the distribution of wins for each team at the end of the season. This is the distribution of wins in every simulation.

```{r distribution of wins by 2001 season}

plot_wins_conference_season(sim_team_outcomes,
                            games_raw,
                            conference='Big 12',
                            season=2001)

```

We can put all this info into one table by counting the number of simulations in which each team fell into number of wins. That will give us our probabilities for each team's win total going into this season.

```{r probability table for 2001 big 12}

table_wins_conference_season(sim_team_outcomes = sim_team_outcomes,
                             games = games_raw,
                             conference = 'Big 12',
                             season =  2001)

```





Man, Nebraska really was given that much of a chance of running the table?

```{r look at nebraska 2001}

table_wins_team_season(sim_team_outcomes,
                       games_raw,
                       team = "Nebraska",
                       season = 2001)

```

Anyway, we can look at expected wins for all teams, not just the Big 12...

```{r 2001 results all teams table}

source(here::here("functions", "table_wins_season.R"))

table_wins_season(sim_team_outcomes = sim_team_outcomes,
                  games = games_raw,
                  season = 2001)

```

### Pre Season Results

How well did this approach do in predicting the 2001 season at the start of the season? I'll examine this first at the game level, looking at how the simulated results did in predicting both the probability and the outcome in every matchup for this season. I'll assess probabilities using the log loss and the outcomes using accuracy.

```{r look at results 2001 games}

assess_metrics = metric_set(yardstick::mn_log_loss,
                           yardstick::accuracy)

sim_game_assessment = sim_game_outcomes %>%
        mutate(HOME_PRED = case_when(HOME_SIM_MARGIN >0 ~ 1,
                                     TRUE ~ 0)) %>%
        group_by(GAME_ID, SEASON, WEEK, GAME_DATE, NEUTRAL_SITE, HOME_TEAM, AWAY_TEAM, HOME_POINTS, AWAY_POINTS) %>%
        summarize(HOME_PROB = sum(HOME_PRED) / n(),
                  PRED_MARGIN = mean(HOME_SIM_MARGIN),
                  .groups = 'drop') %>%
        mutate(AWAY_PROB = 1-HOME_PROB) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >=.5 ~ 'yes',
                                       TRUE ~ 'no'))) %>%
        mutate(PRED_CORRECT = case_when(HOME_WIN == HOME_PRED ~ 'yes',
                                        TRUE ~ 'no'))

null_game_assessment = sim_game_outcomes %>%
        mutate(HOME_PRED =1) %>%
        group_by(GAME_ID, SEASON, WEEK, GAME_DATE, NEUTRAL_SITE, HOME_TEAM, AWAY_TEAM, HOME_POINTS, AWAY_POINTS) %>%
        summarize(HOME_PROB = sum(HOME_PRED) / n(),
                  .groups = 'drop') %>%
        mutate(AWAY_PROB = 1-HOME_PROB) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >=.5 ~ 'yes',
                                       TRUE ~ 'no'))) %>%
        mutate(HOME_PROB = .6)

bind_rows(
        sim_game_assessment %>%
                mutate(method = 'elo'),
        null_game_assessment %>%
                mutate(method = 'home wins')) %>%
        left_join(., games_raw %>%
                          select(GAME_ID, SEASON_TYPE, CONFERENCE_CHAMPIONSHIP),
                  by = c("GAME_ID")) %>%
        filter(SEASON_TYPE == 'regular' & CONFERENCE_CHAMPIONSHIP == F) %>%
        mutate(yes = HOME_PROB) %>%
        group_by(SEASON, method) %>%
        assess_metrics(truth = HOME_WIN,
                       yes,
                       estimate = HOME_PRED,
                       event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        mutate(SEASON = factor(SEASON)) %>%
        flextable() %>%
        autofit()

```

Overall, we were a little over 70% accurate in predicting all games from the start of the season with a log loss in the .54 range. In both cases, this is an improvement over using a simple heuristic such as the home team always wins.

```{r check calibration}

sim_game_assessment %>%
        mutate(HOME_PROB_BIN = plyr::round_any(HOME_PROB, .05, floor)) %>%
        group_by(SEASON, HOME_PROB_BIN, HOME_WIN) %>%
        count() %>%
        spread(HOME_WIN, n) %>%
        ungroup() %>%
        mutate_if(is.numeric, replace_na, 0) %>%
        mutate(games = no + yes) %>%
        mutate(observed_home_prop = yes / games) %>%
        ggplot(., aes(x=HOME_PROB_BIN,
                      size = games,
                      y=observed_home_prop))+
       # facet_wrap(SEASON ~.)+
        geom_point()+
        theme_phil()+
        geom_abline(slope = 1,
                    intercept=0,
                    linetype = 'dashed')+
        geom_smooth(method = 'loess',
                    formula = 'y ~ x',
                    color = 'blue',
                    guides = 'none',
                    show.legend = F)+
        xlab("Predicted Probability of Home Win")+
        ylab("Observed Probability of Home Win")+
        annotate("text",
                 x = 0.2,
                 y = 0.8,
                 size = 6,
                 label = str_wrap("Home Team More Likely to Win Than Predicted",30))+
        annotate("text",
                 x = 0.8,
                 y = 0.2,
                 size = 6,
                 label = str_wrap("Home Team Less Likely to Win Than Predicted",30))+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'))+
        theme(plot.title = element_text(hjust = 0.5,
                                                size = 16),
                      plot.subtitle =  element_text(hjust = 0.5),
                      strip.text.x = element_text(size = 10,
                                                  hjust = 0.5))+
        ggtitle("Calibration of Pregame Probabilities for 2001 Season")


```

We can also break this down by each week. As we would expect, in simulating the entire season, our pre season accuracy drops further into the season we go.

```{r look at results week by week for 2001}

# log loss by week
sim_game_assessment %>%
                mutate(method = 'elo') %>%
        mutate(yes = HOME_PROB) %>%
        group_by(SEASON, WEEK) %>%
        mutate(GAMES= n_distinct(GAME_ID)) %>%
        group_by(SEASON, WEEK, GAMES) %>%
        assess_metrics(truth = HOME_WIN,
                       yes,
                       estimate = HOME_PRED,
                       event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        mutate(SEASON = factor(SEASON)) %>%
        spread(.metric, .estimate) %>%
        select(-.estimator) %>%
        flextable() %>%
        autofit()

```

Next, we can look at how the model did in predicting each team's win totals. 

```{r get win totals}

mode_func <- function(x) {
        ux <- unique(x)
        ux[which.max(tabulate(match(x, ux)))]
}

# get results for all teams
sim_team_wins = sim_team_outcomes %>%
        left_join(., games_raw %>%
                          select(GAME_ID, CONFERENCE_CHAMPIONSHIP),
                  by = c("GAME_ID")) %>%
        filter(SEASON_TYPE == 'regular') %>%
        filter(CONFERENCE_CHAMPIONSHIP == F) %>%
        filter(DIVISION == 'fbs') %>%
        left_join(., valid_games %>%
                          mutate(WINNER = case_when(HOME_POINTS > AWAY_POINTS ~ HOME_TEAM, TRUE ~ AWAY_TEAM)) %>%
                          select(GAME_ID, WINNER),
                  by = c("GAME_ID")) %>%
        mutate(WIN = case_when(TEAM == WINNER ~ 1,
                               TRUE ~ 0)) %>%
        mutate(PRED_WIN = case_when(SIM_MARGIN >0 ~ 1,
                                    TRUE ~ 0)) %>%
        group_by(SEASON, CONFERENCE, TEAM, .id) %>%
        summarize(pred_wins = sum(PRED_WIN),
                  actual_wins = sum(WIN),
                  .groups = 'drop') %>%
        group_by(SEASON, CONFERENCE, TEAM, actual_wins) %>%
        summarize(mean_wins= mean(pred_wins),
                  mode_wins = mode_func(pred_wins),
                  .groups = 'drop')

# get result if each team won every game in which they were favored

# overall
sim_team_wins %>% 
        group_by(SEASON) %>%
        yardstick::rmse(mean_wins,
                        actual_wins) %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(SEASON = factor(SEASON)) %>%
        select(-.estimator) %>%
        flextable() %>%
        autofit()

# by conference
sim_team_wins %>% 
        group_by(SEASON, CONFERENCE) %>%
        yardstick::rmse(mean_wins,
                        actual_wins) %>%
        mutate_if(is.numeric, round, 2) %>%
        select(-.estimator) %>%
        arrange(.estimate) %>%
        mutate(SEASON = factor(SEASON)) %>%
        flextable() %>%
        autofit()
# 
# # by team
# sim_team_wins %>% 
#         group_by(SEASON, TEAM) %>%
#         yardstick::rmse(mean_pred,
#                         actual) %>%
#         mutate_if(is.numeric, round, 2) %>%
#         arrange(.estimate)

# plot of overall
sim_team_wins %>%
        ggplot(., aes(x=mean_wins,
                      label = TEAM,
                      y=actual_wins))+
        geom_jitter(size=3,
                    width = 0.1,
                    height = 0.1)+
        geom_abline(slope =1,
                    intercept = 0)+
        geom_text_repel()+
        theme_phil()+
        coord_cartesian(xlim = c(0, 13),
                        ylim = c(0, 13))+
        theme(strip.text.x = element_text(size = 12))+
        xlab("Predicted Wins")+
        ylab("Actual Wins")+
        stat_cor(
                aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")), 
                label.x = 3,
                p.accuracy = .01,
                color = 'blue')+
        geom_vline(xintercept = 6,
                   linetype = 'dotted')+
        geom_hline(yintercept = 6,
                   linetype = 'dotted')

```

On average, we were off by about 2 wins - a non trivial amount - and the performance was quite a bit different by conference and team. 

### Mid Season Results

So far, we've looked at how well the model did in simulating each team's season. But I'll be planning to run this after every week of games, which will result in updated ratings, which will affect the rest of the season's simulations.

```{r mid season results}

weeks = unique(valid_games$WEEK)

# run elo for specified week of the 2001 season to update our elo ratings with real results
elo_update = calc_elo_ratings(valid_games %>%
                                      filter(SEASON == 2001) %>%
                                      filter(WEEK <=8),
                              teams = elo_train$teams,
                              team_seasons = elo_train$team_seasons,
                              home_field_advantage = pars$home_field_advantage,
                              reversion = pars$reversion,
                              k = pars$k,
                              v = pars$v,
                              verbose = F)

# then, simulate the rest of the way
set.seed(1)
update_sim_seasons = future_replicate(100,
                                      sim_elo_ratings(valid_games %>%
                                                              filter(SEASON == 2001) %>%
                                                              filter(WEEK > 8),
                                                      teams = elo_update$teams,
                                                      team_seasons = elo_update$team_seasons,
                                                      home_field_advantage = selected_pars$home_field_advantage,
                                                      reversion = selected_pars$reversion,
                                                      k = selected_pars$k,
                                                      v = selected_pars$v,
                                                      verbose=F,
                                                      ties =F,
                                                      points_model = points_model))

# get outcomes into data frames
update_team_outcomes = rbindlist(update_sim_seasons['team_outcomes',],
                                 idcol = T)

# now get the updated
update_game_outcomes = rbindlist(update_sim_seasons['game_outcomes',],
                                 idcol = T)

```

This means teams with surprise wins/losses will have their expectations for the rest of the season changed.

Take a look at a team like Maryland that had a bunch of tossups on its schedule early into the season, with two games at the end of the year in which they were expected to lose.

```{r illinois example 2001}

# show pre season vs updated
table_wins_team_season(sim_team_outcomes,
                       games = games_raw,
                       team = 'Maryland',
                       2001) %>%
        set_caption("Pre Season")

```

But, Maryland managed to run the table through their first 8 weeks of the season, so how did their win probabilities for their later games change after week 8?
        
```{r show illinois updated 2001}

# show updated
table_wins_team_season(update_team_outcomes,
                       games = games_raw,
                       team = 'Maryland',
                       2001) %>%
        set_caption("Updated After Week 8")

```

Nebraska was the heavy favorite at the beginning of the season playing at Colorado on the last week of the season, how did the odds of that game change over the course of the season as Colorado became highly rated?

```{r nebraska colorado 2001}

# run elo for specified week of the 2001 season to update our elo ratings with real results
elo_update = calc_elo_ratings(valid_games %>%
                                      filter(SEASON == 2001) %>%
                                      filter(WEEK <=13),
                              teams = elo_train$teams,
                              team_seasons = elo_train$team_seasons,
                              home_field_advantage = pars$home_field_advantage,
                              reversion = pars$reversion,
                              k = pars$k,
                              v = pars$v,
                              verbose = F)

# then, simulate the rest of the way
set.seed(2)
update_sim_seasons = future_replicate(100,
                                      sim_elo_ratings(valid_games %>%
                                                              filter(SEASON == 2001) %>%
                                                              filter(WEEK > 13),
                                                      teams = elo_update$teams,
                                                      team_seasons = elo_update$team_seasons,
                                                      home_field_advantage = selected_pars$home_field_advantage,
                                                      reversion = selected_pars$reversion,
                                                      k = selected_pars$k,
                                                      v = selected_pars$v,
                                                      verbose=F,
                                                      ties =F,
                                                      points_model = points_model))

# get outcomes into data frames
update_team_outcomes = rbindlist(update_sim_seasons['team_outcomes',],
                                 idcol = T)

# now get the updated
update_game_outcomes = rbindlist(update_sim_seasons['game_outcomes',],
                                 idcol = T)

# 
table_wins_team_season(update_team_outcomes,
                       games = games_raw,
                       'Nebraska',
                       2001) %>%
        set_caption("After Week 13")

```

Still favored to win, but by a bit less - and the model was still wrong, as Nebraska was killed in that game, but it's less wrong than at the start.

### Mid Season Changes

We can do this for every week within a season to see how a team's rating and expected end of season win totals change throughout the year.

```{r sims after each week}

weeks = unique(valid_games %>%
                       filter(SEASON == 2001) %>%
                       pull(WEEK))

# first, simulate entire season
sim_seasons = future_replicate(100,
                               sim_elo_ratings(valid_games %>%
                                                       filter(SEASON == 2001),
                                               teams = elo_train$teams,
                                               team_seasons = elo_train$team_seasons,
                                               home_field_advantage = selected_pars$home_field_advantage,
                                               reversion = selected_pars$reversion,
                                               k = selected_pars$k,
                                               v = selected_pars$v,
                                               verbose=F,
                                               ties =F,
                                               points_model = points_model))

# get outcomes into data frames
update_team_outcomes = rbindlist(sim_seasons['team_outcomes',],
                                 idcol = T) %>%
        mutate(SIM_WEEK = 0)

# now get the updated
update_game_outcomes = rbindlist(sim_seasons['game_outcomes',],
                                 idcol = T) %>%
        mutate(SIM_WEEK = 0)

preseason_sims = list("sim_team_outcomes" = update_team_outcomes,
                      "sim_game_outcomes" = update_game_outcomes,
                      "week" = 0)

rm(update_team_outcomes,
   update_game_outcomes)

# now loop over the rest
midseason_sims = foreach(i = 1:length(weeks)-1, .errorhandling = 'pass') %do% {
        
        # run elo for specified week of the 2001 season to update our elo ratings with real results
        elo_update = calc_elo_ratings(valid_games %>%
                                              filter(SEASON == 2001) %>%
                                              filter(WEEK <= weeks[i]),
                                      teams = elo_train$teams,
                                      team_seasons = elo_train$team_seasons,
                                      home_field_advantage = selected_pars$home_field_advantage,
                                      reversion = selected_pars$reversion,
                                      k = selected_pars$k,
                                      v = selected_pars$v,
                                      verbose = F)
        
        # then, simulate the rest of the way
        update_sim_seasons = future_replicate(100,
                                              sim_elo_ratings(valid_games %>%
                                                                      filter(SEASON == 2001) %>%
                                                                      filter(WEEK > weeks[i]),
                                                              teams = elo_update$teams,
                                                              team_seasons = elo_update$team_seasons,
                                                              home_field_advantage = selected_pars$home_field_advantage,
                                                              reversion = selected_pars$reversion,
                                                              k = selected_pars$k,
                                                              v = selected_pars$v,
                                                              verbose=F,
                                                              ties =F,
                                                              points_model = points_model))
        
        # get outcomes into data frames
        update_team_outcomes = rbindlist(update_sim_seasons['team_outcomes',],
                                         idcol = T) %>%
                mutate(SIM_WEEK = weeks[i])
        
        # now get the updated
        update_game_outcomes = rbindlist(update_sim_seasons['game_outcomes',],
                                         idcol = T) %>%
                mutate(SIM_WEEK = weeks[i])
        
        # return list
        out = list("sim_team_outcomes" = update_team_outcomes,
                   "sim_game_outcomes" = update_game_outcomes,
                   "week" = weeks[i])
        
        rm(update_sim_seasons,
           update_team_outcomes,
           update_game_outcomes)
        
        out
        
}

```


```{r can put these together}

midseason_game_outcomes = bind_rows(preseason_sims$sim_game_outcomes,
                                    rbindlist(lapply(midseason_sims,'[[','sim_game_outcomes')))

midseason_team_outcomes = bind_rows(preseason_sims$sim_team_outcomes,
                                    rbindlist(lapply(midseason_sims,'[[','sim_team_outcomes')))

```

Also, looking at predictions that are updated within the season allows me to check on how well the model is doing by how far out it is predicting. As we would expect, our accuracy is highest in predicting games one week out and trailing off after that.

```{r midseason sims}

midseason_game_assessment = midseason_game_outcomes %>%
        mutate(HOME_PRED = case_when(HOME_SIM_MARGIN >0 ~ 1,
                                     TRUE ~ 0)) %>%
        group_by(GAME_ID, SEASON, WEEK, GAME_DATE, SIM_WEEK, NEUTRAL_SITE, HOME_TEAM, AWAY_TEAM, HOME_POINTS, AWAY_POINTS) %>%
        summarize(HOME_PROB = sum(HOME_PRED) / n(),
                  PRED_MARGIN = mean(HOME_SIM_MARGIN),
                  .groups = 'drop') %>%
        arrange(GAME_DATE) %>%
        mutate(AWAY_PROB = 1-HOME_PROB) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                           TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >=.5 ~ 'yes',
                                            TRUE ~ 'no'))) %>%
        mutate(PRED_CORRECT = case_when(HOME_WIN == HOME_PRED ~ 'yes',
                                        TRUE ~ 'no'))

midseason_game_assessment %>%
        mutate(HORIZON = WEEK - SIM_WEEK) %>%
        mutate(yes = HOME_PROB) %>%
        group_by(SEASON, HORIZON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, HORIZON, games) %>%
        assess_metrics(truth = HOME_WIN,
                       yes,
                       estimate = HOME_PRED,
                       event_level = 'second') %>%
        filter(.metric == 'accuracy') %>%
        ggplot(., aes(x=HORIZON,
                      y=.estimate))+
        facet_wrap(SEASON + .metric ~.)+
        geom_point(aes(size = games))+
        geom_line(stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.8)+
        theme_phil()+
        theme(legend.title = element_text())
# mutate_if(is.numeric, round, 2) %>%
# spread(.metric, .estimate) %>%
# select(-.estimator) %>%
# mutate(SEASON = factor(SEASON)) %>%
# flextable() %>%
# autofit()

```

Updating the simulations each week allows us to check in on how a team's expected win totals are changing after every week. Here are the simulated end of season win totals for each ACC team after weeks 3, 7, and 10. 

```{r checking in on simulations}

source(here::here("functions", "plot_forecast_wins_conference_season.R"))

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'ACC',
                                     week = 3)

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'ACC',
                                     week = 7)

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'ACC',
                                     week = 10)

```

Similarly, here's how the Big Ten changed as Wisconsin entered with the most expected wins, only to drop 2 of their first three games. By week 11 Illinois had completed its somewhat stunning charge to the top.

```{r big ten wisconsin midseason}

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'Big Ten',
                                     week = 1)

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'Big Ten',
                                     week = 3)

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'Big Ten',
                                     week = 7)

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'Big Ten',
                                     week = 11)

```

And if we look at the Big 12, we can see the rise of Colorado.

```{r look at big 12 in season midseason simus}

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'Big 12',
                                     week = 3)

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'Big 12',
                                     week = 7)

plot_forecast_wins_conference_season(actual_team_outcomes = elo_update$team_outcomes,
                                     midseason_sim_team_outcomes = midseason_team_outcomes,
                                     games = games_raw,
                                     season = 2001,
                                     conference = 'Big 12',
                                     week = 12)

```

