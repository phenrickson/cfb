---
title: "Predicting Games via Expected Points and Elo"
author: "Phil Henrickson"
date: "6/7/2022"
output: 
  html_document:
    toc: TRUE #adds a Table of Contents
    theme: cerulean
    number_sections: TRUE #number your headings/sections
    toc_float: TRUE #let your ToC follow you as you scroll
    keep_md: no
    fig.caption: yes
    css: "styles.css"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = F,
                      error = F,
                      warning=F,
                      dev="png",
                      fig.width = 10,
                      fig.height = 6)

options(knitr.duplicate.label = "allow")

options(scipen=999)

```

```{r packages, include=F} 

source(here::here("scripts/load_packages.R"))
library(jsonlite)
library(forcats)
conflict_prefer("lag", "dplyr")

library(flextable)
set_flextable_defaults(theme_fun = theme_alafoli,
                       font.color = "grey10",
                       font.size=8,
                       padding.bottom = 6, 
                       padding.top = 6,
                       padding.left = 6,
                       padding.right = 6,
                       background.color = "white")

```

```{r connect to snowflake}

library(DBI)
library(odbc)
library(RODBC)
library(keyring)

# connect to snowflake
myconn <- DBI::dbConnect(odbc::odbc(),
                         "SnowflakeDSII",
                         Database = "CFB_DEMO",
                         warehouse = "DEMO_WH",
                         uid="phil.henrickson",
                         pwd=keyring::key_get("AE_Snowflake"))

```

```{r get games}

# teams raw
teams_raw = DBI::dbGetQuery(myconn,
                             paste('SELECT * FROM CFB_DEMO.CFD_RAW.TEAMS')) %>%
        as_tibble() %>%
        rename(TEAM = SCHOOL,
               TEAM_ID = ID,
               TEAM_MASCOT = MASCOT,
               TEAM_ABBREVIATION = ABBREVIATION)

# get games
games_raw = DBI::dbGetQuery(myconn,
                             paste('SELECT * FROM CFB_DEMO.CFD_RAW.ANALYSIS_GAMES')) %>%
        as_tibble() %>%
        mutate(CONFERENCE_GAME = case_when(CONFERENCE_GAME == 'true' ~ T,
                                           CONFERENCE_GAME == 'false' ~ F)) %>%
        mutate(GAME_DATE = as.Date(START_DATE)) %>%
        mutate(START_DATE = as_datetime(START_DATE)) %>%
        arrange(START_DATE) %>%
        group_by(SEASON, SEASON_TYPE, HOME_CONFERENCE) %>%
        mutate(LAST_GAME = START_DATE == max(START_DATE)) %>% 
        ungroup() %>%
        # logic for flagging conference championship games
        mutate(CONFERENCE_CHAMPIONSHIP = case_when(CONFERENCE_GAME == T & 
                                                           (HOME_CONFERENCE != 'FBS Independents') & 
                                                           (HOME_CONFERENCE != 'FCS Independents') &
                                                           !(HOME_CONFERENCE %in% 'Big Ten' & SEASON < 2014) &
                                                           !(HOME_CONFERENCE %in% 'Western Athletic') &
                                                           (NEUTRAL_SITE == T | 
                                                                    is.na(VENUE) |
                                                                    VENUE == 'Glass Bowl' |
                                                                    VENUE == 'Bright House Networks Stadium' |
                                                                    VENUE == 'Alamodome' |
                                                                    VENUE == 'Georgia Dome' |
                                                                    HOME_CONFERENCE == 'Sun Belt' |
                                                                    HOME_CONFERENCE == 'American Athletic') &
                                                           SEASON > 2000 &
                                                           SEASON_TYPE == 'regular' &
                                                           WEEK > 13 &
                                                           LAST_GAME == T ~ T,
                                                   TRUE ~ F)) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                    HOME_POINTS < AWAY_POINTS ~ 'no',
                                    HOME_POINTS == AWAY_POINTS ~ 'no'),
               levels = c("no", "yes"))) %>%
        mutate(HOME_DIVISION  = replace_na(HOME_DIVISION, "missing"),
               AWAY_DIVISION = replace_na(AWAY_DIVISION, "missing")) %>%
        left_join(., teams_raw %>%
                          select(TEAM, TEAM_ABBREVIATION) %>%
                          rename(HOME_TEAM = TEAM,
                                 HOME_TEAM_ABBR = TEAM_ABBREVIATION),
                  by = c("HOME_TEAM")) %>%
        left_join(., teams_raw %>%
                          select(TEAM, TEAM_ABBREVIATION) %>%
                          rename(AWAY_TEAM = TEAM,
                                 AWAY_TEAM_ABBR = TEAM_ABBREVIATION),
                  by = c("AWAY_TEAM"))

# get team conference mapping
team_conference_season = DBI::dbGetQuery(myconn,
                             paste('SELECT * FROM CFB_DEMO.CFD_RAW.ANALYSIS_TEAM_CONFERENCE_SEASON')) %>%
        as_tibble()

# make mapping for color
team_mapping = teams_raw %>%
        mutate(TEAM_ABBR = gsub(" ", "\\.", gsub("[[:punct:]]", "\\.", TEAM))) %>%
        select(TEAM, TEAM_ABBR, TEAM_ABBREVIATION, COLOR, ALT_COLOR) %>%
        mutate(COLOR = case_when(TEAM == 'Toledo' ~ '#003E7E',
                                 TRUE ~ COLOR))

# get colors
team_colors <- team_mapping %>%
        filter(!is.na(COLOR)) %>%
        pull(COLOR)

# set names
names(team_colors) = team_mapping %>%
        filter(!is.na(COLOR)) %>%
        pull(TEAM)

custom_color_teams <- scale_colour_manual(name = "TEAM", values = team_colors)
custom_fill_teams <- scale_fill_manual(name = "TEAM", values = team_colors)

```

```{r recruiting}

# recruiting
recruiting_teams_raw = DBI::dbGetQuery(myconn,
                             paste('SELECT * FROM CFB_DEMO.CFD_RAW.RECRUITING_TEAMS')) %>%
        as_tibble() %>%
        mutate(POINTS = as.numeric(POINTS)) %>%
        rename(RECRUITING_POINTS = POINTS)

# expand
library(TTR)
recruiting_teams = expand.grid(TEAM = unique(recruiting_teams_raw$TEAM),
             YEAR = seq(min(recruiting_teams_raw$YEAR),
                        max(recruiting_teams_raw$YEAR))) %>%
        left_join(., recruiting_teams_raw,
                  by = c("YEAR", "TEAM")) %>%
        mutate(RECRUITING_POINTS = replace_na(RECRUITING_POINTS, 0)) %>%
        filter(YEAR > 2001) %>%
        left_join(., team_conference_season,
                  by = c("YEAR"="SEASON", "TEAM")) %>%
        filter(DIVISION == 'fbs') %>%
        group_by(TEAM) %>%
        mutate(n = n()) %>%
        filter(n > 4) %>%
        group_by(TEAM) %>%
        select(-n) %>%
        mutate(EMA = EMA(RECRUITING_POINTS, n =4)) %>%
        # now fill with actual in the event that a team doesn't have yet have enough data
        mutate(EMA = case_when(is.na(EMA) & !is.na(RECRUITING_POINTS) ~ RECRUITING_POINTS,
                                  TRUE ~ EMA)) %>%
        # mutate_at(c("EMA", "WMA"),
        #           replace_na, 0) %>%
        arrange(TEAM, YEAR)  %>%
        mutate(SEASON = YEAR) %>%
        ungroup() %>%
        select(TEAM, SEASON, CONFERENCE, DIVISION, RANK, RECRUITING_POINTS, EMA)

# get what we need
recruiting = recruiting_teams %>%
        select(TEAM, SEASON, CONFERENCE, RECRUITING_POINTS, EMA) %>%
        rename(RECRUITING_SCORE = EMA)

```

```{r load functions}

# integer plotting function
int_breaks <- function(x, n = 5) {
  l <- pretty(x, n)
  l[abs(l %% 1) < .Machine$double.eps ^ 0.5] 
}

source(here::here("functions/theme_phil.R"))
source(here::here("functions/clean_plays_func.R"))
source(here::here("functions/make_time_features_func.R"))
source(here::here("functions/expected_points_func.R"))
source(here::here("functions/get_points_added_func.R"))
source(here::here("functions/get_drive_score_events.R"))
source(here::here("functions/assign_play_score_events.R"))


source(here::here("functions/get_expected_score.R"))
source(here::here("functions/get_new_elos.R"))
source(here::here("functions/calc_elo_ratings.R"))
source(here::here("functions", "simulateX.R"))
source(here::here("functions", "sim_game_margin.R"))
source(here::here("functions", "sim_elo_ratings.R"))
source(here::here("functions", "calc_elo_ratings.R"))
source(here::here("functions", "sim_elo_ratings.R"))
source(here::here("functions", "sim_rest_of_season.R"))

rm(a)

```

```{r load in scored and rankings}

# play level
load(here::here("data", "scored_plays.Rdata"))

# season level
load(here::here("data", "season_team_rankings.Rdata"))

# week level
load(here::here("data", "weekly_team_rankings.Rdata"))

```

# Team Efficiency Measures and Elo Ratings

So far, I have developed offensive and defensive efficiency measure for teams at the season weekly levels. These are useful as retrospective measures to understand a team's performance, but how well does it perform as a predictive measure? This notebook compares college football efficiency measures to Elo ratings for the purpose of predicting games. 

## Predicting Games with Elo Ratings

First, I'll compute Elo ratings using all games involving at least one FBS team back to 1869. This will give each team a pregame and postgame Elo rating, which we can use to predict both the winner and the margin of victory. I've tuned the Elo ratings for the college football setting by adding adjustments based on the home field advantage and the margin of victory, as well as mean reversion after each season.

```{r join recruiting with games}

# specify games to run function
games = games_raw %>%
        mutate(HOME_DIVISION = case_when(is.na(HOME_DIVISION) & SEASON < 1900 ~ 'fbs',
                                         TRUE ~ HOME_DIVISION)) %>%
        mutate(AWAY_DIVISION = case_when(is.na(AWAY_DIVISION) & SEASON < 1900 ~ 'fbs',
                                         TRUE ~ AWAY_DIVISION)) %>%
        mutate(FBS = HOME_DIVISION == 'fbs' | AWAY_DIVISION == 'fbs') %>%
        filter((FBS == T) | (FBS==F & SEASON < 1981)) %>%
        arrange(GAME_DATE) %>%
        select(GAME_ID, 
               SEASON, 
               WEEK,
               SEASON_TYPE,
               START_DATE,
               GAME_DATE,
               NEUTRAL_SITE, 
               HOME_TEAM,
               AWAY_TEAM,
               CONFERENCE_GAME,
               CONFERENCE_CHAMPIONSHIP,
               HOME_CONFERENCE,
               AWAY_CONFERENCE,
               HOME_DIVISION,
               AWAY_DIVISION,
               HOME_POINTS,
               AWAY_POINTS) %>%
        left_join(.,
                  recruiting %>%
                          select(SEASON,
                                 TEAM,
                                 RECRUITING_SCORE) %>%
                          rename(HOME_TEAM = TEAM,
                                 HOME_RECRUITING_SCORE = RECRUITING_SCORE),
                  by = c("SEASON",
                         "HOME_TEAM")) %>%
        left_join(.,
                  recruiting %>%
                          select(SEASON,
                                 TEAM,
                                 RECRUITING_SCORE) %>%
                          rename(AWAY_TEAM = TEAM,
                                 AWAY_RECRUITING_SCORE = RECRUITING_SCORE),
                  by = c("SEASON",
                         "AWAY_TEAM"))

```


```{r calc elo scores}

# set pars
selected_pars = tibble(k = 35,
                       v = 400,
                       reversion = 0.2,
                       home_field_advantage = 75)

# now get the elo ratings based on this training set
elo_games= calc_elo_ratings(games %>%
                                    filter(SEASON < 2022),
                            home_field_advantage = selected_pars$home_field_advantage,
                          reversion = selected_pars$reversion,
                          k = selected_pars$k,
                          v = selected_pars$v,
                          verbose = T)

# get fbs teams
teams = elo_games$team_outcomes %>%
        filter(SEASON == 2021) %>%
        filter(DIVISION == 'fbs') %>%
        distinct(TEAM) %>%
        arrange(TEAM)

# extract team elo ratings
team_elo_ratings = elo_games$team_outcomes

# game
game_elo_ratings = elo_games$game_outcomes

```

Elo ratings are the baseline upon which we would hope to improve by including additional data - they only take into account each team's pre game Elo rating and home field advantage, then update based on the outcome of the game and the margin of victory. This can be highly useful for predicting games and simulating seasons, but it doesn't include any information about *why* a particular team might be good. How well do Elo ratings do in predicting the winner of games over the time period 2007 to 2021?

```{r team elo ratings}

# examine elo ratings accuracy
elo_season_accuracy = game_elo_ratings %>%
        filter(SEASON > 2007 & SEASON < 2022) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >=.5 ~ 'yes',
                                       TRUE ~ 'no'))) %>%
        mutate(PRED_CORRECT = case_when(HOME_WIN == HOME_PRED ~ 'yes',
                                        TRUE ~ 'no')) %>%
        group_by(SEASON) %>%
        yardstick::accuracy(truth = HOME_WIN,
                            estimate = HOME_PRED,
                            event_level = 'second') %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(method = 'elo') 


elo_season_accuracy %>%
        select(SEASON, method, .metric, .estimate) %>%
        mutate(SEASON = factor(SEASON)) %>%
        flextable() %>%
        autofit()

```

Generally speaking, the Elo ratings do quite well in predicting games. They hover around the mid 70s accuracy in predicting the winner of games over this time period. But, there is some room for improvement, in that they tend to do better predicting regular season games than postseason games. The latter are going to be inherently harder to predict, but we see a pretty big dropoff in both accuracy and log loss in predicting the winner of postseason games.

```{r elo ratings on postseason games}

elo_season_type_accuracy = game_elo_ratings %>%
        filter(SEASON > 2007 & SEASON < 2022) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >=.5 ~ 'yes',
                                       TRUE ~ 'no'))) %>%
        mutate(PRED_CORRECT = case_when(HOME_WIN == HOME_PRED ~ 'yes',
                                        TRUE ~ 'no')) %>%
        group_by(SEASON_TYPE) %>%
        yardstick::accuracy(truth = HOME_WIN,
                            estimate = HOME_PRED,
                            event_level = 'second') %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(method = 'elo')

elo_season_type_accuracy %>%
        select(SEASON_TYPE, method, .metric, .estimate) %>%
        flextable() %>%
        autofit()


# game_elo_ratings %>%
#         filter(SEASON > 2007 & SEASON < 2022) %>%
#         mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
#                                        TRUE ~'no'))) %>%
#         mutate(HOME_PRED = factor(case_when(HOME_PROB >=.5 ~ 'yes',
#                                        TRUE ~ 'no'))) %>%
#         mutate(PRED_CORRECT = case_when(HOME_WIN == HOME_PRED ~ 'yes',
#                                         TRUE ~ 'no')) %>%
#         group_by(SEASON_TYPE) %>%
#         yardstick::mn_log_loss(truth = HOME_WIN,
#                             estimate = HOME_PROB,
#                             event_level = 'second') %>%
#         mutate_if(is.numeric, round, 2)
# 

```

By incorporating information within the season from play by play data, can we improve upon the baseline Elo ratings?

## Predicting Games with Efficiency Measures

I'll now look at how well we do if we use each team's overall efficiency measures.

```{r get games for overall efficiency}

weekly_ratings_and_outcomes = games %>%
        group_by(SEASON) %>%
        mutate(WEEK = case_when(SEASON_TYPE == 'postseason' ~ max(WEEK) + 1,
               TRUE ~ WEEK)) %>%
        ungroup()  %>%
        left_join(., weekly_team_rankings %>%
                          filter(TYPE == 'adjusted') %>%
                          mutate(WEEK = WEEK +1) %>%
                          select(SEASON, WEEK, TEAM, OVERALL, OFFENSE, DEFENSE) %>%
                          rename(HOME_TEAM = TEAM,
                                 HOME_POWER = OVERALL,
                                 HOME_OFF_POWER = OFFENSE,
                                 HOME_DEF_POWER = DEFENSE),
                  by = c("SEASON", "WEEK", "HOME_TEAM")) %>%
        arrange(SEASON, WEEK) %>%
        left_join(., weekly_team_rankings %>%
                          filter(TYPE == 'adjusted') %>%
                          mutate(WEEK = WEEK +1) %>%
                          select(SEASON, WEEK, TEAM, OVERALL, OFFENSE, DEFENSE) %>%
                          rename(AWAY_TEAM = TEAM,
                                 AWAY_POWER = OVERALL,
                                 AWAY_OFF_POWER = OFFENSE,
                                 AWAY_DEF_POWER = DEFENSE),
                  by = c("SEASON", "WEEK", "AWAY_TEAM")) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS - AWAY_POINTS) %>%
        mutate(HOME_OFF_POWER_DIFF = HOME_OFF_POWER - AWAY_OFF_POWER,
               HOME_DEF_POWER_DIFF = HOME_DEF_POWER - AWAY_DEF_POWER) %>%
        mutate(HOME_POWER_DIFF = (100*HOME_POWER - 100*AWAY_POWER)) %>%
        filter(!is.na(HOME_OFF_POWER_DIFF))

```

Each team has overall efficiency rating based on their offense and defense to that point in the season. I'll predict the winner simply based on the difference in their overall ratings with an adjustment for home field advantage and see how it compares to Elo.

```{r predict via margin}

efficiency_season_accuracy = weekly_ratings_and_outcomes %>%
        filter(SEASON > 2007 & SEASON < 2022) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'),
                                 levels = c("no", "yes"))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_POWER_DIFF +3 >=0 ~ 'yes',
                                       TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        group_by(SEASON) %>%
        yardstick::accuracy(truth = HOME_WIN,
                            estimate = HOME_PRED,
                            event_level = 'second') %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(method = 'efficiency')

efficiency_season_accuracy %>%
        select(SEASON, method, .metric, .estimate) %>%
        mutate(SEASON = factor(SEASON)) %>%
        flextable() %>%
        autofit()

efficiency_season_type_accuracy = weekly_ratings_and_outcomes %>%
        filter(SEASON > 2007 & SEASON < 2022) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'),
                                 levels = c("no", "yes"))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_POWER_DIFF +3 >=0 ~ 'yes',
                                       TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        group_by(SEASON_TYPE) %>%
        yardstick::accuracy(truth = HOME_WIN,
                            estimate = HOME_PRED,
                            event_level = 'second') %>%
        mutate(method = 'efficiency') %>%
        mutate_if(is.numeric, round, 2)


efficiency_season_type_accuracy %>%
        select(SEASON_TYPE, method, .metric, .estimate) %>%
        flextable() %>%
        autofit()

```

Interestingly, we do about the same in predicting the postseason using Elo and the efficiency ratings! We do a bit worse in predicting regular season games, but some of that difference is because I don't have efficiency ratings for FCS teams, so we're not seeing any FCS games included in assessing efficiency measures and these are games that tend to be easier to predict. If we drop games with FCS teams and look at overall performance, the Elo ratings only slightly edge out the raw efficiency ratings.

```{r assess both using only fbs games}

# only fbs games
 weekly_ratings_and_outcomes %>%
        filter(SEASON > 2007 & SEASON < 2022) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'),
                                 levels = c("no", "yes"))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_POWER_DIFF +3 >=0 ~ 'yes',
                                       TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        yardstick::accuracy(truth = HOME_WIN,
                            estimate = HOME_PRED,
                            event_level = 'second') %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(method = 'efficiency') %>%
        bind_rows(.,
                  game_elo_ratings %>%
        filter(SEASON > 2007 & SEASON < 2022) %>%
                filter(HOME_DIVISION == 'fbs' & AWAY_DIVISION == 'fbs') %>%
                mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                mutate(HOME_PRED = factor(case_when(HOME_PROB >=.5 ~ 'yes',
                                       TRUE ~ 'no'))) %>%
                mutate(PRED_CORRECT = case_when(HOME_WIN == HOME_PRED ~ 'yes',
                                        TRUE ~ 'no')) %>%
                yardstick::accuracy(truth = HOME_WIN,
                            estimate = HOME_PRED,
                            event_level = 'second') %>%
                mutate_if(is.numeric, round, 2) %>%
                mutate(method = 'elo')) %>%
        select(method, .metric, .estimate) %>%
        flextable() %>%
        autofit()

```

This means we have two different methods for predicting games, one of which (efficiency) is considerably more complicated than the other. Elo ratings also are *much* simpler to use for the purpose of simulating seasons, as we can easily update them based on simulated outcomes. If we were going purely for simplicity and ease of calculation, we'd go for the Elo ratings everytime.

But, for the sake of science, can we combine the two in order to do better than using just one? I'll fit a logistic regression of the home win on the difference in a team's overall efficiency measures and Elo. Does it do better in predicting games?

```{r combine and predict}

weekly_ratings_and_outcomes %>%
        select(GAME_ID, SEASON, WEEK, SEASON_TYPE, HOME_TEAM, AWAY_TEAM, HOME_SCORE_DIFF, contains("POWER_DIFF")) %>%
        left_join(.,
                  elo_games$game_outcomes %>%
                          select(GAME_ID, SEASON, HOME_PROB, NEUTRAL_SITE, HOME_PREGAME_ELO, AWAY_PREGAME_ELO, HOME_RECRUITING_SCORE, AWAY_RECRUITING_SCORE) %>%
                          mutate(HOME_ELO_DIFF = case_when(NEUTRAL_SITE == T ~ HOME_PREGAME_ELO - AWAY_PREGAME_ELO,
                                                           NEUTRAL_SITE == F ~ selected_pars$home_field_advantage + HOME_PREGAME_ELO - AWAY_PREGAME_ELO)),
                  by = c("SEASON", "GAME_ID")) %>%
        mutate(HOME_RECRUITING_SCORE = replace_na(HOME_RECRUITING_SCORE, 0),
               AWAY_RECRUITING_SCORE = replace_na(AWAY_RECRUITING_SCORE, 0)) %>%
        mutate(HOME_RECRUITING_DIFF = HOME_RECRUITING_SCORE - AWAY_RECRUITING_SCORE) %>%
        mutate(HOME_WIN = factor(case_when(HOME_SCORE_DIFF > 0 ~ 'yes',
                                    TRUE ~ 'no'),
                                 levels = c("no", "yes"))) %>%
        nest() %>%
        mutate(glm = map(data,
                         ~ glm(HOME_WIN ~ HOME_ELO_DIFF + HOME_OFF_POWER_DIFF + HOME_DEF_POWER_DIFF,
                                   family = 'binomial',
                                   data = .x))) %>%
        mutate(tidied = map(glm, tidy, se='robust', conf.int=T)) %>%
        mutate(augmented = map2(glm, data, 
                                ~ augment(.x, 
                                          newdata = .y,
                                          type.predict = 'response'))) %>%
        select(augmented) %>%
        unnest() %>%
        mutate(.pred = factor(case_when(.fitted > .5 ~ 'yes',
                                 TRUE ~ 'no'),
                              levels = c('no', 'yes'))) %>%
        select(GAME_ID, SEASON, WEEK, HOME_TEAM, AWAY_TEAM, HOME_WIN, HOME_PROB, .fitted, .pred) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > 0.5 ~ 'yes',
                                     TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        yardstick::accuracy(truth = HOME_WIN,
                            estimate  = .pred) %>%
        mutate_if(is.numeric, round, 2)
        # ggplot(., aes(x=.fitted,
        #               color = HOME_WIN,
        #               label = paste(HOME_TEAM, AWAY_TEAM, SEASON),
        #               y=HOME_PROB))+
        # geom_point()+
        # geom_text(check_overlap = T,
        #           vjust = -1,
        #           size = 2.5)
        #         

```
And... not really. Even using both, we seem to have hit a ceiling in getting about 73% of games correct. What then is the value of the efficiency measures? The main thing we are interested in is simulating seasons. If efficiency ratings, as well as recruiting data, helps us predict where teams are going to end up in terms of Elo ratings, we can use them to make preseason adjustments to team Elo ratings which will affect our simulations.
 
# Adjusting Elo Ratings with Efficiency and Recruiting

## Comparing Elo Ratings and Efficiency

How do season end Elo ratings compare to each team's end of season offensive/defensive efficiency? This is taking each team's season end overall efficiency metric (offense + defense net points per play) and comparing it to their season end Elo rating.

```{r join elo ratings with power ratings}

# get adjusted team rankings
adjusted_team_weekly_rankings = weekly_team_rankings %>%
        filter(TYPE == 'adjusted') %>%
        select(SEASON, GAME_ID, WEEK, TEAM, TYPE, OFFENSE, DEFENSE, OVERALL) %>%
        group_by(SEASON, TEAM, WEEK) %>%
        filter(GAME_ID == max(GAME_ID)) %>%
        ungroup()

# get end of season elo
adjusted_team_weekly_rankings %>%
        group_by(SEASON, TEAM) %>%
        filter(WEEK == max(WEEK)) %>%
        left_join(.,
                  team_elo_ratings %>%
                          group_by(SEASON, TEAM) %>%
                          arrange(GAME_DATE) %>%
                          mutate(WEEK = row_number()) %>%
                          mutate(LAST_GAME = GAME_DATE == max(GAME_DATE)) %>%
                          filter(LAST_GAME ==T) %>%
                          select(SEASON, TEAM, POSTGAME_ELO),
                  by = c("SEASON", "TEAM")) %>%
        mutate(OVERALL = OVERALL * 100) %>%
        ggplot(., aes(x=OVERALL,
                      label = paste(TEAM, SEASON),
                      y=POSTGAME_ELO))+
        geom_point()+
        geom_text(check_overlap = T, vjust = -1)+
        theme_phil()+
        stat_cor(p.accuracy = 0.01)+
        ylab("Season End Elo Rating")+
        xlab("Team Overall Efficiency")
                  
# adjusted_team_rankings %>%
#         left_join(.,
#                   team_elo_ratings %>%
#                           
```

As we would probably have expected (and hoped), these are highly correlated: teams with an net efficiency on offense and defense tend to win games and improve their Elo rating.

When do they disagree? I'll regress the season end Elo ratings on the season end efficiency measure and look at teams with the largest absolute residuals.

```{r look at disagreements between the two}

nested_lm = adjusted_team_weekly_rankings %>%
        group_by(SEASON, TEAM) %>%
        filter(WEEK == max(WEEK)) %>%
        ungroup() %>%
        left_join(.,
                  team_elo_ratings %>%
                          group_by(SEASON, TEAM) %>%
                          arrange(GAME_DATE) %>%
                          mutate(WEEK = row_number()) %>%
                          mutate(LAST_GAME = GAME_DATE == max(GAME_DATE)) %>%
                          filter(LAST_GAME ==T) %>%
                          select(SEASON, TEAM, POSTGAME_ELO),
                  by = c("SEASON", "TEAM")) %>%
        filter(!is.na(OVERALL) & !is.na(POSTGAME_ELO)) %>%
        mutate(OVERALL = OVERALL * 100) %>%
        distinct(SEASON, WEEK, TEAM, TYPE, OFFENSE, DEFENSE, OVERALL, POSTGAME_ELO) %>%
   #     mutate(OVERALL = (OVERALL - mean(OVERALL) / sd(OVERALL)))
        nest()  %>%
        mutate(lm = map(data,
                        ~ lm(POSTGAME_ELO ~ OVERALL,
                             data = .x))) %>%
        mutate(tidied = map(lm, tidy, se='robust', conf.int=T)) %>%
        mutate(augmented = map(lm, ~ augment(.x) %>%
                                       select(-OVERALL, -POSTGAME_ELO))) %>%
        mutate(resid = map(lm, ~ resid(.x))) 

nested_lm %>%
        select(tidied) %>%
        unnest() %>%
        mutate_if(is.numeric, round, 2)
        
```

The bivariate linear model indicates that team with an overall efficiency of zero will have an Elo rating of around 1520 (about the FBS average), while each additional point of efficiency increases their Elo rating by 12. So a team with an overall efficiency of 40 at the end of the season will be expected to have an Elo rating of 1520 + (12 * 40) = 2000. 

Next we can look at the residuals from this model. Which team's Elo ratings were the furthest away from their expected Elo ratings based on their underlying efficiency?

```{r examine residuals}

nested_lm %>%
        select(data, augmented, resid) %>%
        unnest() %>%
        mutate_if(is.numeric, round, 1) %>%
        arrange(desc(abs(.resid))) %>%
        mutate(SEASON_ELO = POSTGAME_ELO) %>%
        select(SEASON, WEEK, TEAM, OVERALL, SEASON_ELO, .fitted, .resid) %>%
        mutate(SEASON = factor(SEASON)) %>%
        select(-WEEK) %>%
        head(25) %>%
        flextable() %>%
        autofit()

```

Teams with a positive residual have a higher Elo than we would have expected from their overall rating. UMass is potentially even worse than their Elo rating would indicate, given that they have an absurdly low overall efficiency in 2019 and 2020. Similarly, Louisiana 2021 has a higher Elo rating than you would expect given their underlying offensive/defensive efficiency. I wonder if they won a lot of close games? Some other teams like LSU and Ohio State have a higher rating because they started the season very strong, so their Elo rating is still hovering higher than would have expected than from their efficiency alone.

Teams with a negative residual are the opposite, having a lower Elo rating than we would have expected based on their offensive/defensive efficiency. Duke 2008 evidently was a *slightly* above average team based on efficiency, but they ended the season with a very low Elo rating.

## Predicting Next Season's Elo Rating

Given where a team ended its previous season, what predicts a team's Elo for the next season? The Elo framework is really handy because it can allow us to easily simulate games based on simulated results. Given the margin of victory and the winner of the game, we can easily adjust both teams ratings, and then simulate, and so on. So, the goal is to convert efficiency into Elo 

I'll regress each team's season ending Elo from season $t$ on what we observed from the previous season $t-1$. I'll first use their Elo rating alone, then I'll add Elo and their efficiency from the end of season $t-1$. I've centered the previous season's Elo at 0 in order to make the intercept easier to interpret. I'll use the seasons up until 2018 as the set in fitting the models.

```{r predicting next season end elo}

nested_lags = adjusted_team_weekly_rankings %>%
        filter(SEASON < 2018) %>%
        mutate(OVERALL = OVERALL * 100) %>%
        group_by(SEASON, TEAM) %>%
        filter(WEEK == max(WEEK)) %>%
        select(SEASON, WEEK, TEAM, OVERALL) %>%
        group_by(TEAM) %>%
        arrange(SEASON) %>%
        mutate(PREVIOUS_OVERALL = dplyr::lag(OVERALL, 1)) %>%
        ungroup() %>%
        left_join(.,
                  team_elo_ratings %>%
                          group_by(SEASON, TEAM) %>%
                          arrange(GAME_DATE) %>%
                          mutate(WEEK = row_number()) %>%
                          mutate(LAST_GAME = GAME_DATE == max(GAME_DATE)) %>%
                          filter(LAST_GAME ==T) %>%
                          select(SEASON, TEAM, POSTGAME_ELO) %>%
                          rename(ELO = POSTGAME_ELO) %>%
                          group_by(TEAM) %>%
                          mutate(PREVIOUS_ELO = dplyr::lag(ELO, 1)),
                  by = c("SEASON", "TEAM")) %>%
        filter(!is.na(OVERALL) & !is.na(ELO)) %>%
        filter(!is.na(PREVIOUS_ELO) & !is.na(PREVIOUS_OVERALL)) %>%
        mutate(PREVIOUS_ELO = PREVIOUS_ELO-mean(PREVIOUS_ELO))

nested_lags_elo = nested_lags %>%
        nest() %>%
        mutate(lm = map(data, ~ lm(ELO ~ PREVIOUS_ELO,
                                         #  PREVIOUS_OVERALL,
                                   data = .x))) %>%
        mutate(tidied = map(lm, tidy, se='robust', conf.int=T)) %>%
        mutate(augmented = map(lm, ~ augment(.x))) %>%
        mutate(glanced = map(lm, glance))

nested_lags_elo %>%
        select(tidied) %>%
        unnest() %>%
        mutate_if(is.numeric, round, 2)

nested_lags_elo %>%
        select(glanced) %>%
        unnest() %>%
        mutate_if(is.numeric, round, 2) %>%
        select(r.squared, sigma, nobs)

# nested_lags_elo %>%
#         select(data, augmented) %>%
#         unnest() %>%
#         mutate_if(is.numeric, round, 1) %>%
#         arrange(desc(abs(.resid))) %>%
#         select(SEASON, TEAM, PREVIOUS_OVERALL, OVERALL, PREVIOUS_ELO, ELO, .fitted, .resid)

```

Season over season, each point of Elo rating from a previous season is worth about .8 for next season's Elo. A team with an average Elo last season will be expected to have an average of about 1518 for next season.

If we include overall efficiency as well as the Elo rating in predicting the next season's season ending Elo, how does this change?

```{r nested lags with both}

nested_lags_both = nested_lags %>%
        nest() %>%
        mutate(lm = map(data, ~ lm(ELO ~ PREVIOUS_ELO + PREVIOUS_OVERALL,
                                   data = .x))) %>%
        mutate(tidied = map(lm, tidy, se='robust', conf.int=T)) %>%
        mutate(augmented = map(lm, ~ augment(.x))) %>%
        mutate(glanced = map(lm, glance))

nested_lags_both %>%
        select(tidied) %>%
        unnest() %>%
        mutate_if(is.numeric, round, 2)

nested_lags_both %>%
        select(glanced) %>%
        unnest() %>%
        mutate_if(is.numeric, round, 2) %>%
        select(r.squared, sigma, nobs)
# 
# nested_lags_both %>%
#         select(data, augmented) %>%
#         unnest() %>%
#         mutate_if(is.numeric, round, 1) %>%
#         arrange(desc(abs(.resid))) %>%
#         select(SEASON, TEAM, PREVIOUS_OVERALL, OVERALL, PREVIOUS_ELO, ELO, .fitted, .resid)

```

Now, the effect of last year's Elo is lower (and this difference should be significant, from eyeballing the confidence intervals), where one point of Elo from last season only increases this season's Elo by 0.43. This is because we now have the effect of overall efficiency, which indicates that a one point increase in efficiency translates to about 5 Elo points. This means that where a team is expected to end up next year depends both on its previous Elo ratings and on its efficiency. Elo ratings have a larger absolute effect (based on standardizing the variables and examining the effect of a one standard deviation change), but efficiency is a factor as well.

```{r compare predictions for next season}

# pluck(nested_lags_both,
#       "lm", 1) %>%
#         augment(., 
#                 newdata = tibble(PREVIOUS_ELO = 1900 - 1520,
#                           PREVIOUS_OVERALL = seq(-5, 25, 5))) %>%
#         mutate(PREVIOUS_ELO = PREVIOUS_ELO + 1520) %>%
#         mutate_if(is.numeric, round, 0)

pluck(nested_lags_both,
      "lm", 1) %>%
        augment(., 
                newdata = tibble(PREVIOUS_ELO = 0,
                          PREVIOUS_OVERALL = seq(-25, 25, 5))) %>%
        mutate(PREVIOUS_ELO = PREVIOUS_ELO + 1520) %>%
        mutate_if(is.numeric, round, 0)


 # pluck(nested_lags_both, "lm", 1) %>%
 #        predict(., newdata = nested_lags_both %>%
 #                        select(data) %>% 
 #                        unnest() %>% 
 #                        filter(SEASON == 2021) %>% 
 #                        select(SEASON, WEEK, TEAM, OVERALL, ELO) %>% 
 #                        rename(PREVIOUS_OVERALL = OVERALL, 
 #                               PREVIOUS_ELO = ELO) %>%
 #                        mutate(PREVIOUS_ELO = PREVIOUS_ELO - 1520),
 #                interval = 'predict') %>%
 #        bind_cols(newdata = nested_lags_both %>%
 #                        select(data) %>% 
 #                        unnest() %>% 
 #                        filter(SEASON == 2021) %>% 
 #                        select(SEASON, WEEK, TEAM, OVERALL, ELO) %>% 
 #                        rename(PREVIOUS_OVERALL = OVERALL, 
 #                               PREVIOUS_ELO = ELO) %>%
 #                        mutate(PREVIOUS_ELO = PREVIOUS_ELO - 1520),
 #                  .) %>%
 #        mutate_if(is.numeric, round, 2) %>%
 #        select(TEAM, fit, lwr, upr) %>%
 #        arrange(desc(fit))

```

Based on this, we can use efficiency to adjust Elo rating to see whether it improves our ability to predict games and simulate seasons.

## Adding Recruiting

Since we're in the business of evaluating pieces to tinker with in adjusting Elo ratings, I'll also add in a team's composite recruiting scores from recent years. I'm regressing each team's season ending Elo rating on their previous season's Elo rating, their previous season efficiency measures, and their recruiting composite going into the season (which is based on previous years). I'll run this regression up until 2018, then I'll use the 2018-2021 seasons as a validation set.

```{r create workflow and recipe}

# 2022 season 
future_seasons = expand.grid(TEAM = season_team_rankings  %>%
                                  select(TEAM) %>%
                                  distinct(TEAM) %>%
                                  pull(),
                               TYPE = c('raw', 'adjusted'),
                          SEASON = 2022) %>%
        as_tibble()

# get efficiency and elo for historical + future season
teams_data = season_team_rankings %>%
        select(-MARGIN) %>%
        bind_rows(., future_seasons) %>%
        filter(TYPE == 'adjusted') %>%
        select(-TYPE) %>%
        # add in season end elo ratings
        left_join(.,
                  team_elo_ratings %>%
                          group_by(SEASON, TEAM) %>%
                          arrange(GAME_DATE) %>%
                          mutate(WEEK = row_number()) %>%
                          mutate(LAST_GAME = GAME_DATE == max(GAME_DATE)) %>%
                          filter(LAST_GAME ==T) %>%
                          select(SEASON, TEAM, POSTGAME_ELO) %>%
                          rename(ELO = POSTGAME_ELO),
                  by = c("SEASON", "TEAM")) %>%
        # add in season recruiting data
        left_join(., 
                  recruiting %>%
                          select(TEAM, SEASON, RECRUITING_SCORE),
                  by = c("SEASON", "TEAM")) %>%
        arrange(SEASON, TEAM) %>%
        group_by(TEAM) %>%
        mutate(PREVIOUS_OVERALL = lag(OVERALL, 1),
               PREVIOUS_OFFENSE = lag(OFFENSE, 1),
               PREVIOUS_DEFENSE = lag(DEFENSE, 1),
               PREVIOUS_ELO = lag(ELO, 1)) %>%
        ungroup() %>%
        filter(SEASON > 2007)

# split into training and test
teams_train  = teams_data %>%
        filter(SEASON < 2018)

teams_valid = teams_data %>%
        filter(SEASON >=2018 & SEASON < 2022)

teams_test = teams_data %>%
        filter(SEASON == 2022)

# create recipe
teams_recipe =      
        recipe(ELO ~.,
               data = teams_train) %>%
        update_role(all_predictors(),
                    new_role = "id") %>%
        update_role(PREVIOUS_OFFENSE,
                    PREVIOUS_DEFENSE,
                    PREVIOUS_ELO,
                    RECRUITING_SCORE,
                    new_role = 'predictor') %>%
        # manual missingness
        step_mutate(RECRUITING_SCORE = replace_na(RECRUITING_SCORE, 0),
                    PREVIOUS_ELO = replace_na(PREVIOUS_ELO, 1200),
                    PREVIOUS_OFFENSE = replace_na(PREVIOUS_OFFENSE,-0.1),
                    PREVIOUS_DEFENSE = replace_na(PREVIOUS_DEFENSE, -0.1)) %>%
        step_normalize(all_predictors())

# linear model
lm_mod = linear_reg(mode = "regression",
                    engine = "lm")

# create workflow
teams_workflow = workflow() %>%
        add_recipe(teams_recipe) %>%
        add_model(lm_mod)
        
```

I'm using four features from season $t-1, t-2...$ to model each team's Elo rating in season $t$. I normalized each of these features to indicate the effect of a standard deviation change on next season's Elo rating. The linear model indicates that a perfectly average FBS team will be set at 1512. The coefficient for each term is the effect of a one standard deviation increase. This means that a one standard deviation increase in Elo is worth about 90 Elo for next season, a one standard deviation increase in recruiting is worth 50 Elo, and a one standard deviation increase in efficiency for offense and defense is worth about 30 Elo.

```{r fit workflow on training set}

# fit the model
teams_workflow %>%
        fit(teams_train) %>%
        extract_fit_engine() %>%
        tidy() %>%
        mutate_if(is.numeric, round, 2)

```

I'll then predict the Elo ratings for next season using information from past seasons.

```{r predict}

adjusted_teams_valid = 
        teams_workflow %>%
        fit(teams_train) %>%
        augment(bind_rows(teams_train %>%
                                  mutate(dataset = 'train'),
                          teams_valid %>%
                                  mutate(dataset = 'valid'),
                          teams_test %>%
                                  mutate(dataset = 'test'))) %>%
        select(SEASON, dataset, TEAM, starts_with("PREVIOUS"),
               ELO, .pred) %>%
        mutate_if(is.numeric, round, 3) 

adjusted_teams_valid %>%
        filter(dataset != 'test') %>%
        ggplot(., aes(x=.pred, 
                      label = paste(TEAM, SEASON),
                      y=ELO))+
        geom_point()+
        geom_text(check_overlap = T,
                  vjust = -1)+
        theme_phil()+
        facet_wrap(dataset ~.)+
        geom_abline(slope = 1,
                    intercept =0,
                    linetype = 'dashed')

```

I can then run each season, with the adjusted Elo ratings being the starting point adjusted based on recruiting and the previous season's efficiency.

```{r comptue elo ratings from adjusted starting values}

# function to turn teams and elo ratings into a list
teams_to_list = function(x) {
        
        setNames(unlist(x$ELO) %>%
                                          split(., seq(nrow(x))),
                                  rownames(x))
}

team_seasons_to_list = function(x) {
        
        setNames(unlist(x$SEASON) %>%
                                          split(., seq(nrow(x))),
                                  rownames(x))
}

seasons = seq(2018, 2021)
assess_metrics = metric_set(yardstick::mn_log_loss,
                           yardstick::accuracy)


compare = foreach(i = 1:length(seasons),
                  .combine = bind_rows) %do% {
                
        # set input season
        input_season = seasons[i]
        
        # get computed elo as input
        adjusted_teams_elo = adjusted_teams_valid %>%
                filter(SEASON == input_season) %>%
                select(TEAM, .pred) %>%
                rename(ELO = .pred) %>%
                column_to_rownames("TEAM")
                
        # fbs teams
        adjusted_teams = teams_to_list(adjusted_teams_elo)
        
        # get previous fcs teams
        actual_teams_elo = elo_games$team_outcomes %>%
                filter(SEASON < input_season) %>%
                group_by(TEAM) %>%
                filter(GAME_DATE == max(GAME_DATE)) %>%
                slice_max(POSTGAME_ELO, n=1, with_ties = F) %>%
                ungroup() %>%
                select(TEAM, POSTGAME_ELO) %>%
                rename(ELO = POSTGAME_ELO) %>%
                column_to_rownames("TEAM")
        
        # get season of last observed
        team_seasons = elo_games$team_outcomes %>%
                filter(SEASON < input_season) %>%
                group_by(TEAM) %>%
                slice_max(., order_by = SEASON, n=1, with_ties =F) %>%
                ungroup() %>%
                select(TEAM, SEASON) %>%
                column_to_rownames("TEAM")
        
        # create dictionaries
        actual_teams = teams_to_list(actual_teams_elo)
        actual_team_seasons = team_seasons_to_list(team_seasons)
        
        # now get the elo ratings based on this training set
        games_with_actual = calc_elo_ratings(games %>%
                                            filter(SEASON ==input_season),
                                        teams = actual_teams,
                                        team_seasons = actual_team_seasons,
                                        home_field_advantage = selected_pars$home_field_advantage,
                                  reversion = selected_pars$reversion,
                                  k = selected_pars$k,
                                  v = selected_pars$v,
                                  verbose = T)
        
        # get actual
        actual = games_with_actual %$% 
                game_outcomes %>%
                mutate(HOME_WIN = factor(case_when(HOME_OUTCOME == 'win' ~ 'yes',
                                            TRUE ~ 'no'),
                                         levels = c("no", "yes"))) %>%
                mutate(HOME_PRED = factor(case_when(HOME_PROB > .5 ~ 'yes',
                                                    TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                mutate(yes = HOME_PROB) %>%
                group_by(SEASON) %>%
                assess_metrics(truth = HOME_WIN,
                                         yes,
                                    estimate = HOME_PRED,
                               event_level = 'second') %>%
                mutate(type = 'elo')


        # games with adjusted
        # now get the elo ratings based on this training set
        games_with_adjusted = calc_elo_ratings(games %>%
                                            filter(SEASON ==input_season),
                                        teams = c(adjusted_teams, actual_teams[!(names(actual_teams) %in% names(adjusted_teams))]),
                                        team_seasons = actual_team_seasons,
                                        home_field_advantage = selected_pars$home_field_advantage,
                                  reversion = 0,
                                  k = selected_pars$k,
                                  v = selected_pars$v,
                                  verbose = T)
        
        # get actual
        adjusted = games_with_adjusted %$% 
                game_outcomes %>%
                mutate(HOME_WIN = factor(case_when(HOME_OUTCOME == 'win' ~ 'yes',
                                            TRUE ~ 'no'),
                                         levels = c("no", "yes"))) %>%
                mutate(HOME_PRED = factor(case_when(HOME_PROB > .5 ~ 'yes',
                                                    TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                mutate(yes = HOME_PROB) %>%
                group_by(SEASON) %>%
                assess_metrics(truth = HOME_WIN,
                                         yes,
                                    estimate = HOME_PRED,
                               event_level = 'second') %>%
                mutate(type = 'adjusted elo')
        
        bind_rows(actual,
                  adjusted)
        
                  }

compare %>%
        spread(.metric, .estimate) %>%
        mutate_if(is.numeric, round, 3)
        
```

Overall, we're about the same, slightly better on log loss for each season while the accuracy is slightly tends to bounce between seasons.

```{r predict test set after fitting on valid}

adjusted_teams_test =
        teams_workflow %>%
        fit(bind_rows(teams_train,
                      teams_valid)) %>%
        augment(bind_rows(teams_train %>%
                                  mutate(dataset = 'train'),
                          teams_valid %>%
                                  mutate(dataset = 'valid'),
                          teams_test %>%
                                  mutate(dataset = 'test'))) %>%
        select(SEASON, dataset, TEAM, starts_with("PREVIOUS"),
               ELO, .pred) %>%
        mutate_if(is.numeric, round, 3)

```


## Simulating via Elo, Efficiency, and Recruiting

Now, I'll simulate the 2021 season using the standard Elo ratings vs the adjusted Elo ratings.

```{r set up for simulation}

# for parallelization
library(future.apply)
plan(multisession, workers = 7)

# initial season
initial_season = 2021

# set initial games
initial_games = games %>%
        filter(SEASON < initial_season)

# set of pars
elo_pars = tibble(k = 35,
                  v = 400,
                  reversion = 0.2,
                  home_field_advantage = 75,
                  recruiting_weight =0)

# get fbs teams
teams = elo_games$team_outcomes %>%
        filter(SEASON == initial_season) %>%
        filter(DIVISION == 'fbs') %>%
        distinct(TEAM) %>%
        arrange(TEAM)

# train points model
points_model = elo_games$game_outcomes %>%
        filter(SEASON >=1970) %>% 
        filter(SEASON < initial_season) %>%
        mutate(HOME_ELO_DIFF = case_when(NEUTRAL_SITE == F ~ 
                                                 HOME_PREGAME_ELO + 
                                                 elo_pars$home_field_advantage -
                                                 AWAY_PREGAME_ELO,
                                         TRUE ~ HOME_PREGAME_ELO - AWAY_PREGAME_ELO)) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        nest() %>%
        # now fit linear models, i'll do so using stan
        mutate(lm = map(data, ~ lm(HOME_SCORE_DIFF ~ HOME_ELO_DIFF +0,
                                   data = .x))) %>%
        pluck("lm",1)

# functions
source(here::here("functions", "calc_elo_ratings.R"))
source(here::here("functions", "sim_elo_ratings.R"))
source(here::here("functions", "sim_rest_of_season.R"))

```



```{r set up for adjusted elo simulations}

# get computed elo as input
adjusted_teams_elo = adjusted_teams_valid %>%
        filter(SEASON == initial_season) %>%
        select(TEAM, .pred) %>%
        rename(ELO = .pred) %>%
        column_to_rownames("TEAM")
                
# fbs teams
adjusted_teams = teams_to_list(adjusted_teams_elo)

# get previous fcs teams
actual_teams_elo = elo_games$team_outcomes %>%
        filter(SEASON < initial_season) %>%
        group_by(TEAM) %>%
        filter(GAME_DATE == max(GAME_DATE)) %>%
        slice_max(POSTGAME_ELO, n=1, with_ties = F) %>%
        ungroup() %>%
        select(TEAM, POSTGAME_ELO) %>%
        rename(ELO = POSTGAME_ELO) %>%
        column_to_rownames("TEAM")
        
# get season of last observed
team_seasons = elo_games$team_outcomes %>%
        filter(SEASON < initial_season) %>%
        group_by(TEAM) %>%
        slice_max(., order_by = SEASON, n=1, with_ties =F) %>%
        ungroup() %>%
        select(TEAM, SEASON) %>%
        column_to_rownames("TEAM")
        
# create dictionaries
actual_teams = teams_to_list(actual_teams_elo)
actual_team_seasons = team_seasons_to_list(team_seasons)

```


```{r simulate}

elo_initial = list(teams = actual_teams,
                   team_seasons = actual_team_seasons)

# simulate season from the start
set.seed(11)
elo_season_simulations = sim_rest_of_season(games,
                                        season = initial_season,
                                        elo_initial = elo_initial,
                                        elo_pars = elo_pars,
                                        nsims = 1000,
                                        sim_start_week = 0,
                                        season_start_only = T,
                                        aggregate = F,
                                        points_model = points_model)

elo_win_simulations =
        elo_season_simulations %$%
        team_outcomes %>%
        left_join(.,
                  games %>%
                          select(GAME_ID, CONFERENCE_CHAMPIONSHIP),
                  by = c("GAME_ID")) %>%
        filter(SEASON_TYPE == 'regular') %>%
        filter(CONFERENCE_CHAMPIONSHIP != T) %>%
        group_by(SEASON, .id, TEAM, SIM_OUTCOME) %>%
        count() %>%
        group_by(SEASON, TEAM, SIM_OUTCOME) %>%
        summarize(mean = mean(n),
                  median = median(n),
                  .groups = 'drop') %>%
        select(-median) %>%
        spread(SIM_OUTCOME, mean) %>%
        select(SEASON, TEAM, win, loss) %>%
        mutate_if(is.numeric, replace_na, 0) %>%
        mutate_if(is.numeric, round, 2) %>%
        arrange(desc(win))

elo_game_simulations = games %>%
        filter(SEASON_TYPE == 'regular') %>%
        filter(CONFERENCE_CHAMPIONSHIP != T) %>%
        filter(SEASON == initial_season) %>%
        left_join(.,
        elo_season_simulations %$%
        game_outcomes %>%
                     group_by(GAME_ID,
                              SIM_FROM_WEEK,
                              home_field_advantage,
                             # recruiting_weight,
                              reversion,
                              k, v) %>%
                     mutate(HOME_SIM_PRED = case_when(HOME_SIM_MARGIN >0 ~ 1,
                                                      TRUE ~ 0)) %>%
                     summarize(HOME_PREGAME_ELO = mean(HOME_PREGAME_ELO),
                               AWAY_PREGAME_ELO = mean(AWAY_PREGAME_ELO),
                               HOME_POSTGAME_ELO = mean(HOME_POSTGAME_ELO),
                               AWAY_POSTGAME_ELO = mean(AWAY_POSTGAME_ELO),
                               home_wins = sum(HOME_SIM_PRED),
                               home_margin = mean(HOME_SIM_MARGIN),
                               n = n(),
                               .groups = 'drop') %>%
                     mutate(HOME_SIM_PROB = home_wins / n) %>%
                     mutate(HOME_SIM_MARGIN = home_margin) %>%
                     select(GAME_ID,
                            HOME_PREGAME_ELO,
                            AWAY_PREGAME_ELO,
                            HOME_POSTGAME_ELO,
                            AWAY_POSTGAME_ELO,
                            HOME_SIM_PROB,
                            HOME_SIM_MARGIN,
                            SIM_FROM_WEEK,
                            home_field_advantage,
                            #recruiting_weight,
                            reversion, k, v),
             by = "GAME_ID")

elo_team_simulations = elo_game_simulations %>%
                        select(SIM_FROM_WEEK,
                               GAME_ID,
                               SEASON,
                               SEASON_TYPE,
                               WEEK,
                               GAME_DATE,
                               HOME_TEAM,
                               HOME_CONFERENCE,
                               HOME_DIVISION,
                               HOME_SIM_PROB,
                               HOME_SIM_MARGIN,
                               HOME_PREGAME_ELO,
                               HOME_POSTGAME_ELO,
                               AWAY_TEAM,
                               home_field_advantage,
                           #    recruiting_weight,
                               reversion,
                               k,
                               v
                        ) %>%
                        rename(OPPONENT = AWAY_TEAM) %>%
                        set_names(., gsub("HOME_", "", names(.))) %>%
                        bind_rows(.,
                                  elo_game_simulations %>%
                                          mutate(AWAY_SIM_PROB = 1-HOME_SIM_PROB,
                                                 AWAY_SIM_MARGIN = -HOME_SIM_MARGIN) %>%
                                          select(SIM_FROM_WEEK,
                                                 GAME_ID,
                                                 SEASON,
                                                 SEASON_TYPE,
                                                 WEEK,
                                                 GAME_DATE,
                                                 AWAY_TEAM,
                                                 AWAY_CONFERENCE,
                                                 AWAY_DIVISION,
                                                 AWAY_SIM_PROB,
                                                 AWAY_SIM_MARGIN,
                                                 AWAY_PREGAME_ELO,
                                                 AWAY_POSTGAME_ELO,
                                                 HOME_TEAM,
                                                 home_field_advantage,
                                            #     recruiting_weight,
                                                 reversion,
                                                 k,
                                                 v) %>%
                                          rename(OPPONENT = HOME_TEAM) %>%
                                          set_names(., gsub("AWAY_", "", names(.)))) %>%
                        arrange(GAME_DATE)

```


```{r new chunj for adjusted}

# get computed elo as input
adjusted_teams_elo = adjusted_teams_valid %>%
        filter(SEASON == input_season) %>%
        select(TEAM, .pred) %>%
        rename(ELO = .pred) %>%
        column_to_rownames("TEAM")

# fbs teams
adjusted_teams = teams_to_list(adjusted_teams_elo)
elo_initial = list(teams = c(adjusted_teams, actual_teams[!(names(actual_teams) %in% names(adjusted_teams))]),
                   team_seasons = actual_team_seasons)

# elo pars
elo_pars2 = tibble(k = 35,
                  v = 400,
                  reversion = 0,
                  home_field_advantage = 75,
                  recruiting_weight =0)

# simulate season from the start
set.seed(11)
adjusted_season_simulations = sim_rest_of_season(games,
                                        season = initial_season,
                                        elo_initial = elo_initial,
                                        elo_pars = elo_pars2,
                                        nsims = 1000,
                                        sim_start_week = 0,
                                        season_start_only = T,
                                        aggregate = F,
                                        points_model = points_model)

adjusted_win_simulations =
        adjusted_season_simulations %$%
        team_outcomes %>%
        left_join(.,
                  games %>%
                          select(GAME_ID, CONFERENCE_CHAMPIONSHIP),
                  by = c("GAME_ID")) %>%
        filter(SEASON_TYPE == 'regular') %>%
        filter(CONFERENCE_CHAMPIONSHIP != T) %>%
        group_by(SEASON, .id, TEAM, SIM_OUTCOME) %>%
        count() %>%
        group_by(SEASON, TEAM, SIM_OUTCOME) %>%
        summarize(mean = mean(n),
                  median = median(n),
                  .groups = 'drop') %>%
        select(-median) %>%
        spread(SIM_OUTCOME, mean) %>%
        select(SEASON, TEAM, win, loss) %>%
        mutate_if(is.numeric, replace_na, 0) %>%
        mutate_if(is.numeric, round, 2) %>%
        arrange(desc(win))

adjusted_game_simulations = games %>%
        filter(SEASON_TYPE == 'regular') %>%
        filter(CONFERENCE_CHAMPIONSHIP != T) %>%
        filter(SEASON == initial_season) %>%
        left_join(.,
        adjusted_season_simulations %$%
        game_outcomes %>%
                     group_by(GAME_ID,
                              SIM_FROM_WEEK,
                              home_field_advantage,
                             # recruiting_weight,
                              reversion,
                              k, v) %>%
                     mutate(HOME_SIM_PRED = case_when(HOME_SIM_MARGIN >0 ~ 1,
                                                      TRUE ~ 0)) %>%
                     summarize(HOME_PREGAME_ELO = mean(HOME_PREGAME_ELO),
                               AWAY_PREGAME_ELO = mean(AWAY_PREGAME_ELO),
                               HOME_POSTGAME_ELO = mean(HOME_POSTGAME_ELO),
                               AWAY_POSTGAME_ELO = mean(AWAY_POSTGAME_ELO),
                               home_wins = sum(HOME_SIM_PRED),
                               home_margin = mean(HOME_SIM_MARGIN),
                               n = n(),
                               .groups = 'drop') %>%
                     mutate(HOME_SIM_PROB = home_wins / n) %>%
                     mutate(HOME_SIM_MARGIN = home_margin) %>%
                     select(GAME_ID,
                            HOME_PREGAME_ELO,
                            AWAY_PREGAME_ELO,
                            HOME_POSTGAME_ELO,
                            AWAY_POSTGAME_ELO,
                            HOME_SIM_PROB,
                            HOME_SIM_MARGIN,
                            SIM_FROM_WEEK,
                            home_field_advantage,
                            #recruiting_weight,
                            reversion, k, v),
             by = "GAME_ID")

team_simulations = adjusted_game_simulations %>%
                        select(SIM_FROM_WEEK,
                               GAME_ID,
                               SEASON,
                               SEASON_TYPE,
                               WEEK,
                               GAME_DATE,
                               HOME_TEAM,
                               HOME_CONFERENCE,
                               HOME_DIVISION,
                               HOME_SIM_PROB,
                               HOME_SIM_MARGIN,
                               HOME_PREGAME_ELO,
                               HOME_POSTGAME_ELO,
                               AWAY_TEAM,
                               home_field_advantage,
                           #    recruiting_weight,
                               reversion,
                               k,
                               v
                        ) %>%
                        rename(OPPONENT = AWAY_TEAM) %>%
                        set_names(., gsub("HOME_", "", names(.))) %>%
                        bind_rows(.,
                                  adjusted_game_simulations %>%
                                          mutate(AWAY_SIM_PROB = 1-HOME_SIM_PROB,
                                                 AWAY_SIM_MARGIN = -HOME_SIM_MARGIN) %>%
                                          select(SIM_FROM_WEEK,
                                                 GAME_ID,
                                                 SEASON,
                                                 SEASON_TYPE,
                                                 WEEK,
                                                 GAME_DATE,
                                                 AWAY_TEAM,
                                                 AWAY_CONFERENCE,
                                                 AWAY_DIVISION,
                                                 AWAY_SIM_PROB,
                                                 AWAY_SIM_MARGIN,
                                                 AWAY_PREGAME_ELO,
                                                 AWAY_POSTGAME_ELO,
                                                 HOME_TEAM,
                                                 home_field_advantage,
                                            #     recruiting_weight,
                                                 reversion,
                                                 k,
                                                 v) %>%
                                          rename(OPPONENT = HOME_TEAM) %>%
                                          set_names(., gsub("AWAY_", "", names(.)))) %>%
                        arrange(GAME_DATE)

```


```{r actual wins}

actual_wins = games %>% 
        filter(SEASON == 2021) %>% 
        filter(SEASON_TYPE == 'regular') %>% 
        filter(CONFERENCE_CHAMPIONSHIP !=T) %>% 
        group_by(HOME_TEAM) %>% 
        mutate(HOME_WIN = case_when(HOME_POINTS > AWAY_POINTS ~ 'yes', TRUE ~ 'no')) %>% 
        group_by(SEASON, HOME_TEAM, HOME_WIN) %>% 
        count() %>% 
        ungroup() %>%
        spread(HOME_WIN, n) %>%
        rename(TEAM = HOME_TEAM) %>%
        mutate_if(is.numeric, replace_na, 0) %>%
        bind_rows(.,
                  games %>% 
                        filter(SEASON == 2021) %>% 
                        filter(SEASON_TYPE == 'regular') %>% 
                        filter(CONFERENCE_CHAMPIONSHIP !=T) %>% 
                        group_by(AWAY_TEAM) %>% 
                        mutate(AWAY_WIN = case_when(AWAY_POINTS > HOME_POINTS ~ 'yes', TRUE ~ 'no')) %>% 
                        group_by(SEASON, AWAY_TEAM, AWAY_WIN) %>% 
                        count() %>% 
                          ungroup() %>%
                        spread(AWAY_WIN, n) %>%
                        rename(TEAM = AWAY_TEAM) %>%
                        mutate_if(is.numeric, replace_na, 0) 
                  ) %>%
        group_by(SEASON, TEAM) %>%
        summarize(actual_win = sum(yes),
                  actual_loss = sum(no),
                  .groups = 'drop') 

# compare wins
adjusted_win_simulations %>%
        left_join(.,
                  actual_wins,
                  by = c("SEASON", "TEAM")) %>%
        left_join(., team_conference_season,
                  by = c("SEASON", "TEAM")) %>%
        filter(DIVISION == 'fbs') %>%
        group_by(SEASON) %>%
        yardstick::rmse(truth = actual_win,
                        estimate = win) %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(method = 'adjusted elo') %>%
        bind_rows(.,
        elo_win_simulations %>%
                left_join(.,
                          actual_wins,
                          by = c("SEASON", "TEAM")) %>%
                left_join(., team_conference_season,
                          by = c("SEASON", "TEAM")) %>%
                filter(DIVISION == 'fbs') %>%
                group_by(SEASON) %>%
                yardstick::rmse(truth = actual_win,
                                estimate = win) %>%
                mutate_if(is.numeric, round, 2) %>%
                mutate(method = 'elo')) %>%
        select(method, everything()) %>%
        spread(method, .estimate)

```

```{r accuracy of measures}

bind_rows(adjusted_game_simulations %>%
                  mutate(method = 'adjusted elo'),
          elo_game_simulations %>%
                  mutate(method = 'elo')) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                                     TRUE ~ 'no'),
                                           levels = c("no", "yes"))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_SIM_PROB >= .5 ~ 'yes',
                                            TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        group_by(method, SEASON) %>%
        yardstick::accuracy(truth = HOME_WIN,
                               estimate = HOME_PRED,
                               event_level = 'second') %>%
        mutate_if(is.numeric, round, 3)

```

Now compare log loss and calibration of probabilities

```{r get win probs calibration and log loss}

bind_rows(adjusted_game_simulations %>%
                  mutate(method = 'adjusted elo'),
          elo_game_simulations %>%
                  mutate(method = 'elo')) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                                     TRUE ~ 'no'),
                                           levels = c("no", "yes"))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_SIM_PROB >= .5 ~ 'yes',
                                            TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        group_by(method, SEASON) %>%
        yardstick::mn_log_loss(truth = HOME_WIN,
                               estimate = HOME_SIM_PROB,
                               event_level = 'second') %>%
        mutate_if(is.numeric, round, 3)

bind_rows(adjusted_game_simulations %>%
                  mutate(method = 'adjusted elo'),
          elo_game_simulations %>%
                  mutate(method = 'elo')) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                                     TRUE ~ 'no'),
                                           levels = c("no", "yes"))) %>%
        mutate(HOME_PROB_BIN = plyr::round_any(HOME_SIM_PROB, .025, floor)) %>%
        group_by(method, SEASON, HOME_PROB_BIN, HOME_WIN) %>%
        count() %>%
        spread(HOME_WIN, n) %>%
        mutate_at(c("no", "yes"),
                  ~ replace_na(., 0)) %>%
        group_by(method, SEASON) %>%
        mutate(games = no + yes) %>%
        mutate(observed_home_prop = yes / games) %>%
        ggplot(., aes(x=HOME_PROB_BIN,
                      size = games,
                    #  color = method,
                      y=observed_home_prop))+
       # facet_wrap(SEASON ~.)+
        geom_point()+
        theme_phil()+
        geom_abline(slope = 1,
                    intercept=0,
                    linetype = 'dashed')+
        geom_smooth(method = 'loess',
                    formula = 'y ~ x',
                    color = 'blue',
                    guides = 'none',
                    show.legend = F)+
        xlab("Predicted Probability of Home Win")+
        ylab("Observed Probability of Home Win")+
        facet_wrap(method ~.)+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'))+
        theme(plot.title = element_text(hjust = 0.5,
                                                size = 16),
                      plot.subtitle =  element_text(hjust = 0.5),
                      strip.text.x = element_text(size = 10,
                                                  hjust = 0.5))+
        ggtitle("Calibration of Win Probabilities for 2021 Season")

```

And the margin

```{r compare margin}

bind_rows(adjusted_game_simulations %>%
                  mutate(method = 'adjusted elo'),
          elo_game_simulations %>%
                  mutate(method = 'elo')) %>%
        group_by(method, SEASON) %>%
        mutate(HOME_MARGIN = HOME_POINTS - AWAY_POINTS) %>%
        yardstick::rmse(truth = HOME_MARGIN,
                        estimate = HOME_SIM_MARGIN) %>%
        mutate_if(is.numeric, round, 3)

```

Okay so we do a bit better in simulating with the adjusted Elo approach. Hooray!

