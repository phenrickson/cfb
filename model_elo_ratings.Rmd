---
title: "CFB Elo Ratings"
author: "Phil Henrickson"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE #adds a Table of Contents
    theme: cerulean
    number_sections: TRUE #number your headings/sections
    toc_float: TRUE #let your ToC follow you as you scroll
    keep_md: no
    fig.caption: yes
    css: "styles.css"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = F,
                      error = F,
                      warning=F,
                      dev="png",
                      fig.width = 10,
                      fig.height = 8)

options(knitr.duplicate.label = "allow")

options(scipen=999)

```

```{r connect to snowflake}

library(DBI)
library(odbc)
library(RODBC)
library(keyring)

# connect to snowflake
myconn <- DBI::dbConnect(odbc::odbc(),
                         "SnowflakeDSII",
                         Database = "CFB_DEMO",
                         warehouse = "DEMO_WH",
                         uid="phil.henrickson",
                         pwd=keyring::key_get("AE_Snowflake"))

```

```{r packages, include=F} 

source(here::here("scripts/load_packages.R"))
library(rstan)
library(rstanarm)
library(rstantools)
library(jsonlite)
library(forcats)
library(teamcolors)
conflict_prefer("lag", "dplyr")

teamcolors = teamcolors %>%
        mutate(location = case_when(location == 'Ole' ~ name,
                                    TRUE ~ location))

```

```{r get colors for ncaa teams}

# look at colors for ncca
ncaa_colors = teamcolors %>%
        filter(league == 'ncaa') 

# define palette for ncaa
#league_pal("ncaa", 1)

```


```{r flextable settings}

set_flextable_defaults(theme_fun = theme_alafoli,
                       font.color = "grey10",
                       font.size=8,
                       padding.bottom = 6, 
                       padding.top = 6,
                       padding.left = 6,
                       padding.right = 6,
                       background.color = "white")

```

```{r load functions}

# integer plotting function
int_breaks <- function(x, n = 5) {
  l <- pretty(x, n)
  l[abs(l %% 1) < .Machine$double.eps ^ 0.5] 
}

source(here::here("functions/theme_phil.R"))
rm(a)
```

```{r get games}

# get games
games_data_raw = DBI::dbGetQuery(myconn,
                             paste('SELECT * FROM CFB_DEMO.CFD_RAW.GAMES')) %>%
        as_tibble() %>%
        mutate(ID = as.numeric(ID)) %>%
        rename(GAME_ID = ID) %>%
        mutate(GAME_DATE = as.Date(START_DATE))

# recruiting
recruiting_teams_data_raw = DBI::dbGetQuery(myconn,
                             paste('SELECT * FROM CFB_DEMO.CFD_RAW.RECRUITING_TEAMS')) %>%
        as_tibble()
        
```

# What is this? {-}

This notebook explores creating Elo ratings for college football teams using data from the late 1800s to present.

# The Data

The data we're using is at the game level, in which we have features indicating the season, the week, the location, the teams, and the score. 

```{r game level data for all teams}

set.seed(8)
games_data_raw %>%
        filter(SEASON > 2000 & SEASON < 2017) %>%
        sample_n(5) %>%
        select(GAME_ID, SEASON, WEEK, VENUE, HOME_TEAM, AWAY_TEAM, HOME_POINTS, AWAY_POINTS) %>%
        mutate(HOME_WIN = case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                    TRUE ~ 'no')) %>%
        mutate(GAME_ID = as.character(GAME_ID),
               SEASON = as.character(SEASON)) %>%
        flextable() %>%
        autofit()

```

We also have additional features on team conferences, attendance, start time, betting line, as well as information from an existing Elo Model. We'll set that aside for now, as our goal here is to go through the process of developing our own Elo model. 

We lose some information the further we go back, but we have outcomes of games back to the very first game between Rutgers and Princeton in 1869.

```{r get outcomes since 1869}

games_data_raw %>%
        group_by(SEASON) %>%
        summarize(n_games = n_distinct(GAME_ID)) %>%
        ggplot(., aes(x=SEASON,
                      y=n_games))+
        geom_col()+
        theme_phil()

```

# Explaining Elo Ratings

What exactly is an Elo rating? It takes its name from the guy who first came up with the idea, Arpad Elo, who developed it for the purpose of ranking chess players: https://en.wikipedia.org/wiki/Elo_rating_system. I note this because people commonly refer to these as ELO scores - no, it's Elo, it's from his name.

The basic idea is to, for any given matchup between opponents, assign a rating to each opponent that can be used to predict the outcome of the match. The result of the match will then influence their ensuing ratings, with the winner gaining points from the loser. An opponent's rating is always a function of prior results, taking into account their performance in previous matches as well as the ratings of their opponents. 

## The Expected Score

For the full description of how this works, check out the Wikipedia page, but I'll paraphrase it here to the best of my ability. Say we've got an upcoming match between two opponents, $A$ and $B$. In an Elo rating system, the expected result, or score, for Opponent $A$ in a match against Opponent $B$ is: $$E_A = \frac{1}{1+10^{(R_B-R_A)/V}}$$

What does this equation mean?

* $E_A$ is Opponent's A expected score from the match - in the case of matchup between two opponents, this basically just reduces to win probability.

* $R_A$ is Opponent A's pre match rating

* $R_B$ is Opponent B's pre match rating

* 10 is the base value used in the rating system; typically 10 is used in these types of ratings

* V is a scaling factor that sets how much a difference in rating between the two opponents will affect the probability of either side winning. More on this in a second.

How did we get this equation? It's just a logistic curve of the form $f(x) = \frac{1}{1+e^{-x}}$, which is the standard logistic function with a growth rate and curve maximum of 1. Don't worry about this too much, it basically just constrains any input of x to return a number between 0 and 1.

Similarly, the expected score for Opponent B is: $$E_B = \frac{1}{1+10^{(R_A-R_B)/V}}$$

What's the point of this equation? We can input ratings for $A$ and $B$ and get back an expected score for an opponent based on the difference between the two opponent's rating.

```{r write a function for elo, echo=T}

# load function
source(here::here("functions/get_expected_score.R"))

# show code for function
get_expected_score

```

If the two opponents have equal ratings, the difference between their rating is zero, and the the expected score reduces to 0.5. Why? When the difference is zero, anything raised to the 0 is 1, and the scaling factor no longer matters:  $$E_A = \frac{1}{1+10^{0/V}} = \frac{1}{1+1} = \frac{1}{2}$$

So, anytime we have two perfectly equal opponents, the expected score reduces to a coin flip.

Okay, but now suppose that $R_A$ is 100 and $R_B$ is 50 and $V$ is 200. Plugging these numbers into the equation above, we get an expected score for A of `r round(get_expected_score(R_A = 100, R_B = 50, V = 200), 4)`. In this case, $A$ is more likely to win, as they have a higher rating. In this case, the scaling factor $V$ determines how much the difference in rating matters for the expected score:

$$E_A = \frac{1}{1+10^{(50-100)/200}} = \frac{1}{1+10^{-50/200}} = \frac{1}{1+10^{-1/4}} \approx \frac{1}{1+.5623}  \approx  0.64$$
If the scaling factor $V$ was set to 50, the difference in rating between these two teams would be considered much bigger.

$$E_A = \frac{1}{1+10^{(50-100)/50}} = \frac{1}{1+10^{-50/50}} = \frac{1}{1+10^{-1}} \approx \frac{1}{1+.0.1}  \approx  0.909$$
Tangibly, this means the scaling factor indicates the difference at which one opponent's expected score would be ten times greater than their opponent's expected score. I'll plot the expected score for $A$ as a function of the difference between $R_B$ and $R_A$ using different values of $V$ for the scaling factor.

```{r test the func}

a_vals = seq(0, 300, 1)
b_vals = rep(150, length(a_vals))
v_vals = c(10, 25, 50, 100)

foo = foreach(i=1:length(a_vals), .combine=bind_rows) %:% 
        foreach(j = 1:length(v_vals), .combine=bind_rows) %do% {
        
        get_expected_score(a_vals[i],
                           b_vals[i],
                           v_vals[j]) %>%
                        as_tibble() %>%
                        mutate(diff = a_vals[i]-b_vals[i],
                               v = v_vals[j])
        }

# plot
foo %>%
        mutate(v = as.factor(v)) %>%
        ggplot(., aes(x=diff,
              color = v,
              group = v,
             y=value))+
        geom_line(lwd=1.1)+
        theme_bw()+
        xlab("Rating B - Rating A")+
        ylab("Expected Score A")+
        scale_color_viridis_d()

rm(foo)

```
All this is to say that the selection of the scaling factor and the initial Elo values determine the scaling of the ratings, and we can make decisions about these. 

## Updating Elo Ratings

But none of this so far gets to the real heart of why Elo ratings prove to be useful, which is that they are always updating based on new results.  After a match between opponents, the outcome of the match determines each opponent's updated rating.

As we said earlier, before the match, $R_A$ is 100 and $R_B$ is 50 and $V$ is 200. After the match, we recalculate each opponent's score taking into account what happened in the match. How do we do this? The equation looks like this:

$$R'_A = R_A + K*(S_A - E_A)$$
We update $R'_A$ based on the previous rating plus the difference between what they actually scored $S_A$ minus what they were expected to score $E_A$, multiplied by the $K$-factor, which determines the maximum possible adjustment between games.

Let's say that that $A$ lost the match to $B$. Their initial score was 100. If the k-factor is set to 10, the most their score could drop is 10 points. They will drop by slightly less than that, based on their expected score:

$$R'_A = 100 + 10*(0 - 0.64) \approx 93.6$$
$B$, on the other hand, would get a boost to their rating.

$$R'_B = 50 + 10*(1 - 0.36) \approx 56.4$$

In this way, the winner picks up points from the loser, and $K$ determines how many points can be transferred in one match

## Margin of Victory

The original Elo rating updated didn't take into account the margin of victory by the winner, effectively treating close losses and blowouts as the same result. Fortunately, we can make a slight change to the formula for updating ratings in order to account for the margin of victory by the winner. I'm adapting the approach I'll try out here after the methodology described at https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/

How do we account for the margin of victory in updating the rating? We add a multiplier $M$ based on the margin of victory for the winner in addition to the K factor: 

$$R'_A = R_A + M*K*(S_A - E_A)$$

538 uses a margin of victory multiplier in their ratings that looks like this:

$$ M = MarginMulti = ln(WinnerPointDiff + 1) * Adjustment$$
Ignore the adjustment piece for a second. The idea is to multiply the result of the game by taking the natural log of the winner's point differential + 1 (as ln(0) is undefined). When the point differential is zero, the margin multiplier reduces to 0 and there is no multiplier. When the differential is positive the winner gets more points from their opponent based on how much they won by, with diminishing credit the further out we go in the margin of victory.

```{r plot of margin of victory}

tibble(x = seq(0, 50),
       y = log(x+1)) %>%
        ggplot(., aes(x=x,
                      y=y))+
        geom_line()+
        theme_phil()+
        xlab("margin of victory (x)")+
        ylab("margin of victory multiplier log(x+1)")

```

What about that adjustment? 538 discusses adjusting for autocorrelation, which is correlation within a time series between its current and past values. In the case of Elo ratings, if we only include a margin multiplier, we will quickly find that teams that are winning a lot are also winning by high margins. This means we'll end up with a positive feedback loop for winning teams which, lacking any sort of adjustment, can lead to inflated ratings for good teams.

538 suggest adjusting for this based around the difference in pre game Elo between the two teams. 

$$ Adjustment = \frac{2.2}{WinnerEloDiff*0.001+2.2}$$
Let's continue to say that $A$'s pre game Elo rating is 100 and $B$'s is 50. We'll keep $V$ at 200 and $K$ at 10, as before. But let's now say that $A$ lost the match by 7 points in the game. This makes the margin of victory multiplier:

$$M = log(7+1)*\frac{2.2}{-50*0.001+2.2} \approx 2.13$$

Which we then plug in for M in their updated rating formula, which becomes:

$$R'_A = 100 + 2.13*10*(0 - 0.64) \approx 87.01$$
If $A$ had only lost by one point, the margin of victory multiplier would decrease:

$$M = log(1+1)*\frac{2.2}{-50*0.001+2.2} \approx 0.709 $$

and $A$'s updated rating would become:

$$R'_A = 100 + 0.709*10*(0 - 0.64) \approx 95.46$$
Functionally, this means that the margin of victory multiplier will kick in more depending on whether you're the pre game favorite or not. This means if you win by a lot as the favorite, you won't necessarily pick up more points, but if you lose by a lot as the favorite you can take a serious hit to your points.

Here's what $A$'s updated rating would look like as a function of the margin of victory for the winner in the event of a win or a loss.

```{r update rating function, echo=T}

update_team_rating <- function(team_rating,
                               opponent_rating,
                               expected_score,
                               observed_score,
                               winner_mov,
                               k_factor = 10) {
        
        if (observed_score == 1) {
                mov_multi = log(winner_mov+1) * (2.2 / (((team_rating - opponent_rating)*0.001) + 2.2))
        } else if (observed_score ==0) {
                mov_multi = log(winner_mov+1) * (2.2 / (((opponent_rating - team_rating)*0.001) + 2.2))
        } else {
                 mov_multi = 2.2*log(2)
        }
        
        # multiplier for margin of victory

         # now compute updated rating taking into account initial rating and mov
         updated_rating = 
                 team_rating + 
                 (mov_multi *
                          k_factor * 
                          (observed_score - expected_score))
         
         return(updated_rating)
         
}

mov = seq(1, 50)
outcome = c(0,1)
a = 100
b = 50
v = 200

win_mov = foreach(i = 1:length(mov),
                  .combine = bind_rows) %:%
        foreach(j = 1:length(outcome),
                .combine = bind_rows) %do% {
                          
                           a_update = update_team_rating(
                                   team_rating = a,
                                   opponent_rating = b,
                                   expected_score = get_expected_score(a, b, v),
                                   observed_score  = outcome[j],
                                   winner_mov = mov[i],
                                   k_factor = 10) %>%
                                   tibble(updated_rating = .,
                                          team = 'A',
                                          outcome = outcome[j],
                                          mov = mov[i])
                           
                           b_update = update_team_rating(
                                   team_rating = b,
                                   opponent_rating = a,
                                   expected_score = get_expected_score(b, a, v),
                                   observed_score  = outcome[j],
                                   winner_mov = mov[i],
                                   k_factor = 10) %>%
                                   tibble(updated_rating = .,
                                          team = 'B',
                                          outcome = outcome[j],
                                          mov = mov[i])
                           
                           bind_rows(a_update, b_update)

}

# look at A's updated rating as a function of the margin of victory
win_mov %>%
        mutate(outcome = case_when(outcome == 0 & team=='A' ~ 'B win',
                                   outcome == 1 & team == 'B' ~ 'B win',
                                   outcome == 0 & team == 'B' ~ 'A win',
                                   outcome == 1 & team == 'A' ~ 'A win')) %>%
        mutate(mov = case_when(outcome == 'B win' ~ -mov,
                               outcome == 'A win' ~ mov)) %>%
        ggplot(., aes(x=mov,
                      color = team,
                      y=updated_rating))+
        geom_line()+
        theme_phil()+
        geom_hline(yintercept = 100,
                   color = 'red',
                   linetype = 'dashed')+
        xlab("A Score - B Score")+
        ylab("Updated Elo Rating")+
        scale_color_manual(values = c("red", "blue"))+
                geom_hline(yintercept = 50,
                   color = 'blue',
                   linetype = 'dashed')+
        annotate("text",
                 y=105,
                 x=-37,
                 color = "red",
                 label = "A Pregame Rating")+
                annotate("text",
                 y=55,
                 x=-37,
                 color = "blue",
                 label = "B Pregame Rating")+
        geom_vline(xintercept=0,
                   linetype = 'dotted')

```

If $A$, the heavy pre game favorite, wins by 50, the most they can really move up is 10 points (in this case, the K factor). If $A$ loses by 50 as the favorite, they can drop nearly 25. 

What's with the constants of 2.2 and 0.001? As far as I can tell, this is because they initialized their ratings for teams at around 1500 (others have less) and they expect the maximum difference in Elo to be no greater than 2200 (2.2 / 0.001). The equation itself no longer works if the WinnerEloDiff is -2200, as the denominator goes to zero. The decision for these constants should then depend on the scale factor and the initial ratings selected.

```{r remove}

rm(a_update,
   b_update,
   win_mov,
   a,
   b,
   a_vals,
   b_vals,
   mov,
   outcome)

```

# Optimizing Elo Ratings for CFB

At this point we have what we need in order to build out Elo Ratings for college football games. We could start this analysis going back all the way to 1869 as the starting point, but I'll start at 1970 for now since I'm at least somewhat familiar with teams in this era and will be able to eyeball some of the results

I'll set every team FBS's initial rating to 1500, non FBS teams to 1200, and the scaling factor to 400. The appropriate value for K is something that I'll want to explore empirically, but I'll start it at 25 and go from there. 

I'm using two more functions here to determine elo ratings. The first determines how Elo ratings are updated after each game, looking at each team's pre game Elo rating and then determing their new rating baed on the outcome and the settings for $K$ and $V$. I also have added in the option to add a slight bump to the Elo rating for the home team in order to capture home field advantage, I'll leave it at zero for now.

```{r load function for updating elo ratings, echo=T}

# functions for updating elo ratings
source(here::here("functions/get_new_elos.R"))

# updating
get_new_elos

```

The next function is what I'll use to calculate Elo ratings based on historical data, using a set of games as an input and settings for the various options to calculate Elo ratings.

```{r load function for getting elo ratings based on historical games}

source(here::here("functions/calc_elo_ratings.R"))

calc_elo_ratings

```

## Initial Settings

Let's run from 1970 to 2000 to see how the results change each team's Elo rating over the course of a couple decades. This means we loop over every game, look at the result, and then update each team's Elo rating based on the outcome and the margin of victory (as well as our other settings).

```{r run v1 from 1970 to 2000, include=F}

# now loop through a set of games
games = games_data_raw %>%
        filter(SEASON >=1970) %>%
        filter(SEASON <= 2000) %>%
        arrange(GAME_DATE) %>%
        select(GAME_ID, 
               SEASON, 
               WEEK,
               SEASON_TYPE,
               GAME_DATE,
               NEUTRAL_SITE, 
               HOME_TEAM,
               AWAY_TEAM,
               HOME_CONFERENCE,
               AWAY_CONFERENCE,
               HOME_POINTS,
               AWAY_POINTS)

# run over games
elo_returned = calc_elo_ratings(games,
                          home_field_advantage = 0,
                          reversion = 0,
                          k = 25,
                          v = 400)

```

Running this gives us a the pre and post game elo for every team for every game over the selected time period.

```{r show example}

set.seed(1999)

elo_returned$game_outcomes %>%
        sample_n(5) %>%
        arrange(GAME_DATE) %>%
        rename(DATE = GAME_DATE,
               HOME = HOME_TEAM,
               AWAY = AWAY_TEAM,
               HOME_PRE_ELO = HOME_PREGAME_ELO,
               AWAY_PRE_ELO = AWAY_PREGAME_ELO,
               HOME_POST_ELO = HOME_POSTGAME_ELO,
               AWAY_POST_ELO = AWAY_POSTGAME_ELO) %>%
        select(SEASON, HOME, AWAY, HOME_POINTS, AWAY_POINTS, HOME_PRE_ELO, AWAY_PRE_ELO, HOME_POST_ELO, AWAY_POST_ELO) %>%
        mutate(SEASON = factor(SEASON)) %>%
        mutate_if(is.numeric, round, 0) %>%
        flextable() %>%
        autofit()

```

### Examples

We can see how these ratings adjust for selected teams over time. At the current settings, here is each SEC team's Elo ratings fared over the selected time period.

```{r get SEC v1}

elo_returned$team_outcomes %>%
        filter(CONFERENCE == 'SEC') %>%
        group_by(SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ TEAM)) %>%
        left_join(.,
                  teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                          by = c("TEAM")) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      label = TEAM_LABEL,
                      color = name,
                      y=POSTGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 3,
               direction = "y",
               hjust = 0,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1.05,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.25)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 7)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(NA, 15))+
        scale_color_teams(name = "TEAM")+
        ggtitle("Elo Ratings at Initial Settings")

```

And here's the Big 10.

```{r get big ten v1}

elo_returned$team_outcomes %>%
        filter(CONFERENCE == 'Big Ten') %>%
        group_by(SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ TEAM)) %>%
        left_join(.,
                  teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                          by = c("TEAM")) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      label = TEAM_LABEL,
                      color = name,
                      y=POSTGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 3,
               direction = "y",
               hjust = 0,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1.05,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.25)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 7)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(NA, 15))+
        scale_color_teams(name = "TEAM")+
        ggtitle("Elo Ratings at Initial Settings")

```

And the Southwest Conference (RIP).

```{r get swc v1}

elo_returned$team_outcomes %>%
        filter(CONFERENCE == 'Southwest') %>%
        group_by(SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ TEAM)) %>%
        left_join(.,
                  teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                          by = c("TEAM")) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      label = TEAM_LABEL,
                      color = name,
                      y=POSTGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 3,
               direction = "y",
               hjust = 0,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1.05,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.25)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 7)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(NA, 15))+
        scale_color_teams(name = "TEAM")+
        ggtitle("Elo Ratings at Initial Settings")

```

### Results

How do we evaluate how *good* these Elo ratings are? First, we can look at the log loss for the predicted score for the home team (ie, the probability the home team wins) compared to the outcome for the home team.

How did we do? I'll compare the result of the Elo ratings to a baseline model that assigns the home team a 60% chance of winning. 

```{r log loss by week by season}

# log loss
elo_returned$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, games) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate(method = 'Elo') %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PROB = .6) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                          mn_log_loss(truth = HOME_WIN,
                                    estimate = HOME_PROB,
                                    event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=SEASON,
                      color = method,
                      y=.estimate,
                      size = games))+
        geom_point()+
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("log-loss")+
        scale_color_manual(values = c("deepskyblue1",
                                      "grey40"))+
        ggtitle("Average Log-Loss by Season")+
        coord_cartesian(ylim = c(0, 1))

```

As we would hope, the Elo ratings outperform simply predicting that the home team will win.

We can also just look at the number of games correctly predicted, and find similar results.

```{r accuracy by each for initial version}

# accuracy
elo_returned$game_outcomes %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes', TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, games) %>%
        yardstick::accuracy(truth = HOME_WIN,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        mutate(method = 'Elo') %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PRED = factor('yes',
                                                    levels = c('no', 'yes'))) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                             yardstick::accuracy(truth = HOME_WIN,
                                                 estimate = HOME_PRED,
                                                 event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=SEASON,
                      color = method,
                      y=.estimate,
                      size = games))+
        geom_point()+
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.25)+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("Accuracy")+
        scale_color_manual(values = c("deepskyblue1",
                                      "grey40"))+
        ggtitle("Average Accuracy by Season")+
        coord_cartesian(ylim = c(0, 1))

```

```{r save v1 results for comparison}

elo_returned_v1 = elo_returned

```



<!-- I'll save the Elo ratings computed with these settings so we can see how they compare with changes to the parameters/methodology. -->

<!-- ```{r save v1 results} -->

<!-- elo_ratings_returned_v1 = elo_ratings_returned -->

<!-- rm(team_elo_ratings, -->
<!--    elo_game_results, -->
<!--    elo_ratings_returned) -->

<!-- ``` -->


## Adding Mean Reversion

As it's constructed so far, the Elo ratings for a team in one season pick up right from where they left off in the previous season. This is particularly an issue in the world college football, given that players graduate and turnover in rosters year over year is high. We shouldn't expect, in other words, a team at the beginning of its 1980 season to be the exact same as it ended its 1979 season.

Currently, our Elo ratings do not reset in anyway, and this means we ended up being a little bit worse at the beginning of each season.

```{r look at results by season and week}

## accuracy
elo_returned$game_outcomes %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes', TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON, WEEK) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, WEEK, games) %>%
        yardstick::accuracy(truth = HOME_WIN,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        mutate(method = 'Elo') %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PRED = factor('yes',
                                                    levels = c('no', 'yes'))) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON, WEEK) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, WEEK, games) %>%
                          yardstick::accuracy(truth = HOME_WIN,
                                                 estimate = HOME_PRED,
                                                 event_level = 'second') %>%
                          mutate(method = 'Home Win')) %>%
        ggplot(., aes(x=WEEK,
                      group = method,
                      color = method,
                      by = SEASON,
                      y=.estimate))+
        geom_point(aes(size = games),
                   alpha = 0.5)+
        geom_smooth(
                  stat = 'smooth',
                  method = 'loess',
                  se=F,
                  formula = 'y ~ x')+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("Accuracy")+
        scale_color_manual(values = c("deepskyblue1",
                                      "grey40"))+
        ggtitle("Accuracy by Week of Season")+
        coord_cartesian(ylim = c(0, 1))

# elo_returned$game_outcomes %>%
#         mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes', TRUE ~ 'no'),
#                                   levels = c("no", "yes"))) %>%
#         mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
#                                        TRUE ~'no'))) %>%
#         group_by(SEASON, WEEK) %>%
#         mutate(games = n_distinct(GAME_ID)) %>%
#         group_by(SEASON, WEEK, games) %>%
#         yardstick::mn_log_loss(truth = HOME_WIN,
#                     estimate = HOME_PROB,
#                     event_level = 'second') %>%
#         mutate(method = 'Elo') %>%
#         bind_rows(.,
#                   elo_returned$game_outcomes %>%
#                           mutate(HOME_PROB = .6) %>%
#                           mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
#                                        TRUE ~'no'))) %>%
#                           group_by(SEASON, WEEK) %>%
#                           mutate(games = n_distinct(GAME_ID)) %>%
#                           group_by(SEASON, WEEK, games) %>%
#                           yardstick::mn_log_loss(truth = HOME_WIN,
#                                                  estimate = HOME_PROB,
#                                                  event_level = 'second') %>%
#                           mutate(method = 'Home Win')) %>%
#         ggplot(., aes(x=WEEK,
#                       group = method,
#                       color = method,
#                       by = SEASON,
#                       y=.estimate))+
#         geom_point(aes(size = games),
#                    alpha = 0.5)+
#         geom_smooth(
#                   stat = 'smooth',
#                   method = 'loess',
#                   se=F,
#                   formula = 'y ~ x')+
#         theme_phil()+
#         theme(legend.title = element_text())+
#         guides(size = guide_legend(title = 'Number of Games',
#                                    title.position = 'top'),
#                color = guide_legend(title.position = 'top'))+
#             scale_x_continuous(breaks = int_breaks)+
#         ylab("Log Loss")+
#         scale_color_manual(values = c("deepskyblue1",
#                                       "grey40"))+
#         ggtitle("Average Log-Loss by Week of Season")+
#         coord_cartesian(ylim = c(0, 1))


```

One way we can try to improve this is by regressing each team back towards the mean at the start of each season. If we think about Elo ratings in a Bayesian way, we started each team out with a mostly uninformative prior that we updated based on new information. By the end of a season we have a much better idea about how good a team is. But, between seasons, teams can change quite a bit, and we should become less certain about what we know about a team at the start of a season. 

I'll handle this by mean reverting FBS and non FBS teams slightly back to their original setting (1500 and 1200, respectively) at the start of each season. To start, I'll set this reversion to 1/4, meaning that a team's rating at the start of a season is 3/4 the result of its prior games and 1/4 back to the mean. Good teams will get pulled down slightly and bad teams will get pulled up.

```{r now re run with mean reversion, include=F}

rm(elo_returned)

# run over games
elo_returned = calc_elo_ratings(games,
                          home_field_advantage = 0,
                          reversion = 1/4,
                          k = 25,
                          v = 400)

```

### Examples

How do the ratings with mean reversion compare to the ratings with no reversion?

I'll take a team like Wisconsin and compare their pregame ratings over this time period under both version. At the start, the two ratings are the same, as Wisconsin hasn't played enough games to shift that far from the initial setting of 1500. But over the years, we can see how reverting Wisconsin a bit at the start of each season changes its rating. Over the course of 40 odd years, this difference starts to really add up in terms of the scale.

```{r look at the same team for both versions}

bind_rows(elo_returned_v1$team_outcomes %>%
                  mutate(method = 'Elo'),
        elo_returned$team_outcomes %>%
                mutate(method = 'Elo + Mean Reversion')) %>%
        left_join(.,
                          teamcolors %>%
                                  filter(league == 'ncaa') %>%
                                  mutate(TEAM = location),
                          by = c("TEAM")) %>%
        filter(TEAM == 'Wisconsin') %>%
        group_by(method, SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ PREGAME_ELO)) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      linetype = method,
                      by = name,
                      label = round(TEAM_LABEL,0),
                      color = name,
                      y=PREGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 3,
               direction = "y",
               hjust = 0,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1.05,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.25)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 7)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(NA, 15),
                        ylim = c(1200, 2200))+
        scale_color_teams(name = "TEAM")

```

If we look at all Big Ten teams over this time period, after we add reversion we should see the ratings tighten up at the beginning of each season.

```{r now plot SEC with mena reversion}

elo_returned$team_outcomes %>%
        mutate(method = 'Elo + Reversion') %>%
        filter(CONFERENCE == 'Big Ten') %>%
        mutate(SEASON_WEEK = factor(paste(SEASON, WEEK))) %>%
        ungroup() %>%
        left_join(.,
                  teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                  by = c("TEAM"))  %>%
        group_by(SEASON, TEAM) %>%
        mutate(GAME = row_number()) %>%
        mutate(LAST_GAME = GAME == max(GAME)) %>%
        mutate(TEAM_LABEL = case_when(LAST_GAME == T ~ TEAM)) %>%
      #  mutate(name = TEAM) %>%
        ggplot(., aes(x=GAME,
                      by = name,
                      label = TEAM_LABEL,
                      color = name,
                      y=PREGAME_ELO))+
       geom_text_repel(
               fontface = "bold",
               size = 3,
               direction = "y",
               hjust = 0,
               segment.size = .7,
               segment.alpha = .5,
               segment.linetype = "dotted",
               box.padding = .4,
               segment.curvature = -0.1,
               segment.ncp = 3,
               segment.angle = 20
       ) +
        geom_line(lwd = 1.05,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.25)+
        facet_wrap(CONFERENCE + SEASON~.,
                   ncol = 7)+
        theme_phil()+
        guides(label = "none",
               color = "none")+
        coord_cartesian(clip = 'off',
                        xlim = c(NA, 15))+
        scale_color_teams(name = "TEAM")


```

### Results

I'll now compare the initial ratings to the updated ratings that include mean reversion. Looking at the two methods season over season, there's not much difference between them and if anything the initial version seems to do a bit better in the 80s. I could do like, a Wald test, to compare the two formally, but who has time for that.

```{r compare initial to reversion accuracy and log loss}

# accracy
elo_returned_v1$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes',
                                            TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
        group_by(SEASON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, games) %>%
        yardstick::accuracy(truth = HOME_WIN,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        mutate(method = 'Elo ') %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                                  mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          mutate(HOME_PRED = factor(case_when(HOME_PROB >= .5 ~ 'yes',
                                            TRUE ~ 'no'),
                                  levels = c("no", "yes"))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                          yardstick::accuracy(truth = HOME_WIN,
                                              estimate = HOME_PRED,
                                              event_level = 'second') %>%
                          mutate(method = 'Elo + Mean Reversion')) %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PRED = factor('yes',
                                                    levels = c("no", "yes"))) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                         yardstick::accuracy(truth = HOME_WIN,
                                    estimate = HOME_PRED,
                                    event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=SEASON,
                      color = method,
                      y=.estimate,
                      size = games))+
        geom_point()+
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("Accuracy")+
        scale_color_manual(values = c("deepskyblue1",
                                      "navy",
                                      "grey40"))+
        ggtitle("Average Accuracy by Season")+
        coord_cartesian(ylim = c(0, 1))

# log loss
elo_returned_v1$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, games) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate(method = 'Elo ') %>%
        bind_rows(.,
                     elo_returned$game_outcomes %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                          mn_log_loss(truth = HOME_WIN,
                                    estimate = HOME_PROB,
                                    event_level = 'second') %>%
                          mutate(method = 'Elo + Mean Reversion')) %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PROB = .6) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                          group_by(SEASON) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, games) %>%
                          mn_log_loss(truth = HOME_WIN,
                                    estimate = HOME_PROB,
                                    event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=SEASON,
                      color = method,
                      y=.estimate,
                      size = games))+
        geom_point()+
        geom_line(lwd = 1,
                  stat = 'smooth',
                  method = 'loess',
                  formula = 'y ~ x',
                  span = 0.2)+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("log-loss")+
        scale_color_manual(values = c("deepskyblue1",
                                      "navy",
                                      "grey40"))+
        ggtitle("Average Log-Loss by Season")+
        coord_cartesian(ylim = c(0, 1))

```

If anything this reversion seems to be hurting the results over this time period. I wonder if we do better in the early weeks but worse in the latter?

```{r show results by week by season with updated version}

elo_returned_v1$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
        group_by(SEASON, WEEK) %>%
        mutate(games = n_distinct(GAME_ID)) %>%
        group_by(SEASON, WEEK, games) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate(method = 'Elo ') %>%
        bind_rows(.,
                     elo_returned$game_outcomes %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                         group_by(SEASON, WEEK) %>%
                        mutate(games = n_distinct(GAME_ID)) %>%
                        group_by(SEASON, WEEK, games) %>%
                        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
                          mutate(method = 'Elo + Mean Reversion')) %>%
        bind_rows(.,
                  elo_returned$game_outcomes %>%
                          mutate(HOME_PROB = .6) %>%
                          mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS  ~ 'yes',
                                       TRUE ~'no'))) %>%
                       group_by(SEASON, WEEK) %>%
                          mutate(games = n_distinct(GAME_ID)) %>%
                          group_by(SEASON, WEEK, games) %>%
                          mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
                          mutate(method = 'Home Win')
                  ) %>%
        ggplot(., aes(x=WEEK,
                      group = method,
                      color = method,
                      by = SEASON,
                      y=.estimate))+
        geom_point(aes(size = games),
                   alpha = 0.5)+
        geom_smooth(
                  stat = 'smooth',
                  method = 'loess',
                  se=F,
                  formula = 'y ~ x')+
        theme_phil()+
        theme(legend.title = element_text())+
        guides(size = guide_legend(title = 'Number of Games',
                                   title.position = 'top'),
               color = guide_legend(title.position = 'top'))+
            scale_x_continuous(breaks = int_breaks)+
        ylab("Log Loss")+
        scale_color_manual(values = c("deepskyblue1",
                                      "navy",
                                      "grey40"))+
        ggtitle("Log Loss by Week of Season")+
        coord_cartesian(ylim = c(0, 1))

```

Hmmm. The opposite, if anything. I'll try tuning the reversion at different settings, I have another idea of how to handle season over season changes, but we need to have recruiting data for that.

```{r save v2}

elo_returned_v2 = elo_returned

```

## Adding Home Field Advantage

I've seen other Elo rating systems explicitly add points to teams that are at home as a means of accounting for home field advantage. Right now, I'm not doing that, so we should expect to see the home team outperform their Elo rating. Is that the case? I'll look at the performance of the predictions from each of the models so far. Generally speaking, we don't do as well detecting when predicting the away team is going to win.

```{r results by home team}

class_metrics = metric_set(yardstick::precision,
                           yardstick::recall,
                           yardstick::f_meas,
                           yardstick::accuracy,
                           yardstick::bal_accuracy,
                           yardstick::kap,
                           yardstick::npv,
                           yardstick::ppv)

# v1 initial settings
elo_returned_v1$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        yardstick::conf_mat(HOME_WIN, HOME_PRED,
                            dnn = c("Home Pred", "Home Win"))

# v2 mean reversion
elo_returned_v2$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        yardstick::conf_mat(HOME_WIN, HOME_PRED, 
                            dnn = c("Home Pred", "Home Win"))

# bind together
bind_rows(
         elo_returned_v1$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo'),
         elo_returned_v2$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion')) %>%
        group_by(method) %>%
        class_metrics(truth = HOME_WIN,
                      estimate = HOME_PRED, 
                      event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()

```

The Elo ratings with mean reversion are basically the same overall, though both versions tend to do better when predicting that the home team will win (PPV). When the models predict that the home team will lose, they don't do as well (NPV). Right now, we tend to under predict how much the home team wins.

```{r look at probs for home vs away along with classification}

library(jcolors)

elo_returned_v2$game_outcomes %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        mutate(method = 'Elo + Mean Reversion') %>%
        mutate(PRED_CORRECT = HOME_PRED == HOME_WIN) %>%
        ggplot(., aes(x=HOME_PROB,
                      fill = PRED_CORRECT))+
        geom_histogram(bins = 100)+
        facet_wrap(HOME_WIN + method ~.,
                   ncol = 1)+
        theme_phil()+
        theme(legend.title = element_text())+
      #  scale_fill_jcolors()+
        geom_vline(xintercept = 0.5,
                   linetype = 'dashed')
        

```

One other parameter I want to toggle here is a home team advantage. I'll add a 25 point bump to the Elo score for the home team, as this roughly maps to about a 3% increase in the probability of winning. I won't add any bonus for games played at neutral sites. I'll now re run making this home team adjustment.

```{r amend functions to include home team adjustment}

rm(elo_returned)

# run over games
elo_returned = calc_elo_ratings(games,
                          home_field_advantage = 25,
                          reversion = 1/4,
                          k = 25,
                          v = 400)

elo_returned_v3 = elo_returned

```

### Results

I'll now compare the various versions we've computed so far, summarizing to their performance over the entire time period we've looked at.

```{r compare performance between all methods}

# bind together
bind_rows(
         elo_returned_v1$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo'),
         elo_returned_v2$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion'),
         elo_returned_v3$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion + Home Advantage')) %>%
        group_by(method) %>%
        class_metrics(truth = HOME_WIN,
                      estimate = HOME_PRED, 
                      event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()

bind_rows(
         elo_returned_v1$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo'),
         elo_returned_v2$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion'),
         elo_returned_v3$game_outcomes %>%
                 mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
                 mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
                 mutate(method = 'Elo + Mean Reversion + Home Advantage')) %>%
        group_by(method) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()


```

The home field advantage makes us slightly more accurate by picking up a few more home wins, but generally speaking the differences are all still pretty negligible.

## Tuning Over All Parameters

At this point, we've covered a few different parameters that we can toggle when computing Elo ratings: the K factor, the scaling factor (V), mean reversion, and home field advantage. I'll now tune over these directly and see we do by changing these to a variety of different settings.

```{r tune elo rating, include=Fs}

k_pars = seq(5, 50, 15)
v_pars = 400
reversion_pars = seq(0, 0.3, 0.1)
home_pars = seq(0, 75, 25)

# make grid
grid_pars = expand.grid(k = k_pars,
            v = v_pars,
            reversion = reversion_pars,
            home_field_advantage = home_pars)

```

```{r set parallel cores}

# doParallel::registerDoParallel(
#         parallel::detectCores()-2
#         )

# registerDoSEQ()
```


```{r loop over parameters}

# loop over settings in tuning grid in parallel
elo_tuning_results = foreach(i = 1:nrow(grid_pars), .combine = bind_rows) %do% {
        
        # get par row
        pars = grid_pars[i,]
        
        # compute elo
        elo_returned = calc_elo_ratings(games,
                          home_field_advantage = pars$home_field_advantage,
                          reversion = pars$reversion,
                          k = pars$k,
                          v = pars$v,
                          verbose = F)
        
        # grab game outcomes
        out = elo_returned$game_outcomes %>%
                mutate(home_field_advantage = pars$home_field_advantage,
                       reversion = pars$reversion,
                       k = pars$k,
                       v = pars$v)
            
        # progress           
        cat("\r", i, "of", nrow(grid_pars), "grid parameters completed");  flush.console()
        
        # return
        out
        
}

```

I'll now look at how the models performed across the combination of tuning parameters.

```{r check on tuning results}

# log loss
elo_tuning_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        arrange(.estimate) %>%
        mutate_if(is.numeric, round, 3) %>%
        head(10) %>%
        flextable() %>%
        autofit()

# roc auc
elo_tuning_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        roc_auc(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        arrange(desc(.estimate)) %>%
        mutate_if(is.numeric, round, 3) %>%
        head(10) %>%
        flextable() %>%
        autofit()

# prediction metrics
elo_tuning_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        class_metrics(truth = HOME_WIN,
                      estimate = HOME_PRED, 
                      event_level = 'second') %>%
        filter(.metric == 'f_meas') %>%
        arrange(desc(.estimate)) %>%
        mutate_if(is.numeric, round, 3) %>%
        head(25) %>%
        flextable() %>%
        autofit()

```

```{r plot across tuning parameters}

elo_tuning_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        mutate(home_field_advantage = factor(home_field_advantage)) %>%
        ggplot(., aes(x=k,
                      y=.estimate,
                      color = home_field_advantage))+
        facet_wrap(v + reversion~.,
                   ncol = 2)+
        theme_bw()+
        geom_point()+
        geom_line()+
        scale_color_viridis_d()

```

As before, we get pretty similar results across the board. I'll go with parameters with the lowest log loss over the selected time period, which we'll set K to 35, reversion to 0.1, and home field advantage to 50.

```{r select tuning parametets}

best_pars = elo_tuning_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        arrange(.estimate) %>%
        slice_min(., order_by = .estimate, n=3) %>%
        select(k, v, reversion, home_field_advantage)

```


## Validating Selected Parameters

At the best candidates for the selected parameters, I'll compute the Elo ratings over the time period we've used so far. I'll then evaluate their performance over the next decade of games.

```{r get games for time periods}

# now loop through a set of games
train_games = games_data_raw %>%
        filter(SEASON >=1970) %>%
        filter(SEASON <= 2000) %>%
        arrange(GAME_DATE) %>%
        select(GAME_ID, 
               SEASON, 
               WEEK,
               SEASON_TYPE,
               GAME_DATE,
               NEUTRAL_SITE, 
               HOME_TEAM,
               AWAY_TEAM,
               HOME_CONFERENCE,
               AWAY_CONFERENCE,
               HOME_POINTS,
               AWAY_POINTS)

# valid
valid_games = games_data_raw %>%
        filter(SEASON > 2000) %>%
        filter(SEASON <= 2010) %>%
        arrange(GAME_DATE) %>%
        select(GAME_ID, 
               SEASON, 
               WEEK,
               SEASON_TYPE,
               GAME_DATE,
               NEUTRAL_SITE, 
               HOME_TEAM,
               AWAY_TEAM,
               HOME_CONFERENCE,
               AWAY_CONFERENCE,
               HOME_POINTS,
               AWAY_POINTS)

```


```{r get results on validation set}

elo_valid_results = foreach(i = 1:nrow(best_pars), .combine = bind_rows) %do% {
        
        # get par row
        pars = best_pars[i,]
        
        # compute elo
        elo_train = calc_elo_ratings(train_games,
                          home_field_advantage = pars$home_field_advantage,
                          reversion = pars$reversion,
                          k = pars$k,
                          v = pars$v,
                          verbose = F)
        
        # now use the ratings from this for the valid set
        elo_valid = calc_elo_ratings(valid_games,
                                     teams = elo_train$teams,
                                     team_seasons = elo_train$team_seasons,
                          home_field_advantage = pars$home_field_advantage,
                          reversion = pars$reversion,
                          k = pars$k,
                          v = pars$v,
                          verbose = F)
        
        # grab game outcomes
        out = elo_valid$game_outcomes %>%
                mutate(home_field_advantage = pars$home_field_advantage,
                       reversion = pars$reversion,
                       k = pars$k,
                       v = pars$v)
            
        # progress           
        cat("\r", i, "of", nrow(best_pars), "grid parameters completed");  flush.console()
        
        # return
        out 
        
}

```

Of these candidate parameters, which approach did the best for the next decade of games?

```{r next set of games}

elo_valid_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        mn_log_loss(truth = HOME_WIN,
                    estimate = HOME_PROB,
                    event_level = 'second') %>%
        arrange(.estimate) %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()

elo_valid_results %>%
        mutate(HOME_WIN = factor(case_when(HOME_POINTS > AWAY_POINTS ~ 'yes',
                                               TRUE ~'no'))) %>%
        mutate(HOME_PRED = factor(case_when(HOME_PROB > AWAY_PROB ~ 'yes',
                                             TRUE ~ 'no'),
                                          levels = c("no", "yes"))) %>%
        group_by(k, v, reversion, home_field_advantage) %>%
        class_metrics(truth = HOME_WIN,
                    estimate = HOME_PRED,
                    event_level = 'second') %>%
        mutate_if(is.numeric, round, 3) %>%
        flextable() %>%
        autofit()

```

Again, the results are pretty consistent, so I'll go with the values stated previously.

```{r get elo ratings for trainng and valid, include=F}

selected_pars = best_pars[1,]

# compute elo on train
elo_train = calc_elo_ratings(train_games,
                  home_field_advantage = selected_pars$home_field_advantage,
                  reversion = selected_pars$reversion,
                  k = selected_pars$k,
                  v = selected_pars$v,
                  verbose = T)
        
# now use the ratings from this as the initial ratings for the validation set
elo_valid = calc_elo_ratings(valid_games,
                             teams = elo_train$teams,
                             team_seasons = elo_train$team_seasons,
                  home_field_advantage = selected_pars$home_field_advantage,
                  reversion = selected_pars$reversion,
                  k = selected_pars$k,
                  v = selected_pars$v,
                  verbose = T)

```

# Elo Ratings and the Margin of Victory

All of the work so far has been focused on predicting the winner of games in terms of both the probability and the label. 

But, a nice feature of the Elo rating framework is that it also extends to predicting the difference in points between the two teams. Here is the relationship between the point differential (Home Points - Away Points) and difference in pre game Elo ratings (Home Pre Elo - Away Pre Elo) from the seasons 1975 to 2000.

```{r we can also look at the predicted spread vs the actual}

elo_train$game_outcomes %>%
        filter(SEASON >= 1975) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        ggplot(., aes(x=HOME_ELO_DIFF, y=HOME_SCORE_DIFF))+
        geom_point()+
        facet_wrap(SEASON ~., ncol =5) +
        theme_phil()+
        geom_vline(xintercept =0,
                   linetype = 'dotted')+
        geom_hline(yintercept =0,
                   linetype = 'dotted')+
        geom_smooth(method = 'lm',
                    formula = 'y ~ x')+
        ggtitle("Predicted Point Spread via Pre Game Elo Scores")+
        xlab("Home Pregame Elo - Away Pre Game Elo")+
        ylab("Home Points - Away Points")

```

I'm omitting the first few years, as as the initial few seasons are being used to learn the Elo scores, but once we have a few seasons worth of data we start to see a pretty consistent relatioship between the difference in Elo ratings and the game's point differential.

## Linear Models

What is this relationship? I'll regress the point spread on the difference in pregame Elo ratings for each of these individual years and over all years. Then, we'll look at the coefficent on the difference in Elo ratings.

```{r fit linear model to each year, include=F}

# fit linear models at each year
nested_lm_points = elo_train$game_outcomes %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        nest(-SEASON) %>%
        # now fit linear models, i'll do so using stan
        mutate(lm = map(data, ~ stan_glm(HOME_SCORE_DIFF ~ HOME_ELO_DIFF,
                             data = .x)))

# fit linear model after first few years in training
lm_points = elo_train$game_outcomes %>%
        filter(SEASON >=1975) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        nest() %>%
        # now fit linear models, i'll do so using stan
        mutate(lm = map(data, ~ stan_glm(HOME_SCORE_DIFF ~ HOME_ELO_DIFF,
                             data = .x))) %>%
        mutate(tidied = map(lm, tidy, conf.int=T)) %>%
        mutate(glanced = map(lm, glance))

# fit linear model after first few years in training
points_model = elo_train$game_outcomes %>%
        filter(SEASON >=1975) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        nest() %>%
        # now fit linear models, i'll do so using stan
        mutate(lm = map(data, ~ lm(HOME_SCORE_DIFF ~ HOME_ELO_DIFF,
                             data = .x))) %>%
        mutate(tidied = map(lm, tidy, conf.int=T)) %>%
        mutate(glanced = map(lm, glance)) %>%
        pluck("lm",)

```

For the model trained on most of the training set of games, the estimated effect of HOME_ELO_DIFF was about .05. The intercept is a little over 4, which indicates the expected point differential in which the two teams have equivalent Elo ratings; this means the home team is expected to win by 4 given two equivalent teams. Note: the pregame Elo scores on which this model was trained do not include the home team advantage, so this is baking in the home field advantage effect. 

This means that a 20 point lead in Elo translates to about a point on the spread; being ahead by 200 on Elo going into a game is expected to win by 10 points.

```{r look at lm points}

lm_points %>%
        select(tidied) %>%
        unnest(tidied) %>%
        mutate_if(is.numeric, round, 2) %>%
        flextable() %>%
        autofit

```

## Estimated Effect of Elo Differential

```{r tidy and plot the effect}

library(tidybayes)
library(broom.mixed)

# plot the coef
nested_lm_points %>%
        mutate(tidied = map(lm, tidy, conf.int=T)) %>%
        select(SEASON, tidied) %>%
        unnest(tidied) %>%
        filter(term == 'HOME_ELO_DIFF') %>%
        ggplot(., aes(x=SEASON,
                      y=estimate,
                      ymin = conf.low,
                      ymax = conf.high))+
        geom_point()+
        geom_pointrange()+
        theme_phil()+
        geom_hline(yintercept = 0,
                   linetype = 'dashed',
                   lwd = 1.1)+
        geom_hline(yintercept = 0.05,
                   linetype = 'dotted',
                   color ='grey60',
                   lwd = 1.1)

```

This isn't to say that a 200 point lead on Elo means a 10 point spread is suddely a sure thing. We can look at the standard deviation of the residuals (sigma) from each model to get a sense of how far off, on average, predictions for the point differential were. Generally, sigma was around 16, meaning the predictions were off by 16 points *on average*.

```{r look at the uncertainty}

# show sigma for model trained on multiple seasons
lm_points %>%
        select(glanced) %>%
        unnest(glanced) %>%
        mutate_if(is.numeric, round, 2) %>%
        flextable() %>%
        autofit
        
# show sigma by season
nested_lm_points %>%
        mutate(tidied = map(lm, tidy, conf.int=T)) %>%
        mutate(glanced = map(lm, glance)) %>%
        select(SEASON, tidied, glanced) %>%
        select(SEASON, glanced) %>% 
        unnest() %>%
        select(SEASON, algorithm, nobs, sigma) %>%
        ggplot(., aes(x=SEASON, y = sigma)) +
        geom_point(aes(size=nobs))+
        geom_line()+
        theme_phil()

```

That number seems laughably high, but it's skewed somewhat by the model being *really off* for certain games. The in sample mean absolute error was closer to 12 points, with an Rsquared of around .38.

```{r look at the distribution of predicted vs actual}

library(forcats)

lm_points %>%
        mutate(fitted = map(lm, ~ predict(.x))) %>%
        select(data, fitted) %>%
        unnest() %>%
        mutate(resid = round(fitted,0) - HOME_SCORE_DIFF) %>%
        ggplot(., aes(x=fitted, y=HOME_SCORE_DIFF))+
        geom_point(alpha = 0.2)+
        theme_phil()+
        geom_abline(slope =1, 
                    intercept=0)+
        geom_vline(xintercept = 0,
                   linetype = 'dashed')+
        geom_hline(yintercept=0,
                   linetype = 'dashed')+
        stat_cor(
                  aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")), 
                  label.x = 3,
                  p.accuracy = .01)

lm_points %>%
        mutate(fitted = map(lm, ~ predict(.x))) %>%
        select(data, fitted) %>%
        unnest() %>%
        mutate(resid = round(fitted,0) - HOME_SCORE_DIFF) %>%
        yardstick::mae(truth = HOME_SCORE_DIFF,
                       estimate = fitted) %>%
        flextable() %>%
        autofit()

# lm_points %>%
#         mutate(fitted = map(lm, ~ predict(.x))) %>%
#         select(data, fitted) %>%
#         unnest() %>%
#         mutate(resid = round(fitted,0) - HOME_SCORE_DIFF) %>%
#         mutate(SEASON = factor(SEASON)) %>%
#         ggplot(., aes(x=resid,
#                       y=fct_rev(SEASON)))+
#         stat_density_ridges(quantile_lines = T,
#                             quantiles = c(0.1, 0.5, 0.9))+
#         theme_phil()+
#         geom_vline(xintercept = c(-21, -7, 0, 7, 21),
#                    linetype = 'dotted')
        
```

## Simulating the Margin of Victory

What's the point of this? In addition to using Elo ratings to predict the outcomes of games, we can also use them to simulate the expected point differential. I'll use one game as an example illustrate.

Let's go to a week 1 game in 2001: Wisconsin vs Virginia. Based on computed Elo ratings, Wisconsin was the favorite going into this game, with around a 130 point advantage in pregame Elo. As the home team, this means Wisconsin is about a 10.5 point favorite according to our model (home team advantage). That's the point estimate, but we can simulate from the model a few thousand times and plot the distribution of the simulations to see the uncertainty around the estimate.

```{r predict the point differential for one game in }

# get model object out
lm_train_points = lm_points %>%
        pluck("lm", 1)

# get sample game
sample_game = elo_valid$game_outcomes %>% 
        filter(GAME_ID == 212370275) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS)

# simulate this - add predicted draws
sample_sims = sample_game %>%
        add_predicted_draws(lm_train_points)

# plot the simulations
sample_sims %>%
        # mutate(HOME_WIN = case_when(sim > 0 ~ 'yes',
        #                             TRUE ~ 'no')) %>%
        ggplot(., aes(x=.prediction))+
        geom_histogram(bins = 100)+
        geom_vline(xintercept = sample_game$HOME_SCORE_DIFF,
                   lwd = 1.1,
                   color = 'red')+
        geom_vline(xintercept = 0,
                   linetype = 'dotted')+
        theme_phil()+
        xlab("Simulated Point Differential")

# show distribution
sample_sims %>%
        median_qi(.width =c(.5, .8)) %>%
        select(.prediction, .lower, .upper, .width) %>%
        mutate_if(is.numeric, round, 1) %>%
        flextable() %>%
        autofit()
        

```

The 50% prediction interval for the game ranged from about -1 to 21, while the 80% interval ranged from about -11 to 31. 

What was the actual score? Wisconsin 26, Virginia 17, for a point differential of 9. So in this case, the model did kinda nail it, but the point is the uncertainty is generally quite high with our estimates. If we look at all games from week 1, we can see that the model got some right and some wrong. 

```{r use model to simulate 2001}

# get sample game
sample_games = elo_valid$game_outcomes %>% 
        filter(SEASON == 2001 & WEEK ==1) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS)

# simulate this - add predicted draws
sample_sims = sample_games%>%
        add_predicted_draws(lm_train_points)

# now predict the 2001 season
sample_sims %>%
        median_qi(.width = c(0.5, .8)) %>%
        ggplot(aes(y = paste(HOME_TEAM,
                             AWAY_TEAM,
                             sep = "-"),
                   x = .prediction, xmin = .lower, xmax = .upper))+
        geom_pointinterval(position = position_dodge(width = .3))+
        geom_point(data = sample_sims %>%
                           median_qi(.width = c(0.5, .8)),
                   aes(x = HOME_SCORE_DIFF),
                   size = 4,
                   color = 'blue')+
        theme_phil()+
        geom_vline(xintercept = 0,
                   linetype = 'dashed')+
        ylab("Game")+
        xlab("Home Points - Away Points")
        
```

We can do this for the entire season and look to see how the model generally did over the entire 2001 season.

```{r look at sims for 2001 season}

elo_valid$game_outcomes %>% 
        filter(SEASON == 2001) %>%
        mutate(HOME_ELO_DIFF = HOME_PREGAME_ELO - AWAY_PREGAME_ELO) %>%
        mutate(HOME_SCORE_DIFF = HOME_POINTS-AWAY_POINTS) %>%
        add_predicted_draws(lm_train_points, ndraws = 1) %>%
        mutate(.pred = round(.prediction, 0)) %>%
        mutate(HOME_SCORE_DIFF_char = case_when(HOME_SCORE_DIFF > 0 ~ paste("+", HOME_SCORE_DIFF, sep=""),
                                    HOME_SCORE_DIFF == 0 ~ paste("Tie"),
                                    HOME_SCORE_DIFF < 0 ~ paste(HOME_SCORE_DIFF, sep=""))) %>%
        mutate(sim_char = case_when(.pred > 0 ~ paste("+", .pred, sep=""),
                                    .pred == 0 ~ paste("Tie"),
                                    .pred < 0 ~ paste(.pred, sep="")),
               GAME = paste(HOME_TEAM, " ", AWAY_TEAM, "\n",
                            "Actual: ", HOME_TEAM, " ", HOME_SCORE_DIFF_char, "\n",
                            "Sim: ", HOME_TEAM, " ", sim_char, sep="")) %>%
        mutate(CORRECT = case_when(.pred >0 & HOME_SCORE_DIFF >0 ~ 'yes',
                                   .pred <0 & HOME_SCORE_DIFF <0 ~ 'yes',
                                   TRUE ~ 'no')) %>%
        ggplot(., aes(x=HOME_SCORE_DIFF,
                      color = CORRECT,
                      label = GAME,
                      y=.pred))+geom_point()+
        facet_wrap(SEASON + WEEK ~., ncol =3)+
        geom_text(check_overlap=T, 
                  vjust=-0.5, size=2)+
        theme_phil()+
       scale_color_manual(values = c("red", "blue"))+
        geom_vline(xintercept =0,
                   linetype = 'dotted')+
        geom_hline(yintercept = 0,
                   linetype = 'dotted')+
        xlab("Simulated Point Diff")+
        ylab("Actual Point Diff")


```

<!-- ```{r simulate and compare point differential} -->

<!-- set.seed(2) -->
<!-- foo %>% -->
<!--         filter(SEASON == 2001) %>% -->
<!--         mutate(sim = map(lm, simulate)) %>% -->
<!--         select(SEASON, data, sim) %>% -->
<!--         unnest() %>% -->
<!--         mutate(sim_1 = round(sim_1, 0)) %>% -->
<!--         mutate(HOME_SCORE_DIFF_char = case_when(HOME_SCORE_DIFF > 0 ~ paste("+", HOME_SCORE_DIFF, sep=""), -->
<!--                                     HOME_SCORE_DIFF == 0 ~ paste("Tie"), -->
<!--                                     HOME_SCORE_DIFF < 0 ~ paste(HOME_SCORE_DIFF, sep=""))) %>% -->
<!--         mutate(sim_char = case_when(sim_1 > 0 ~ paste("+", sim_1, sep=""), -->
<!--                                     sim_1 == 0 ~ paste("Tie"), -->
<!--                                     sim_1 < 0 ~ paste(sim_1, sep="")), -->
<!--                GAME = paste(HOME_TEAM, " ", AWAY_TEAM, "\n", -->
<!--                             "Actual.:", HOME_TEAM,  HOME_SCORE_DIFF_char, "\n", -->
<!--                             "Sim:", HOME_TEAM, sim_char, sep="")) %>% -->
<!--         mutate(CORRECT = case_when(sim_1 >0 & HOME_SCORE_DIFF >0 ~ 'yes', -->
<!--                                    sim_1 <0 & HOME_SCORE_DIFF <0 ~ 'yes', -->
<!--                                    TRUE ~ 'no')) %>% -->
<!--         ggplot(., aes(x=HOME_SCORE_DIFF, -->
<!--                       color = CORRECT, -->
<!--                       label = GAME, -->
<!--                       y=sim_1))+geom_point()+ -->
<!--         geom_text(check_overlap=T, -->
<!--                   vjust=-0.5, size=2)+ -->
<!--         theme_phil()+ -->
<!--         scale_color_manual(values = c("red", "blue"))+ -->
<!--         facet_wrap(SEASON + WEEK~., -->
<!--                    ncol = 4)+ -->
<!--         geom_vline(xintercept =0, -->
<!--                    linetype = 'dotted')+ -->
<!--         geom_hline(yintercept = 0, -->
<!--                    linetype = 'dotted') -->

<!-- ``` -->

# Simulating Sesons

Okay, so at this point we have everything we need to try out simulating entire seasons. 

To simulate a season, we simulate each game based on the pre game Elo scores for each team. Say that Team A has a 67% chance of winning based on the two teams Elo scores. In that case, I can simulate the game by pulling from a Bernoulli distribution where p =.67. Based on that result, I then compute the outcome and update the ratings. I repeat this process N times for every game, meaning that the Elo ratings are themselves updated on the basis of each simulated result. If I simulate a season 1000 times, I get a distribution of simulated outcomes for every game and team, which I can then use to obtain probabilities of specific events. 

The snag with this approach is that I'm using a margin of victory multiplier to update ratings after each game. To use this approach, I need to not only simulate the winner, but also the point margin of each game.  This is where the model to predict the point differential comes in.

I can simulate the outcome and margin of each game by pulling one simulation from the points model, then use this to update the ratings. 

```{r load sim elo ratings func}

source(here::here("functions", "simulateX.R"))
source(here::here("functions", "sim_game_margin.R"))
source(here::here("functions", "sim_elo_ratings.R"))

# show functions
sim_game_margin
sim_elo_ratings

```

To show this works, I'll simulate the entire 2001 season starting with the Elo ratings we computed on games from 1970-2000.

```{r set up parallel}

library(future.apply)
plan(multisession, workers = 7)

```


```{r simulate 2001 season}

sim_seasons = future_replicate(1,
                  sim_elo_ratings(valid_games %>%
                                          filter(SEASON == 2001) %>%
                                          filter(WEEK < 15),
                                  teams = elo_train$teams,
                                  team_seasons = elo_train$team_seasons,
                                  home_field_advantage = selected_pars$home_field_advantage,
                                  reversion = selected_pars$reversion,
                                  k = selected_pars$k,
                                  v = selected_pars$v,
                                  verbose=F,
                                  ties =F,
                                  points_model = points_model))

beepr::beep(3)

```


```{r get the simulated seasons}

sim_team_outcomes = rbindlist(sim_seasons['team_outcomes',],
          idcol = T)

sim_team_outcomes %>%
        filter(SEASON == 2001) %>%
        filter(CONFERENCE == 'Big 12') %>%
        left_join(., teamcolors %>%
                          filter(league == 'ncaa') %>%
                          mutate(TEAM = location),
                          by = c("TEAM")) %>%
        ggplot(., aes(x=GAME_DATE,
                      color = name,
                      group = .id,
                      y = POSTGAME_ELO)) +
        geom_line(alpha = 0.05)+
        # geom_line(stat = 'smooth',
        #           method = 'loess',
        #           formula = 'y ~ x',
        #           span = 0.25,
        #           alpha = 0.5)+
        theme_phil()+
        facet_wrap(SEASON + TEAM ~.,
                   ncol =)+
        scale_color_teams(name = "TEAM")+
        guides(color = 'none')+
        ggtitle("Simulated Elo Ratings by Season")

```

I'll simulate for Pittsburgh just for Rob.


```{r run simulation for seasons}

sims_elo_returned = foreach(s = 1:100) %do% {

       out =  sim_season_with_reversion_home_adjust(input_game_matchups = games_data,
                                                         input_team_games = team_games,
                                                         input_initial_team_elo_ratings = initial_team_elo_ratings,
                                                         input_points_model = points_model,
                                                         k = 25,
                                                         v = 400,
                                                         reversion = 1/4,
                                                         home_adjust = 25)
     
       out
       
}

```

```{r look at the results for one season}

sims_elo_returned %>%
        rbindlist(., idcol=T) %>%
        mutate(TEAM = HOME_TEAM,
               OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'win',
                                   HOME_OUTCOME == 0 ~ 'loss',
                                   HOME_OUTCOME == 0.5 ~ 'tie'),
               POINT_DIFF = HOME_POINT_DIFF) %>%
        select(SEASON, GAME_ID, TEAM, OUTCOME, POINT_DIFF) %>%
        mutate(HOST = 'HOME') %>%
        bind_rows(.,
                  sims_elo_returned %>%
                          rbindlist(., idcol=T) %>%
                          mutate(TEAM = AWAY_TEAM,
                                 OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'loss',
                                   HOME_OUTCOME == 0 ~ 'win',
                                   HOME_OUTCOME == 0.5 ~ 'tie'),
                                 POINT_DIFF = -HOME_POINT_DIFF) %>%
                          select(SEASON, GAME_ID, TEAM, OUTCOME, POINT_DIFF) %>%
                          mutate(HOST='AWAY')) %>%
        group_by(SEASON, TEAM) %>%
        count(OUTCOME) %>%
        spread(OUTCOME, n) %>%
        select(SEASON, TEAM, win, loss, tie) %>%
        arrange(desc(win)) %>% 
        ungroup() %>%
        mutate_if(is.numeric, replace_na, 0)

```

Predicted wins vs actual

```{r compare predicted vs actual wins for simulation}

games_data %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(TEAM = HOME_TEAM,
               OUTCOME = case_when(OUTCOME == 1 ~ 'win',
                                   OUTCOME == 0 ~ 'loss',
                                   OUTCOME == 0.5 ~ 'tie'),
               POINT_DIFF = HOME_POINTS - AWAY_POINTS,
               OPPONENT = AWAY_TEAM) %>%
        select(SEASON, GAME_ID, TEAM, OPPONENT, OUTCOME, POINT_DIFF) %>%
        mutate(HOST = 'HOME') %>%
        bind_rows(.,
                  games_data %>%
                          mutate(TEAM = AWAY_TEAM,
                                 OUTCOME = case_when(OUTCOME == 1 ~ 'loss',
                                                     OUTCOME == 0 ~ 'win',
                                                     OUTCOME == 0.5 ~ 'tie'),
                                 POINT_DIFF = AWAY_POINTS - HOME_POINTS,
                                 OPPONENT = HOME_TEAM) %>%
                          select(SEASON, GAME_ID, TEAM, OUTCOME, OPPONENT, POINT_DIFF) %>%
                          mutate(HOST='AWAY')) %>%
        arrange(GAME_ID) %>%
        group_by(SEASON, TEAM) %>%
        count(OUTCOME) %>%
        spread(OUTCOME, n) %>%
        select(SEASON, TEAM, win, loss, tie) %>%
        arrange(desc(win)) %>% 
        ungroup() %>%
        mutate_if(is.numeric, replace_na, 0) %>%
        rename(actual_wins = win) %>%
        select(SEASON, TEAM, actual_wins) %>%
        left_join(.,
                sims_elo_returned %>%
                        rbindlist(., idcol=T) %>%
                        mutate(TEAM = HOME_TEAM,
                               OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'win',
                                                   HOME_OUTCOME == 0 ~ 'loss',
                                                   HOME_OUTCOME == 0.5 ~ 'tie'),
                               POINT_DIFF = HOME_POINT_DIFF) %>%
                        select(SEASON, GAME_ID, TEAM, OUTCOME, POINT_DIFF) %>%
                        mutate(HOST = 'HOME') %>%
                        bind_rows(.,
                                  sims_elo_returned %>%
                                          rbindlist(., idcol=T) %>%
                                          mutate(TEAM = AWAY_TEAM,
                                                 OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'loss',
                                                   HOME_OUTCOME == 0 ~ 'win',
                                                   HOME_OUTCOME == 0.5 ~ 'tie'),
                                                 POINT_DIFF = -HOME_POINT_DIFF) %>%
                                          select(SEASON, GAME_ID, TEAM, OUTCOME, POINT_DIFF) %>%
                                          mutate(HOST='AWAY')) %>%
                        group_by(SEASON, TEAM) %>%
                        count(OUTCOME) %>%
                        spread(OUTCOME, n) %>%
                        select(SEASON, TEAM, win, loss, tie) %>%
                        arrange(desc(win)) %>% 
                        ungroup() %>%
                        mutate_if(is.numeric, replace_na, 0) %>%
                        rename(pred_wins = win) %>%
                        select(SEASON, TEAM, pred_wins),
                by = c("SEASON", "TEAM")) %>%
        left_join(.,
                  team_games %>%
                          select(SEASON, CONFERENCE, TEAM) %>%
                          unique(),
                  by = c("SEASON", "TEAM")) %>%
        filter(!is.na(CONFERENCE)) %>%
        ggplot(., aes(x=pred_wins,
                      label = TEAM,
                      y = actual_wins))+
        geom_point(position=ggforce::position_jitternormal())+
        geom_text_repel(size=2.5)+
        facet_wrap(CONFERENCE ~.)+
        theme_bw()+
        geom_abline(slope=1,
                    intercept=0,
                    linetype = 'dashed')
        
```



```{r sims elo returned}

sims_elo_returned %>%
        rbindlist(., idcol=T) %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(TEAM = HOME_TEAM,
               TEAM_PRE_ELO = HOME_PRE_ELO,
               TEAM_POST_ELO = HOME_POST_ELO,
               OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'win',
                                   HOME_OUTCOME == 0 ~ 'loss',
                                   HOME_OUTCOME == 0.5 ~ 'tie'),
               POINT_DIFF = HOME_POINT_DIFF,
               OPPONENT = AWAY_TEAM) %>%
        select(GAME_ID, TEAM, TEAM_PRE_ELO, TEAM_POST_ELO, OPPONENT, OUTCOME, POINT_DIFF) %>%
        mutate(HOST = 'HOME') %>%
        bind_rows(.,
                  sims_elo_returned %>%
                          rbindlist(., idcol=T) %>%
                          mutate(TEAM = AWAY_TEAM,
                                 TEAM_PRE_ELO = AWAY_PRE_ELO,
                                 TEAM_POST_ELO = AWAY_POST_ELO,
                                 OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'loss',
                                   HOME_OUTCOME == 0 ~ 'win',
                                   HOME_OUTCOME == 0.5 ~ 'tie'),
                                 POINT_DIFF = -HOME_POINT_DIFF,
                                 OPPONENT = HOME_TEAM) %>%
                          select(GAME_ID, TEAM, TEAM_PRE_ELO, TEAM_POST_ELO, OUTCOME, OPPONENT, POINT_DIFF) %>%
                          mutate(HOST='AWAY')) %>%
        arrange(GAME_ID) %>%
      #  arrange(GAME_ID) %>%
                filter(TEAM == 'Texas A&M')

```

```{r sim multiple times}

sims_elo_returned = foreach(s = 1:100) %do% {

       out =  sim_season_with_reversion_home_adjust(input_game_matchups = games_data,
                                                         input_team_games = team_games,
                                                         input_initial_team_elo_ratings = initial_team_elo_ratings,
                                                         input_points_model = points_model,
                                                         k = 25,
                                                         v = 400,
                                                         reversion = 1/4,
                                                         home_adjust = 25)
     
       out
       
}

```

```{r look at simulations}

sims_elo_returned %>%
        rbindlist(., idcol=T) %>%
        mutate_if(is.numeric, round, 2) %>%
        mutate(TEAM = HOME_TEAM,
               TEAM_PRE_ELO = HOME_PRE_ELO,
               TEAM_POST_ELO = HOME_POST_ELO,
               OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'win',
                                   HOME_OUTCOME == 0 ~ 'loss',
                                   HOME_OUTCOME == 0.5 ~ 'tie'),
               POINT_DIFF = HOME_POINT_DIFF,
               OPPONENT = AWAY_TEAM) %>%
        select(SEASON, GAME_ID, TEAM, TEAM_PRE_ELO, TEAM_POST_ELO, OPPONENT, OUTCOME, POINT_DIFF, .id) %>%
        mutate(HOST = 'HOME') %>%
        bind_rows(.,
                  sims_elo_returned %>%
                          rbindlist(., idcol=T) %>%
                          mutate(TEAM = AWAY_TEAM,
                                 TEAM_PRE_ELO = AWAY_PRE_ELO,
                                 TEAM_POST_ELO = AWAY_POST_ELO,
                                 OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'loss',
                                   HOME_OUTCOME == 0 ~ 'win',
                                   HOME_OUTCOME == 0.5 ~ 'tie'),
                                 POINT_DIFF = -HOME_POINT_DIFF,
                                 OPPONENT = HOME_TEAM) %>%
                          select(SEASON, GAME_ID, TEAM, TEAM_PRE_ELO, TEAM_POST_ELO, OUTCOME, OPPONENT, POINT_DIFF, .id) %>%
                          mutate(HOST='AWAY')) %>%
        left_join(.,
                  team_games %>%
                          select(SEASON, CONFERENCE, TEAM) %>%
                          unique(),
                  by = c("SEASON", "TEAM")) %>%
        filter(CONFERENCE == 'Big Ten') %>%
        group_by(SEASON, TEAM, .id) %>%
        count(OUTCOME) %>%
        filter(OUTCOME == 'win') %>%
        ggplot(., aes(x=n))+
        geom_histogram(bins=30)+
        facet_wrap(SEASON + TEAM~.)+
        theme_bw()

```

```{r sim seasons}

mode_func <- function(codes){
  which.max(tabulate(codes))
}

sims_elo_returned %>%
        rbindlist(., idcol=T) %>%
     mutate_if(is.numeric, round, 2) %>%
     mutate(TEAM = HOME_TEAM,
           TEAM_PRE_ELO = HOME_PRE_ELO,
           TEAM_POST_ELO = HOME_POST_ELO,
            OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'win',
                                HOME_OUTCOME == 0 ~ 'loss',
                                HOME_OUTCOME == 0.5 ~ 'tie'),
            POINT_DIFF = HOME_POINT_DIFF,
            OPPONENT = AWAY_TEAM) %>%
     select(SEASON, GAME_ID, TEAM, TEAM_PRE_ELO, TEAM_POST_ELO, OPPONENT, OUTCOME, POINT_DIFF, .id) %>%
     mutate(HOST = 'HOME') %>%
     bind_rows(.,
               sims_elo_returned %>%
                   rbindlist(., idcol=T) %>%
                   mutate(TEAM = AWAY_TEAM,
                          TEAM_PRE_ELO = AWAY_PRE_ELO,
                          TEAM_POST_ELO = AWAY_POST_ELO,
                          OUTCOME = case_when(HOME_OUTCOME == 1 ~ 'loss',
                                              HOME_OUTCOME == 0 ~ 'win',
                                              HOME_OUTCOME == 0.5 ~ 'tie'),
                          POINT_DIFF = -HOME_POINT_DIFF,
                          OPPONENT = HOME_TEAM) %>%
                   select(SEASON, GAME_ID, TEAM, TEAM_PRE_ELO, TEAM_POST_ELO, OUTCOME, OPPONENT, POINT_DIFF, .id) %>%
                   mutate(HOST='AWAY')) %>%
     left_join(.,
               team_games %>%
                   select(SEASON, CONFERENCE, TEAM) %>%
                   unique(),
               by = c("SEASON", "TEAM")) %>%
     group_by(SEASON, TEAM, .id) %>%
     count(OUTCOME) %>%
        filter(OUTCOME == 'win') %>% 
        group_by(SEASON, TEAM) %>%
        summarize(mean_wins = mean(n),
                  mode_wins = mode_func(n),
                  .groups = 'drop') %>% 
        arrange(desc(mode_wins)) %>%
        mutate_if(is.numeric, round, 2)

```

